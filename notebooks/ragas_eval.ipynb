{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c13e2386",
   "metadata": {},
   "source": [
    "# RAGAS Evaluation\n",
    "\n",
    "Streamlined notebook for loading a lightweight evaluation dataset, indexing a small knowledge base, and computing RAGAS metrics against the project workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b17ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "from app.models.models import Document\n",
    "from app.db.vector_db import VectorDB\n",
    "from app.workflow.rag_workflow import RAGWorkflow\n",
    "from app.workflow.reranker import Reranker\n",
    "from app.workflow.router import LLMClient\n",
    "from app.core.config import settings\n",
    "\n",
    "from qdrant_client import models as qdrant_models\n",
    "\n",
    "from ragas import EvaluationDataset, evaluate\n",
    "from ragas.metrics import (\n",
    "    LLMContextRecall,\n",
    "    Faithfulness,\n",
    "    FactualCorrectness,\n",
    "    LLMContextPrecisionWithReference,\n",
    "    ResponseRelevancy,\n",
    ")\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from langchain.chat_models import init_chat_model\n",
    "from ragas.run_config import RunConfig\n",
    "from ragas.embeddings import OpenAIEmbeddings\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4db7185",
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_COLLECTION_NAME = f\"{settings.qdrant_collection_name}-ragas\"\n",
    "KNOWLEDGE_SAMPLE_SIZE = 1000\n",
    "EVALUATION_SAMPLE_SIZE = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375e6d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_settings = settings.model_copy(\n",
    "    update={\n",
    "        \"qdrant_collection_name\": EVAL_COLLECTION_NAME,\n",
    "        \"enable_reranker\": False,\n",
    "    }\n",
    ")\n",
    "\n",
    "vector_db = VectorDB(eval_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3d36a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db.client.recreate_collection(\n",
    "    collection_name=EVAL_COLLECTION_NAME,\n",
    "    vectors_config={\n",
    "        \"dense\": qdrant_models.VectorParams(\n",
    "            size=eval_settings.embeddings_dim,\n",
    "            distance=qdrant_models.Distance.COSINE,\n",
    "        )\n",
    "    },\n",
    "    sparse_vectors_config={\n",
    "        \"bm25\": qdrant_models.SparseVectorParams(modifier=qdrant_models.Modifier.IDF)\n",
    "    },\n",
    ")\n",
    "\n",
    "print(f\"Collection '{EVAL_COLLECTION_NAME}' ready for evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca1b027",
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge_dataset = load_dataset(\"jamescalam/ai-arxiv2-chunks\", split=\"train\")\n",
    "knowledge_sample = knowledge_dataset.select(\n",
    "    range(min(KNOWLEDGE_SAMPLE_SIZE, len(knowledge_dataset)))\n",
    ")\n",
    "\n",
    "knowledge_docs = [\n",
    "    Document(\n",
    "        text=row[\"chunk\"],\n",
    "        metadata={\n",
    "            \"metadata\": {\n",
    "                \"source\": row.get(\"source\", \"\"),\n",
    "                \"title\": row.get(\"title\", \"\"),\n",
    "            }\n",
    "        },\n",
    "    )\n",
    "    for row in knowledge_sample\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492413c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2f0c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_titles = set(knowledge_sample[\"title\"])\n",
    "print(f\"Number of unique titles: {len(unique_titles)}\")\n",
    "for title in list(unique_titles):\n",
    "    print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0980cab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = sum(1 for title in knowledge_sample[\"title\"] if title == \"Mixtral of Experts\")\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162af7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "kept_docs = []\n",
    "dense_embeddings = []\n",
    "sparse_embeddings = []\n",
    "\n",
    "for doc in tqdm(knowledge_docs, desc=\"Embedding knowledge chunks\"):\n",
    "    try:\n",
    "        dense_embeddings.append(vector_db.get_embeddings(doc.text))\n",
    "        sparse_embeddings.append(next(vector_db.sparse_model.embed([doc.text])))\n",
    "        kept_docs.append(doc)\n",
    "    except Exception as exc:\n",
    "        print(f\"Skipping chunk due to embedding error: {exc}\")\n",
    "\n",
    "if not kept_docs:\n",
    "    raise RuntimeError(\"No chunks were indexed; abort evaluation.\")\n",
    "\n",
    "vector_db.add_documents(\n",
    "    kept_docs,\n",
    "    np.vstack(dense_embeddings),\n",
    "    sparse_embeddings,\n",
    ")\n",
    "\n",
    "print(f\"Indexed {len(kept_docs)} knowledge chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "719f4810",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_dataset = load_dataset(\"aurelio-ai/ai-arxiv2-ragas-mixtral\", split=\"train\")\n",
    "\n",
    "evaluation_sample = evaluation_dataset.select(\n",
    "    range(min(EVALUATION_SAMPLE_SIZE, len(evaluation_dataset)))\n",
    ")\n",
    "\n",
    "print(f\"Prepared {len(evaluation_sample)} evaluation questions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6a8868",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = RAGWorkflow()\n",
    "workflow.config = eval_settings\n",
    "workflow.vector_db = vector_db\n",
    "workflow.llm = LLMClient(eval_settings)\n",
    "workflow.reranker = Reranker(eval_settings)\n",
    "\n",
    "graph = workflow.build()\n",
    "print(\"Workflow compiled for evaluation collection.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05044dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_records = []\n",
    "\n",
    "for idx, row in enumerate(tqdm(evaluation_sample, desc=\"Running workflow\")):\n",
    "    question = row[\"question\"]\n",
    "    reference = (\n",
    "        row[\"ground_truth\"][0]\n",
    "        if isinstance(row[\"ground_truth\"], list)\n",
    "        else row[\"ground_truth\"]\n",
    "    )\n",
    "\n",
    "    state = {\"question\": question}\n",
    "    config_dict = {\"configurable\": {\"thread_id\": f\"ragas_eval_{idx}\"}}\n",
    "    result = graph.invoke(state, config=config_dict)\n",
    "\n",
    "    answer = result.get(\"answer\", \"\")\n",
    "    contexts = [doc.text for doc in result.get(\"context\", []) if hasattr(doc, \"text\")]\n",
    "\n",
    "    evaluation_records.append(\n",
    "        {\n",
    "            \"user_input\": question,\n",
    "            \"retrieved_contexts\": contexts,\n",
    "            \"response\": answer,\n",
    "            \"reference\": reference,\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(f\"Collected {len(evaluation_records)} responses.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467ddfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"evaluation_records.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(evaluation_records, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"Saved evaluation records to evaluation_records.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6ddddb",
   "metadata": {},
   "source": [
    "## Load Evaluation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48bc9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"evaluation_records.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    loaded_records = json.load(f)\n",
    "print(loaded_records[0])\n",
    "\n",
    "evaluation_records = loaded_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48fa4b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not evaluation_records:\n",
    "    raise RuntimeError(\"No evaluation records created.\")\n",
    "\n",
    "evaluation_data = EvaluationDataset.from_list(evaluation_records)\n",
    "\n",
    "llm = init_chat_model(\n",
    "    \"deepseek-ai/DeepSeek-V3.1\",\n",
    "    model_provider=\"together\",\n",
    "    api_key=settings.llm_api_key,\n",
    "    timeout=180,\n",
    "    max_retries=10,\n",
    ")\n",
    "\n",
    "evaluator_llm = LangchainLLMWrapper(llm)\n",
    "\n",
    "embeddings_client = OpenAI(\n",
    "    base_url=settings.embeddings_base_url,\n",
    "    api_key=settings.embeddings_api_key,\n",
    "    timeout=180,\n",
    "    max_retries=10,\n",
    ")\n",
    "\n",
    "embeddings = OpenAIEmbeddings(client=embeddings_client, model=settings.embeddings_model)\n",
    "\n",
    "run_config = RunConfig(\n",
    "    # max_retries=10,\n",
    "    # timeout=180,\n",
    "    max_workers=8,\n",
    ")\n",
    "\n",
    "result = evaluate(\n",
    "    dataset=evaluation_data,\n",
    "    metrics=[\n",
    "        LLMContextRecall(),\n",
    "        Faithfulness(),\n",
    "        FactualCorrectness(),\n",
    "        LLMContextPrecisionWithReference(),\n",
    "        # ResponseRelevancy(),\n",
    "    ],\n",
    "    llm=evaluator_llm,\n",
    "    embeddings=embeddings,\n",
    "    run_config=run_config,\n",
    ")\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a7c7f9",
   "metadata": {},
   "source": [
    "#### Faithfulness\n",
    "The Faithfulness metric measures how factually consistent a response is with the retrieved context. It ranges from 0 to 1, with higher scores indicating better consistency.\n",
    "A response is considered faithful if all its claims can be supported by the retrieved context. \n",
    "\n",
    "#### Context Precision\n",
    "Context Precision is a metric that evaluates the retriever’s ability to rank relevant chunks higher than irrelevant ones for a given query in the retrieved context. Specifically, it assesses the degree to which relevant chunks in the retrieved context are placed at the top of the ranking.\n",
    "It is calculated as the mean of the precision@k for each chunk in the context. Precision@k is the ratio of the number of relevant chunks at rank k to the total number of chunks at rank k.\n",
    "\n",
    "#### Context Recall\n",
    "Context Recall measures how many of the relevant documents (or pieces of information) were successfully retrieved. It focuses on not missing important results. Higher recall means fewer relevant documents were left out. In short, recall is about not missing anything important. Since it is about not missing anything, calculating context recall always requires a reference to compare against.\n",
    "\n",
    "#### Response Relevancy\n",
    "The ResponseRelevancy metric measures how relevant a response is to the user input. Higher scores indicate better alignment with the user input, while lower scores are given if the response is incomplete or includes redundant information. \n",
    "\n",
    "#### Factual Correctness\n",
    "FactualCorrectness is a metric that compares and evaluates the factual accuracy of the generated response with the reference. This metric is used to determine the extent to which the generated response aligns with the reference. The factual correctness score ranges from 0 to 1, with higher values indicating better performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d6eaae",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91efd992",
   "metadata": {},
   "source": [
    "{'context_recall': 0.8553, 'faithfulness': 0.8710, 'factual_correctness(mode=f1)': 0.3626, 'llm_context_precision_with_reference': 0.7379}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-workflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
