{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e0e2bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from app.workflow.rag_workflow import RAGWorkflow\n",
    "from app.core.config import settings as config\n",
    "from qdrant_client import QdrantClient, models\n",
    "from app.ingestion.pdf_loader.pdf_to_text import extract_text_from_pdf\n",
    "from app.ingestion.ingest import split_documents, generate_embeddings, store_documents\n",
    "from app.models.models import State\n",
    "from ragas import EvaluationDataset\n",
    "from ragas import evaluate\n",
    "from ragas.llms import LangchainLLMWrapper\n",
    "from langchain.chat_models import init_chat_model\n",
    "from ragas.metrics import LLMContextRecall, Faithfulness, FactualCorrectness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5babbcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"jamescalam/ai-arxiv2-chunks\", split=\"train[:10000]\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7b2442",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2461f449",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = QdrantClient(config.qdrant_url, api_key=config.qdrant_api_key)\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=\"ragas-test\",\n",
    "    vectors_config={\n",
    "        \"dense\": models.VectorParams(\n",
    "            size=config.embeddings_dim, distance=models.Distance.COSINE\n",
    "        )\n",
    "    },\n",
    "    sparse_vectors_config={\n",
    "        \"bm25\": models.SparseVectorParams(modifier=models.Modifier.IDF)\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7ef715",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from qdrant_client import models\n",
    "import uuid\n",
    "from openai import OpenAI\n",
    "import time\n",
    "\n",
    "# Initialize OpenAI client for batch embeddings (more efficient than VectorDB for large batches)\n",
    "embeddings_client = OpenAI(\n",
    "    api_key=config.embeddings_api_key, base_url=config.embeddings_base_url\n",
    ")\n",
    "\n",
    "# Convert dataset to pandas dataframe for easier manipulation\n",
    "data = dataset.to_pandas()\n",
    "\n",
    "batch_size = 50  # Reduced batch size\n",
    "embedding_batch_size = 10  # Much smaller batch for embeddings API to avoid 413 errors\n",
    "\n",
    "print(f\"Starting to upsert {len(data)} documents to Qdrant...\")\n",
    "\n",
    "for i in tqdm(range(0, len(data), batch_size)):\n",
    "    i_end = min(len(data), i + batch_size)\n",
    "    # get batch of data\n",
    "    batch = data.iloc[i:i_end]\n",
    "\n",
    "    # generate unique ids for each chunk\n",
    "    ids = [str(uuid.uuid4()) for _ in range(len(batch))]\n",
    "\n",
    "    # get text to embed\n",
    "    texts = [x[\"chunk\"] for _, x in batch.iterrows()]\n",
    "\n",
    "    # generate embeddings in smaller sub-batches for efficiency\n",
    "    embeddings = []\n",
    "    for j in range(0, len(texts), embedding_batch_size):\n",
    "        sub_texts = texts[j : j + embedding_batch_size]\n",
    "\n",
    "        try:\n",
    "            # Get embeddings for this sub-batch\n",
    "            response = embeddings_client.embeddings.create(\n",
    "                input=sub_texts, model=config.embeddings_model\n",
    "            )\n",
    "\n",
    "            # Extract embeddings from response\n",
    "            batch_embeddings = [emb.embedding for emb in response.data]\n",
    "            embeddings.extend(batch_embeddings)\n",
    "\n",
    "            # Small delay to avoid rate limiting\n",
    "            time.sleep(0.1)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting embeddings for batch {j}: {e}\")\n",
    "            # Use zero embeddings as fallback\n",
    "            batch_embeddings = [[0.0] * config.embeddings_dim for _ in sub_texts]\n",
    "            embeddings.extend(batch_embeddings)\n",
    "\n",
    "    # prepare points for Qdrant\n",
    "    points = []\n",
    "    for j, (text, embedding) in enumerate(zip(texts, embeddings)):\n",
    "        row = batch.iloc[j]\n",
    "        point = models.PointStruct(\n",
    "            id=ids[j],  # Use UUID or other unique identifier\n",
    "            payload={\n",
    "                \"text\": row[\"chunk\"],\n",
    "                \"source\": row[\"source\"],\n",
    "                \"title\": row[\"title\"],\n",
    "                \"metadata\": {\"source\": row[\"source\"], \"title\": row[\"title\"]},\n",
    "            },\n",
    "            vector={\"dense\": embedding},  # Embedding is already a list from API\n",
    "        )\n",
    "        points.append(point)\n",
    "\n",
    "    # upsert to Qdrant\n",
    "    try:\n",
    "        client.upsert(collection_name=\"ragas-test\", points=points)\n",
    "    except Exception as e:\n",
    "        print(f\"Error upserting batch {i}: {e}\")\n",
    "\n",
    "print(f\"Successfully upserted {len(data)} documents to Qdrant!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2692671",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the impact of encoding the input prompt on inference speed in generative inference?\"\n",
    "query_embedding_response = embeddings_client.embeddings.create(\n",
    "    input=query, model=config.embeddings_model\n",
    ")\n",
    "query_vector = query_embedding_response.data[0].embedding\n",
    "results = client.query_points(\n",
    "    collection_name=\"ragas-test\",\n",
    "    query=query_vector,\n",
    "    limit=3,\n",
    "    using=\"dense\",\n",
    ")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc8c51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CORRECTED: Test query with proper embedding extraction\n",
    "query = \"What is the impact of encoding the input prompt on inference speed in generative inference?\"\n",
    "\n",
    "# Get the embedding for the query\n",
    "query_embedding_response = embeddings_client.embeddings.create(\n",
    "    input=query, model=config.embeddings_model\n",
    ")\n",
    "\n",
    "# Extract the actual embedding vector from the response\n",
    "query_vector = query_embedding_response.data[0].embedding\n",
    "\n",
    "print(f\"Query vector type: {type(query_vector)}\")\n",
    "print(f\"Query vector length: {len(query_vector)}\")\n",
    "\n",
    "# Query Qdrant with the embedding vector\n",
    "results = client.query_points(\n",
    "    collection_name=\"ragas-test\",\n",
    "    query=query_vector,  # Use the actual vector, not the response object\n",
    "    limit=3,\n",
    "    using=\"dense\",\n",
    ")\n",
    "\n",
    "print(f\"\\nFound {len(results.points)} results:\")\n",
    "for i, result in enumerate(results.points):\n",
    "    print(f\"\\nResult {i+1} (Score: {result.score:.4f}):\")\n",
    "    print(f\"Text: {result.payload['text'][:200]}...\")\n",
    "    print(f\"Source: {result.payload['source']}\")\n",
    "    print(f\"Title: {result.payload['title']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3071fa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ragas_data = load_dataset(\"aurelio-ai/ai-arxiv2-ragas-mixtral\", split=\"train\")\n",
    "ragas_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4629a45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ragas_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31828ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the collection name issue and re-run evaluation\n",
    "print(\"=== CORRECTED RAG EVALUATION ===\")\n",
    "\n",
    "# Import needed classes\n",
    "from app.db.vector_db import VectorDB\n",
    "\n",
    "# Create a temporary config with the correct collection name\n",
    "import copy\n",
    "\n",
    "temp_config = copy.deepcopy(config)\n",
    "temp_config.qdrant_collection_name = \"ragas-test\"\n",
    "\n",
    "# Initialize workflow with corrected config\n",
    "corrected_workflow = RAGWorkflow()\n",
    "corrected_workflow.config = temp_config\n",
    "corrected_workflow.vector_db = VectorDB(\n",
    "    temp_config\n",
    ")  # Re-initialize with correct collection\n",
    "\n",
    "corrected_graph = corrected_workflow.build()\n",
    "\n",
    "# Create new DataFrame for corrected results\n",
    "df_corrected = pd.DataFrame(\n",
    "    {\"question\": [], \"contexts\": [], \"answer\": [], \"ground_truth\": []}\n",
    ")\n",
    "\n",
    "limit = 3  # Start even smaller for testing\n",
    "\n",
    "print(f\"Starting corrected evaluation of {limit} questions...\")\n",
    "\n",
    "for i, row in tqdm(enumerate(ragas_data), total=limit):\n",
    "    if i >= limit:\n",
    "        break\n",
    "\n",
    "    question = row[\"question\"]\n",
    "    ground_truth = row[\"ground_truth\"]\n",
    "\n",
    "    try:\n",
    "        # Use corrected RAG workflow\n",
    "        config_dict = {\"configurable\": {\"thread_id\": f\"corrected_eval_{i}\"}}\n",
    "        result = corrected_graph.invoke({\"question\": question}, config=config_dict)\n",
    "\n",
    "        answer = result[\"answer\"]\n",
    "\n",
    "        # Extract contexts from the retrieved documents\n",
    "        contexts = []\n",
    "        if result[\"context\"]:\n",
    "            for doc in result[\"context\"]:\n",
    "                if hasattr(doc, \"text\"):\n",
    "                    contexts.append(doc.text)\n",
    "                else:\n",
    "                    contexts.append(str(doc))\n",
    "\n",
    "        print(f\"\\nQ{i+1}: {question[:80]}...\")\n",
    "        print(f\"Found {len(contexts)} contexts\")\n",
    "        print(f\"A{i+1}: {answer[:80]}...\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing question {i+1}: {e}\")\n",
    "        answer = \"ERROR\"\n",
    "        contexts = []\n",
    "\n",
    "    # Add to DataFrame\n",
    "    new_row = pd.DataFrame(\n",
    "        {\n",
    "            \"question\": [question],\n",
    "            \"answer\": [answer],\n",
    "            \"contexts\": [contexts],\n",
    "            \"ground_truth\": [ground_truth],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    df_corrected = pd.concat([df_corrected, new_row], ignore_index=True)\n",
    "\n",
    "print(f\"\\nCorrected evaluation complete!\")\n",
    "print(\n",
    "    f\"Questions with contexts found: {len([c for c in df_corrected['contexts'] if len(c) > 0])}\"\n",
    ")\n",
    "print(\n",
    "    f\"Questions with no contexts: {len([c for c in df_corrected['contexts'] if len(c) == 0])}\"\n",
    ")\n",
    "\n",
    "df_corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0c002b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now evaluate using RAGAS metrics\n",
    "print(\"=== RAGAS EVALUATION ===\")\n",
    "\n",
    "# Prepare the dataset for RAGAS evaluation\n",
    "# Convert our DataFrame to the format expected by RAGAS\n",
    "evaluation_data = []\n",
    "\n",
    "for _, row in df_corrected.iterrows():\n",
    "    evaluation_data.append(\n",
    "        {\n",
    "            \"user_input\": row[\"question\"],\n",
    "            \"retrieved_contexts\": row[\"contexts\"],  # List of context strings\n",
    "            \"response\": row[\"answer\"],\n",
    "            \"reference\": (\n",
    "                row[\"ground_truth\"][0]\n",
    "                if isinstance(row[\"ground_truth\"], list)\n",
    "                else row[\"ground_truth\"]\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(f\"Prepared {len(evaluation_data)} samples for RAGAS evaluation\")\n",
    "\n",
    "# Create RAGAS evaluation dataset\n",
    "evaluation_dataset = EvaluationDataset.from_list(evaluation_data)\n",
    "\n",
    "# Initialize LLM for evaluation\n",
    "llm = init_chat_model(\n",
    "    \"meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\", model_provider=\"together\"\n",
    ")\n",
    "evaluator_llm = LangchainLLMWrapper(llm)\n",
    "\n",
    "print(\n",
    "    \"Starting RAGAS evaluation with metrics: LLMContextRecall, Faithfulness, FactualCorrectness\"\n",
    ")\n",
    "\n",
    "# Run RAGAS evaluation\n",
    "try:\n",
    "    result = evaluate(\n",
    "        dataset=evaluation_dataset,\n",
    "        metrics=[LLMContextRecall(), Faithfulness(), FactualCorrectness()],\n",
    "        llm=evaluator_llm,\n",
    "    )\n",
    "\n",
    "    print(\"\\n=== RAGAS EVALUATION RESULTS ===\")\n",
    "    print(result)\n",
    "\n",
    "    # Display metrics summary\n",
    "    if hasattr(result, \"scores\"):\n",
    "        print(\"\\n=== METRICS SUMMARY ===\")\n",
    "        for metric, score in result.scores.items():\n",
    "            print(f\"{metric}: {score:.4f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during RAGAS evaluation: {e}\")\n",
    "    print(\"This might be due to API rate limits or configuration issues.\")\n",
    "\n",
    "    # Show what we have so far\n",
    "    print(\"\\n=== EVALUATION DATA PREVIEW ===\")\n",
    "    for i, item in enumerate(evaluation_data):\n",
    "        print(f\"\\nSample {i+1}:\")\n",
    "        print(f\"Question: {item['user_input'][:100]}...\")\n",
    "        print(f\"Response: {item['response'][:100]}...\")\n",
    "        print(f\"Contexts: {len(item['retrieved_contexts'])} documents\")\n",
    "        print(f\"Reference: {item['reference'][:100]}...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-workflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
