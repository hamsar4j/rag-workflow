[
    {
        "user_input": "What is the impact of encoding the input prompt on inference speed in generative inference?",
        "retrieved_contexts": [
            "The generative inference workload consists of two phases: 1) encoding the input prompt and 2) generating tokens conditioned on that prompt. The key difference between these two phases is that prompt tokens are encoded in parallel (layer-by-layer), whereas the generation runs sequentially (token-by-token and layer-by-layer). In general, phase 1 works relatively well with existing Mixture- of-Experts algorithms, since each layer can only be loaded once for the entire prompt. In turn, when generating tokens, one must load layer once per each token generated. In practice, this means that inference speed is limited by how fast one can fetch parameters from system memory.\nBelow, we look for patterns in how the MoE model loads its experts and propose ways to exploit these patterns to speed up inference time.\n4To learn more about these methods, please refer to surveys such as Gholami et al. (2021); Liang et al. (2021) 5As opposed to running a processing a large batch of texts over many seconds, as in Sheng et al. (2023)\n3\nSelected experts for Mixtral-8x7B-Instruct woe 0 (top) and 15 ae =n a oa ao a âme: a n: ee Layer 15 expert # Layer 0 expert # MAUR STARR O However about |= and 4 training data owerful language model based trained Trans former f architecture",
            "For attention, we compare against the fastest implementation that we are aware of (FlashAttention-2 (Dao 2023)), with causal mask. Note that FlashAttention-2 with causal mask is about 1.7Ã faster than without causal mask, since approximately only half of the attention entries are computed. We use batch size of 1 and increase the sequence length from 29 = 512, 210 â 1ð¾, 211 â 2ð¾, up to 219 â 500ð¾ (some of the baselines run out of memory before reaching 500K). We use a model dimension of ð· = 1024 and state dimension ð = 16. We measure with BF16 inputs, which is the data type most commonly used for large scale training.\nEnd-to-end Inference. We measure the inference throughput of a Mamba 1.4B model and an untrained Mamba 6.9B model, against a standard Transformer (GPT3 architecture) at 1.3B and 6.7B size. We use the standard Transformer implementation in the Huggingface transformers library.",
            "D. Experimental Setup\nTo establish a robust experimental framework, our con- figuration includes prompt engineering, ensuring equilibrium between positive and negative examples, optimizing hyper- parameters, and configuring evaluators.\nPrompt engineering. The prompt engineering technique employed is âintent + instruction + 3-shot (explainable) prompting.â Intent delineates the LLMâs role, instruction out- lines the task for the LLM to execute, and the prompt incorpo- rates three examples to aid the LLMâs few-shot learning [1]. Furthermore, political content in examples is prohibited to ad- here to content policies from model service providers. Explain- able prompting entails not merely acquiring results but also eliciting the modelâs rationale behind its responses, regardless of the impact on evaluation speed and cost. In discriminative and selective evaluations, it is indiscernible whether the model is conjecturing the outcome or discerning the presence of hallucinations. Consequently, the use of explainable prompting enables the validation of the modelâs confidence through the analysis of experimental results.",
            "Below are our key findings:\n1. Generative AI models understand and can be influenced by emotional stimuli. Emo- tionPrompt and EmotionAttack demonstrate consistent effectiveness in semantic under- standing and reasoning tasks. As shown in Fig. 2(a), the textual and visual Emotion- Prompt improve the semantic understanding performance by 13.88% and 16.79%, re- spectively, and improve the reasoning performance by 11.76% and 15.13%, respectively. In contrast, the textual and visual EmotionAttack impair the semantic understanding per- formance by 10.13% and 53.14%, respectively, and decrease the reasoning performance by 12.30% and 37.53%, respectively.\n2. As for generation tasks, EmotionPrompt demonstrates consistent improvement in performance, truthfulness, and responsibility over most generative questions. As shown in Fig. 1(a), EmotionPrompt improves these metrics by 15%, 9%, and 9%, re- spectively. This verifies that emotional stimuli can also work in generative tasks. The detailed results can be found in Appendices B and C.",
            "Figure 1: Comparison of XOT versus other prompting paradigms.\nInput-Output (IO) Prompting (Fig. 1 (a)). The IO method is the most straightforward approach to instruct LLMs to address a problem without the provision of any intermediate thought processes.\nChain-of-thought (CoT) Wei et al. (2022) (Fig. 1 (b)). CoT decomposes problem-solving into a sequential chain of thoughts, allowing LLMs to approach complex problems step by step.\nSelf-consistency CoT (CoT-SC) Wang et al. (2023a) (Fig. 1 (c)). CoT-SC employs multiple in- stances of the CoT to generate multiple outputs from LLMs. It selects the the best results from multiple LLM outputs, offering more robust and consistent inference compared to the vanilla CoT.\nTree-of-thought (ToT) Yao et al. (2023) (Fig. 1 (d)). ToT organizes thoughts in a tree-like structure and utilizes search algorithms (e.g., Breadth-First Search, Depth-First Search) to expand the tree in pursuit of an optimal solution. However, thought evaluation in ToT relies on LLMs themselves, necessitating multiple costly and inefficient LLM inference calls.",
            "# 5 Conclusion and Future Work\nIn this work, we explore strategies for accelerating Mixture-of-Experts based language models on consumer hardware with limited GPU memory. We propose a MoE-centric approach to offloading\n7\nand explore how mixed quantization affects perplexity and performance on language understanding tasks. We evaluate the proposed strategies and show that they produce a significant increase in generation speed compared to naÂ¨ve approaches on consumer-grade hardware, including free-tier Google Colab.\nOur method provides a practical solution for inferencing large MoE language models on resource- constricted hardware, enabling broader access to these powerful models for research and development. As future work, we plan to explore further offloading strategies, based on speculative expert predic- tion.\n# Acknowledgements\nAuthors would like to acknowledge mobicham@ for helpful discussions on Mixtral quantization.\n# References\nAminabadi, R. Y., Rajbhandari, S., Awan, A. A., Li, C., Li, D., Zheng, E., Ruwase, O., Smith, S., Zhang, M., Rasley, J., and He, Y. Deepspeed-inference: Enabling efficient inference of transformer models at unprecedented scale. In Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis, SC â22. IEEE Press, 2022. ISBN 9784665454445.",
            "Model Llama 2 ChatGPT GPT-4 Avg Setting Instruction Induction (Zero-shot) 0.3409 Original+Zero-shot-CoT 0.3753 0.3778 0.4070 Original Original+Ours (avg) Original+Ours (max) 0.7581 0.7636 0.7826 0.8068 0.7858 0.5773 0.8018 0.8178 0.6283 0.5721 0.6541 0.6772 Setting Instruction Induction (Few-shot) 0.0590 Original+Zero-shot-CoT 0.0769 0.0922 0.1026 Original Original+Ours (avg) Original+Ours (max) 0.7750 0.7887 0.7934 0.8105 0.8235 0.7003 0.8447 0.8660 0.5525 0.5220 0.5768 0.5930 Setting Big-Bench (Zero-shot) 1.3332 Original+Zero-shot-CoT 1.9575 2.8094 3.4200 Original Original+Ours (avg) Original+Ours (max) 18.0068 18.448 20.9779 21.8116 17.4984 21.6865 19.7243 22.8790 12.28 14.03 14.50 16.04",
            "Augmenting LLMs with RL. Enhancing the capabilities of LLMs through the incorporation of ex- ternal models constitutes an effective strategy for improving their overall quality. The foundational work of ChatGPT Ouyang et al. (2022) leverages RL from human feedback to enable LLMs to ad- here to human guidance, resulting in a substantial enhancement of their truthfulness and a reduction in toxic output. Similarly, GLAM Carta et al. (2023) employs online RL to establish alignment between LLMsâ knowledge and the broader environment, thus enhancing their ability to generalize to new objects or tasks and ultimately improving their performance. Additionally, an interesting study in Yuan et al. (2023) utilizes RL to acquire basic skills in the context of Minecraft Cipollone et al. (2014), with subsequent high-level planning carried out by LLMs. This approach demon- strates promising performance across various Minecraft tasks. Furthermore, the ESPER framework Yu et al. (2023) harnesses RL to achieve alignment between multimodal inputs and language model generations, all without the need for direct supervision. This empowers LLMs to effectively tackle multimodal tasks and provides robust visual alignment and rapid inference speeds while preserving the textual domain. Collectively, these research endeavors underscore the considerable potential in augmenting LLMs with reinforcement learning techniques.\n14\n# 6 DISCUSSION",
            "Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601, 2023.\nYoungjae Yu, Jiwan Chung, Heeseung Yun, Jack Hessel, Jae Sung Park, Ximing Lu, Rowan Zellers, Prithviraj Ammanabrolu, Ronan Le Bras, Gunhee Kim, et al. Fusing pre-trained language mod- els with multimodal prompts through reinforcement learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10845â10856, 2023.\nHaoqi Yuan, Chi Zhang, Hongcheng Wang, Feiyang Xie, Penglin Cai, Hao Dong, and Zongqing Lu. Plan4mc: Skill reinforcement learning and planning for open-world minecraft tasks. arXiv preprint arXiv:2303.16563, 2023.\nZhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. Automatic chain of thought prompting in large language models. arXiv preprint arXiv:2210.03493, 2022.",
            "These neural networks and their training algorithms may be complex, and the scope of their impact broad and wide, but nonetheless they are simply functions in a high-dimensional space. A trained neural network takes a vector as input, crunches and transforms it in various ways, and produces another vector, often in some other space. An image may thereby be turned into a vector, a song into a sequence of vectors, and a social network as a structured collection of vectors. It seems as though much of human knowledge, or at least what is expressed as text, audio, image, and video, has a vector representation in one form or another.\nIt should be noted that representing data as vectors is not unique to neural networks and deep learning. In fact, long before learnt vector representations of pieces of dataâwhat is commonly known as âembeddingsââcame along, data was often encoded as hand-crafted feature vectors. Each feature quanti- fied into continuous or discrete values some facet of the data that was deemed relevant to a particular task (such as classification or regression). Vectors of that form, too, reflect our understanding of a real-world object or concept."
        ],
        "response": "Encoding the input prompt in parallel during the first phase of generative inference improves speed by loading each layer once for the entire prompt, whereas token generation runs sequentially, loading layers per token, which limits speed due to memory fetch constraints [http://arxiv.org/pdf/2312.17238].",
        "reference": "The encoding of the input prompt has an impact on inference speed in generative inference. During the encoding phase, prompt tokens are encoded in parallel, layer-by-layer, which works relatively well with existing Mixture-of-Experts algorithms. Each layer only needs to be loaded once for the entire prompt. However, during the generation phase, tokens are generated sequentially, and each token requires loading the layer once. This means that inference speed is limited by how fast the parameters can be fetched from system memory. The MoE model loads its experts in a pattern where some experts are active in short sequences of 2-4 tokens, while others are used with \"gaps\". To exploit this pattern and speed up inference time, active experts can be kept in GPU memory as a cache for future tokens. If the same experts are activated again in the future, they will be available instantaneously. However, even with caching, most of the inference time is still spent waiting for the next expert to be loaded because MoE offloading cannot effectively overlap expert loading with computation like dense models can."
    },
    {
        "user_input": "How does generating tokens affect the inference speed in generative inference?",
        "retrieved_contexts": [
            "The generative inference workload consists of two phases: 1) encoding the input prompt and 2) generating tokens conditioned on that prompt. The key difference between these two phases is that prompt tokens are encoded in parallel (layer-by-layer), whereas the generation runs sequentially (token-by-token and layer-by-layer). In general, phase 1 works relatively well with existing Mixture- of-Experts algorithms, since each layer can only be loaded once for the entire prompt. In turn, when generating tokens, one must load layer once per each token generated. In practice, this means that inference speed is limited by how fast one can fetch parameters from system memory.\nBelow, we look for patterns in how the MoE model loads its experts and propose ways to exploit these patterns to speed up inference time.\n4To learn more about these methods, please refer to surveys such as Gholami et al. (2021); Liang et al. (2021) 5As opposed to running a processing a large batch of texts over many seconds, as in Sheng et al. (2023)\n3\nSelected experts for Mixtral-8x7B-Instruct woe 0 (top) and 15 ae =n a oa ao a âme: a n: ee Layer 15 expert # Layer 0 expert # MAUR STARR O However about |= and 4 training data owerful language model based trained Trans former f architecture",
            "# 2.4 Hardware Setup\nWhile our analysis is not specific to any hardware setup, we target the hardware specifications of cheap / free-tier cloud instances Google (2023) and the upper half of gaming computers Steam (2023): i) enough system memory to hold model parameters, ii) a GPU with 11-16GB VRAM and iii) host-to-device communication at 8-16GB/s (PCIe Gen.3). If we examine popular open-access MoE models (Mixtral-8x7B and switch-c-2048), we find that all non-experts can fit a fraction of available GPU memory. In turn, the experts that constitute vast majority of model parameters do not fit even with quantization. Finally, even if we could fit the model parameters in memory, running generative inference requires additional memory for layer activations and past attention keys & values.\n# 3 Method\nIn this work, we aim to systematically find the optimal way to inference modern Mixture-of-Experts LLMs on desktop or low-end cloud instances. More specifically, we focus on the task of generating tokens interactively, i.e. generate multiple tokens per second at batch size 15.",
            "# 5 Conclusion and Future Work\nIn this work, we explore strategies for accelerating Mixture-of-Experts based language models on consumer hardware with limited GPU memory. We propose a MoE-centric approach to offloading\n7\nand explore how mixed quantization affects perplexity and performance on language understanding tasks. We evaluate the proposed strategies and show that they produce a significant increase in generation speed compared to naÂ¨ve approaches on consumer-grade hardware, including free-tier Google Colab.\nOur method provides a practical solution for inferencing large MoE language models on resource- constricted hardware, enabling broader access to these powerful models for research and development. As future work, we plan to explore further offloading strategies, based on speculative expert predic- tion.\n# Acknowledgements\nAuthors would like to acknowledge mobicham@ for helpful discussions on Mixtral quantization.\n# References\nAminabadi, R. Y., Rajbhandari, S., Awan, A. A., Li, C., Li, D., Zheng, E., Ruwase, O., Smith, S., Zhang, M., Rasley, J., and He, Y. Deepspeed-inference: Enabling efficient inference of transformer models at unprecedented scale. In Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis, SC â22. IEEE Press, 2022. ISBN 9784665454445.",
            "This technique works particularly well when processing large batches of data, during train- ing Pudipeddi et al. (2020); Ren et al. (2021) or large-batch non-interactive inference Aminabadi et al. (2022); Sheng et al. (2023), where each layer processes a lot of tokens each time the layer is loaded from RAM. In turn, when doing interactive inference (e.g. as a chat assistants), offloading works significantly slower than on-device inference. This is because interactive inference generates tokens autoregressively, from left to right. This way, the inference system processes one or few tokens at a time, and therefore spends most of the time waiting for next layerâs parameters to be loaded.\n# 2.4 Hardware Setup",
            "However, both discriminative and selective evaluations en- counter a substantial challenge. They are predicated on the assumption that âLLMsâs capacity to produce reliable text is contingent upon their discernment between hallucinated and non-hallucinated content.â These methods do not simulate the evaluation of the modelâs output for hallucinations. Conse- quently, generative evaluation is crucial as it directly evaluates the presence of hallucinations in the text generated by the LLM. Specifically, the LLM under evaluation is provided with an initial text and is then tasked with generating a continuation. Subsequently, various reference-based techniques are utilized to determine if the continuation includes hallucinations. How- ever, the challenge arises from the fact that it is not feasible to automatically and accurately ascertain if newly generated text is hallucinated; if it were, annotated datasets would be redun- dant. In scenarios of unrestrained text generation, this issue becomes increasingly complex. This complexity stems from the fact that text generated without constraints may introduce a multitude of entities and facts absent in the reference material, complicating the verification of their accuracy. Despite these hurdles, generative evaluation continues to be a predominant strategy in Natural Language Generation (NLG) tasks [26].",
            "We illustrate an example of how LRU cache saves experts in Figure 1 (see caption). LRU is a very simple strategy that does not consider factors like expert activation frequencies, varying cache size between MoE layers, or any sequential patterns in expert activation. However, we found that even this simple strategy can significantly speed up inference for modern Mixture-of-Experts models such as Mixtral-8x7B (see Section 4 for detailed evaluation).\n# 3.2 Speculative Expert Loading\nWhile LRU caching can reduce the average expert loading time, most of the inference time is still spent waiting for the next expert to be loaded. The reason behind this is that, unlike with dense models, MoE offloading cannot effectively overlap expert loading with computation. To understand this problem, let us zoom into the process of generating a single token, layer-by-layer. The full compute workload starts by embedding the previous token via look-up, then alternates between running self-attention and MLP for each transformer block in the model. Finally, the outputs from the last transformer block are used to predict next token logits with a linear projection.",
            "Below are our key findings:\n1. Generative AI models understand and can be influenced by emotional stimuli. Emo- tionPrompt and EmotionAttack demonstrate consistent effectiveness in semantic under- standing and reasoning tasks. As shown in Fig. 2(a), the textual and visual Emotion- Prompt improve the semantic understanding performance by 13.88% and 16.79%, re- spectively, and improve the reasoning performance by 11.76% and 15.13%, respectively. In contrast, the textual and visual EmotionAttack impair the semantic understanding per- formance by 10.13% and 53.14%, respectively, and decrease the reasoning performance by 12.30% and 37.53%, respectively.\n2. As for generation tasks, EmotionPrompt demonstrates consistent improvement in performance, truthfulness, and responsibility over most generative questions. As shown in Fig. 1(a), EmotionPrompt improves these metrics by 15%, 9%, and 9%, re- spectively. This verifies that emotional stimuli can also work in generative tasks. The detailed results can be found in Appendices B and C.",
            "Code: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/mixtral-of-experts/\n# Introduction\nIn this paper, we present Mixtral 8x7B, a sparse mixture of experts model (SMoE) with open weights, licensed under Apache 2.0. Mixtral outperforms Llama 2 70B and GPT-3.5 on most benchmarks. As it only uses a subset of its parameters for every token, Mixtral allows faster inference speed at low batch-sizes, and higher throughput at large batch-sizes.\nMixtral is a sparse mixture-of-experts network. It is a decoder-only model where the feedforward block picks from a set of 8 distinct groups of parameters. At every layer, for every token, a router network chooses two of these groups (the âexpertsâ) to process the token and combine their output additively. This technique increases the number of parameters of a model while controlling cost and latency, as the model only uses a fraction of the total set of parameters per token.\nMixtral is pretrained with multilingual data using a context size of 32k tokens. It either matches or exceeds the performance of Llama 2 70B and GPT-3.5, over several benchmarks. In particular,\nMixture of Experts Layer i gating inputs af outputs router expert",
            "Our approach engenders a more heterogeneous generation of hallucinations, mitigating the bias that may arise from the use of a single model and promoting equity within the dataset. This is due to the varying architectures and training corpora inherent to different LLMs. Furthermore, we have adopted an unconstrained generation methodology for the continuation of natural language content. This entails directly inputting the text to be continued into the model without any restrictive prompt thereby obtaining organic results. For each input example, we concurrently generate five candidate continuations. To maintain consistency across all models, we employ uniform parameter settings, with a temperature coefficient set at 1.0 and max new tokens limited to 1024.\n# C. Hallucination Ranking\nGiven the unconstrained nature of our generation paradigm, the task of discerning whether the generated content is indeed hallucinated presents a significant challenge. Upon generating the continuations, a straightforward reliance on human verifi- cation is infeasible. An exclusive dependence on human anno- tation would incur substantial costs and may not be sustainable at scale, whereas a purely machine-based approach, such as utilizing GPT4, could potentially yield less accurate results.",
            "â¢ When the context length ð¿ is greater than 217, we turn each segment into two samples, one that begins with the prescribed segment and one that ends with the prescribed segment. Thus each epoch has 2ð items and 2ðð¿\n32\ntokens per epoch. For example, at sequence length 218 = 262144 there are 4Ã as many tokens as the default, and at sequence length 220 there are 16Ã as many tokens.\nOther training details generally follow the same protocol as our language modeling experiments (Appendix E.2). For example, we use the AdamW with (ð½1, ð½2) = (0.9, 0.95), no dropout, weight decay 0.1. We use a cosine learning rate scheduler with linear warmup for 10% of total steps.\n# E.3.2 Scaling: Model Size Details\nModels. The models we consider are: â¢ Transformer++: a Transformer with improved architecture, notably the usage of RoPE positional encodings (Su et al. 2021). Informally, we found these to be noticeably better than vanilla positional encodings from (Vaswani et al. 2017)."
        ],
        "response": "Generating tokens sequentially in phase 2 limits inference speed because each token requires loading layers from system memory, unlike parallel encoding in phase 1 [source: http://arxiv.org/pdf/2312.17238]. This sequential loading causes delays as the system waits for parameters to be fetched [source: http://arxiv.org/pdf/2312.17238]. In interactive inference, the autoregressive nature of token generation exacerbates this issue, leading to slower performance compared to non-interactive scenarios [source: http://arxiv.org/pdf/2312.17238]. The inability to overlap expert loading with computation further reduces efficiency [source: http://arxiv.org/pdf/2312.17238]. Strategies like LRU caching can mitigate but not fully resolve these bottlenecks [source: http://arxiv.org/pdf/2312.17238].",
        "reference": "Generating tokens affects the inference speed in generative inference by slowing it down. In interactive inference, where tokens are generated autoregressively from left to right, the inference system processes one or few tokens at a time, resulting in a longer waiting time for the next layer's parameters to be loaded. Additionally, the inference speed is limited by how fast parameters can be fetched from system memory. However, by keeping active experts in GPU memory as a cache, the inference time can be sped up if the same experts are activated again in the future. Overall, while caching can reduce the average expert loading time, most of the inference time is still spent waiting for the next expert to be loaded."
    },
    {
        "user_input": "How does the architecture of Mixtral 8x7B differ from Mistral 7B in terms of feedforward blocks and active parameters used during inference?",
        "retrieved_contexts": [
            "Abstract\nWe introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine- tuned to follow instructions, Mixtral 8x7B â Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â chat model on human bench- marks. Both the base and instruct models are released under the Apache 2.0 license.\nCode: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/mixtral-of-experts/",
            "We also present Mixtral 8x7B â Instruct, a chat model fine-tuned to follow instructions using supervised fine-tuning and Direct Preference Optimization [25]. Its performance notably surpasses that of GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â chat model on human evaluation benchmarks. Mixtral â Instruct also demonstrates reduced biases, and a more balanced sentiment profile in benchmarks such as BBQ, and BOLD. We release both Mixtral 8x7B and Mixtral 8x7B â Instruct under the Apache 2.0 license1, free for academic and commercial usage, ensuring broad accessibility and potential for diverse applications. To enable the community to run Mixtral with a fully open-source stack, we submitted changes to the vLLM project, which integrates Megablocks CUDA kernels for efficient inference. Skypilot also allows the deployment of vLLM endpoints on any instance in the cloud.\n# 2 Architectural details\nMixtral is based on a transformer architecture [31] and uses the same modifications as described in [18], with the notable exceptions that Mix- tral supports a fully dense context length of 32k tokens, and the feed- forward blocks are replaced by Mixture-of-Expert layers (Section 2.1). The model architecture parameters are summarized in Table 1. Parameter Value",
            "Table 2: Comparison of Mixtral with Llama. Mixtral outperforms or matches Llama 2 70B performance on almost all popular benchmarks while using 5x fewer active parameters during inference.\n70 Mixtral 8x7B. âMixtral 8x7B Mixtral 8x7B 355 =o = Es & E60! Mistral 78 % 2681 Mistral 78 3 3 s0 5 = A % 66 50 g 4 45 64 78 138 348708 78 138 348708 78 138 348 70B S66 Mixtral 8x7B 50 Mixtral 8x7B 5 = 564 340 g al Mistral 78 ee Mistral 78 3 5 Â§ 30 5 eo â= Mistral Â° 20 âe LlaMA2 78 (138 348 70B 7B (138 348 708 7B Â«13B 34B 708 Active Params Active Params Active Params\nFigure 3: Results on MMLU, commonsense reasoning, world knowledge and reading comprehension, math and code for Mistral (7B/8x7B) vs Llama 2 (7B/13B/70B). Mixtral largely outperforms Llama 2 70B on all benchmarks, except on reading comprehension benchmarks while using 5x lower active parameters. It is also vastly superior to Llama 2 70B on code and math.",
            "Detailed results for Mixtral, Mistral 7B and Llama 2 7B/13B/70B and Llama 1 34B2 are reported in Table 2. Figure 2 compares the performance of Mixtral with the Llama models in different categories. Mixtral surpasses Llama 2 70B across most metrics. In particular, Mixtral displays a superior performance in code and mathematics benchmarks.\nSize and Efficiency. We compare our performance to the Llama 2 family, aiming to understand Mixtral modelsâ efficiency in the cost-performance spectrum (see Figure 3). As a sparse Mixture- of-Experts model, Mixtral only uses 13B active parameters for each token. With 5x lower active parameters, Mixtral is able to outperform Llama 2 70B across most categories.\nNote that this analysis focuses on the active parameter count (see Section 2.1), which is directly proportional to the inference compute cost, but does not consider the memory costs and hardware utilization. The memory costs for serving Mixtral are proportional to its sparse parameter count, 47B, which is still smaller than Llama 2 70B. As for device utilization, we note that the SMoEs layer introduces additional overhead due to the routing mechanism and due to the increased memory loads when running more than one expert per device. They are more suitable for batched workloads where one can reach a good degree of arithmetic intensity.",
            "Code: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/mixtral-of-experts/\n# Introduction\nIn this paper, we present Mixtral 8x7B, a sparse mixture of experts model (SMoE) with open weights, licensed under Apache 2.0. Mixtral outperforms Llama 2 70B and GPT-3.5 on most benchmarks. As it only uses a subset of its parameters for every token, Mixtral allows faster inference speed at low batch-sizes, and higher throughput at large batch-sizes.\nMixtral is a sparse mixture-of-experts network. It is a decoder-only model where the feedforward block picks from a set of 8 distinct groups of parameters. At every layer, for every token, a router network chooses two of these groups (the âexpertsâ) to process the token and combine their output additively. This technique increases the number of parameters of a model while controlling cost and latency, as the model only uses a fraction of the total set of parameters per token.\nMixtral is pretrained with multilingual data using a context size of 32k tokens. It either matches or exceeds the performance of Llama 2 70B and GPT-3.5, over several benchmarks. In particular,\nMixture of Experts Layer i gating inputs af outputs router expert",
            "# 6 Conclusion\nIn this paper, we introduced Mixtral 8x7B, the first mixture-of-experts network to reach a state-of-the- art performance among open-source models. Mixtral 8x7B Instruct outperforms Claude-2.1, Gem- ini Pro, and GPT-3.5 Turbo on human evaluation benchmarks. Because it only uses two experts at each time step, Mixtral only uses 13B active parameters per token while outperforming the previous best model using 70B parameters per token (Llama 2 70B). We are making our trained and fine-tuned mod- els publicly available under the Apache 2.0 license. By sharing our models, we aim to facilitate the de- velopment of new techniques and applications that can benefit a wide range of industries and domains.",
            "Since then, several lines of work explore Mixture-of-Experts with Transformer-based language models for machine translation Lepikhin et al. (2020), masked language modeling Fedus et al. (2021), general-purpose LLMs Du et al. (2022) and others. Most of these models follow traditional (dense) Transformer architecture for embeddings and attention layers, and only use Mixture for the feedforward (MLP) blocks and use a linear token-level gating function. A common observation across most of these works is that MoE models are cheaper to train and inference Fedus et al. (2021); Lepikhin et al. (2020), but require more parameters than a dense model with equivalent perplexity. Pre-trained Mixture-of-Experts LLMs have been openly available for over a year3. However, these models seem to have gained less traction than equivalent dense models, arguable because their sheer model size (over a trillion parameters) makes them difficult to use. Most recently, Mistral AI released a family of sparse Mixture of Experts models called Mixtral-8x7B with near state-of-the-art performance Mixtral AI team (2023). This model has already inspired several follow-up works and practical applications, but it still requires a high-end GPU accelerator.\n# 2.2 Post-training Quantization of LLMs",
            "# 3.1 Multilingual benchmarks\nCompared to Mistral 7B, we significantly upsample the proportion of multilingual data during pretraining. The extra capacity allows Mixtral to perform well on multilingual benchmarks while maintaining a high accuracy in English. In particular, Mixtral significantly outperforms Llama 2 70B in French, German, Spanish, and Italian, as shown in Table 4.\nActive Params French Arc-c HellaS MMLU German Arc-c HellaS MMLU Spanish Arc-c HellaS MMLU Italian Arc-c HellaS MMLU 33B 70B 13B 42.9% 65.4% 49.0% 39.3% 68.1% 49.9% 49.9% 72.5% 64.3% 49.4% 70.9% 65.1% 58.2% 77.4% 70.9% 54.3% 73.0% 71.5% 55.4% 77.6% 72.5% 52.8% 75.1% 70.9% 41.1% 63.3% 48.7% 47.3% 68.7% 64.2% 45.7% 69.8% 52.3% 50.5% 74.5% 66.0%",
            "Comparison with Llama 2 70B and GPT-3.5. In Table 3, we report the performance of Mixtral 8x7B compared to Llama 2 70B and GPT-3.5. We observe that Mixtral performs similarly or above the two other models. On MMLU, Mixtral obtains a better performance, despite its significantly smaller capacity (47B tokens compared to 70B). For MT Bench, we report the performance of the latest GPT-3.5-Turbo model available, gpt-3.5-turbo-1106.\n2Since Llama 2 34B was not open-sourced, we report results for Llama 1 34B.\n4",
            "We illustrate an example of how LRU cache saves experts in Figure 1 (see caption). LRU is a very simple strategy that does not consider factors like expert activation frequencies, varying cache size between MoE layers, or any sequential patterns in expert activation. However, we found that even this simple strategy can significantly speed up inference for modern Mixture-of-Experts models such as Mixtral-8x7B (see Section 4 for detailed evaluation).\n# 3.2 Speculative Expert Loading\nWhile LRU caching can reduce the average expert loading time, most of the inference time is still spent waiting for the next expert to be loaded. The reason behind this is that, unlike with dense models, MoE offloading cannot effectively overlap expert loading with computation. To understand this problem, let us zoom into the process of generating a single token, layer-by-layer. The full compute workload starts by embedding the previous token via look-up, then alternates between running self-attention and MLP for each transformer block in the model. Finally, the outputs from the last transformer block are used to predict next token logits with a linear projection."
        ],
        "response": "Mixtral 8x7B replaces the single feedforward block in Mistral 7B with 8 experts per layer, and only 2 experts are selected per token by a router network. During inference, each token uses 13B active parameters out of 47B total parameters. This sparse mixture design allows Mixtral to outperform larger models like Llama 2 70B while using 5x fewer active parameters. [source: http://arxiv.org/pdf/2401.04088]",
        "reference": "The architecture of Mixtral 8x7B differs from Mistral 7B in terms of feedforward blocks and active parameters used during inference. Mixtral 8x7B has 8 feedforward blocks (experts) in each layer, while Mistral 7B does not specify the number of feedforward blocks. Additionally, Mixtral 8x7B uses 13B active parameters during inference, while the number of active parameters for Mistral 7B is not mentioned."
    },
    {
        "user_input": "When is offloading used on the A100 server for accelerating MoE-based language models?",
        "retrieved_contexts": [
            "1When deployed in 16-bit precision, Falcon-180B needs approximately 360GB, while LLaMA-2 70B requires 140GB of combined accelerator memory.\nIn this work, we systematically develop techniques for running large MoE language models with limited GPU memory. Our main objective is inferencing (generating tokens) with Mixtral-8x7B- Instruct â a MoE-based chat assistant â on a desktop-grade hardware where only a fraction of experts fit into the accelerator memory. To that end:\nwe observe how MoE language model accesses its experts between tokens, and find several regularities: i) some experts are reused between adjacent tokens and ii) the model hidden states of early layers already âknowâ which experts are to be used at subsequent layers. â¢ we design a MoE-specific offloading strategy that takes advantage of these regularities: i) it uses LRU cache to significantly reduces GPU-RAM communication, leading to faster generation and ii) it guesses which experts are needed ahead of time to better overlap expert loading with computation.\nâ¢ we consider the specific scenario of running Mixtral-8x7B-Instruct on a T4, RTX 3060 and RTX 3080 Mobile and develop a practical combination of mixed quantization and the proposed offloading algorithm to run this model interactively at 2-3 tokens per second depending on the hardware. The source code with our implementation is available online2",
            "We consider four hardware configurations: a free-tier Colab instance with a T4 GPU (16GB VRAM, PCIe Gen.3), a past generation gaming laptop with RTX 3080 Mobile (16GB, PCIe Gen.4), a mid- range gaming desktop with RTX 3060 (12GB, PCIe Gen.3) and a high-end data-center server with A100-80GB-SXM. Note that the A100 server could run the model without offloading. We use offloading on A100 mostly to provide a reference for other setups. Finally, when evaluating 3-bit models, we use a cloud T4 from Microsoft Azure because the free-tier colab instances did not have enough RAM for this specific configuration. We use k = 2 for RTX 3060 and k = 4 for all other GPUs.\nAs shown in Table 2, all evaluated setups can generate 2-4 tokens per second with the full algorithm. Using pre-loading appears to be most beneficial on RTX 3060, possibly due to lower LRU cache size. Cursiously, RTX 3060 (desktop) performs nearly equally with a much higher end 3080 Mobile. We attribute this to the fact that both GPUs are still bottlenecked by host-to-device bandwidth, limited by the PCIe architecture. Finally, all schemes significantly outperform naive offloading that loads the entire MoE layer.\n# 5 Conclusion and Future Work",
            "# Denis Mazur Moscow Institute of Physics and Technology Yandex Researchcore denismazur8@gmail.com\n# Abstract\nWith the widespread adoption of Large Language Models (LLMs), many deep learning practitioners are looking for strategies of running these models more efficiently. One such strategy is to use sparse Mixture-of-Experts (MoE) â a type of model architectures where only a fraction of model layers are active for any given input. This property allows MoE-based language models to generate tokens faster than their âdenseâ counterparts, but it also increases model size due to having multiple âexpertsâ. Unfortunately, this makes state-of-the-art MoE language models difficult to run without high-end GPUs. In this work, we study the problem of running large MoE language models on consumer hardware with limited accelerator memory. We build upon parameter offloading algorithms and propose a novel strategy that accelerates offloading by taking advantage of innate properties of MoE LLMs. Using this strategy, we build can run Mixtral-8x7B with mixed quantization on desktop hardware and free-tier Google Colab instances.\n# Introduction",
            "# 5 Conclusion and Future Work\nIn this work, we explore strategies for accelerating Mixture-of-Experts based language models on consumer hardware with limited GPU memory. We propose a MoE-centric approach to offloading\n7\nand explore how mixed quantization affects perplexity and performance on language understanding tasks. We evaluate the proposed strategies and show that they produce a significant increase in generation speed compared to naÂ¨ve approaches on consumer-grade hardware, including free-tier Google Colab.\nOur method provides a practical solution for inferencing large MoE language models on resource- constricted hardware, enabling broader access to these powerful models for research and development. As future work, we plan to explore further offloading strategies, based on speculative expert predic- tion.\n# Acknowledgements\nAuthors would like to acknowledge mobicham@ for helpful discussions on Mixtral quantization.\n# References\nAminabadi, R. Y., Rajbhandari, S., Awan, A. A., Li, C., Li, D., Zheng, E., Ruwase, O., Smith, S., Zhang, M., Rasley, J., and He, Y. Deepspeed-inference: Enabling efficient inference of transformer models at unprecedented scale. In Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis, SC â22. IEEE Press, 2022. ISBN 9784665454445.",
            "For regular (dense) models, this architecture allows for efficient offloading schedule that pre-loads the next transformer layer ahead of time, while the previous layer is still running. Unfortunately, this schedule is no longer possible for Mixture-of-Experts models, where MoE MLP layers choose which experts to load just-in-time for computation. This is because the system cannot pre-fetch the next layer until it learns which experts should be loaded. Modern open-access MoE language models choose active experts using the final outputs of the previous layer, which means they cannot be pre-fetched them in parallel with previous layer. While it is not possible6 to pre-reliably prefetch the next set of experts ahead of time, the system could still try to guess the likely next experts and load them speculatively, while processing the previous layer. It the guess is correct, it will speed up the next layer inference; if not, it can load the actual next layerâs experts later. In other words, this type of speculative loading does not change the final model predictions, but may reduce latency if the guess is accurate enough.\n6More specifically, not possible without changing the model architecture, which would require re-training\n4",
            "To speed up offloading in practice, we allocate all expert parameters in a contiguous memory buffer that can be moved as a single host-to-device copy. For host-side (RAM) experts, we pin8 this memory buffer for faster communication. Our implementation additionally allocates b=4 on-device buffers used to copy and prefetch experts asynchronously, without modifying existing experts. These buffers are shared between all MoE layers to reduce memory footprint. Overall, the system requires num_layers Ã num_experts expert memory buffers split between host and device memory and b=4 temporary buffers, the size of each buffer being equal to a single expert.\n# 4 Experiments\nIn this section, we verify our earlier hypotheses about MoE behavior and benchmark the inference latency in different conditions. We focus our evaluations on Mixtral-8x7B and Mixtral-8x7B-Instruct models since they represent the current state of the art among open-access MoE models. We organize this section as follows: Section 4.1 measures the effectiveness of expert caching and pre-loading in isolation, Section 4.2 compares different model compression algorithms and verifies our hypotheses from Section 3.3. Finally, Section 4.3 measures the inference latency in several hardware setups.",
            "Since then, several lines of work explore Mixture-of-Experts with Transformer-based language models for machine translation Lepikhin et al. (2020), masked language modeling Fedus et al. (2021), general-purpose LLMs Du et al. (2022) and others. Most of these models follow traditional (dense) Transformer architecture for embeddings and attention layers, and only use Mixture for the feedforward (MLP) blocks and use a linear token-level gating function. A common observation across most of these works is that MoE models are cheaper to train and inference Fedus et al. (2021); Lepikhin et al. (2020), but require more parameters than a dense model with equivalent perplexity. Pre-trained Mixture-of-Experts LLMs have been openly available for over a year3. However, these models seem to have gained less traction than equivalent dense models, arguable because their sheer model size (over a trillion parameters) makes them difficult to use. Most recently, Mistral AI released a family of sparse Mixture of Experts models called Mixtral-8x7B with near state-of-the-art performance Mixtral AI team (2023). This model has already inspired several follow-up works and practical applications, but it still requires a high-end GPU accelerator.\n# 2.2 Post-training Quantization of LLMs",
            "# 4.3 Practical offloading performance\nFinally we evaluate the performance of the Mixtral8x7B-Instruct model using the offloading tech- niquesproposed throughout this report. Based on the perplexity evaluations from the previous section, we chose 4-bit HQQ quantization for the shared attention layers and 2- or 3-bit quantization for experts. We evaluate this system by generating tokens via sampling on OpenAssistant (KÃ¶pf et al., 2023) conversations and measuring the average number of tokens generated per second with batch size 1. For this evaluation, we always sample proportionally to the predicted probabilities, i.e. without temperature or nucleus sampling.",
            "Table 1: Perplexity and model size evaluation of Mixtral-8x7B with different quantization for shared attention (Attn quant) and experts (Experts quant) layers. For comprarison, a Mistral-7B 4-bit quantized model has Wiki2 perplexity 5.03, C4 perplexity 7.56 and MMLU score 61.3%. See Section 4.2 for details. Green values correspond to the configurations we chose for full system evaluation.\n6\nAlgorithm 2-bit Experts 3-bit Experts A100 3080 Mobile 3060 T4 (Colab) A100 3080 Mobile 3060 T4 (Cloud) 3.061 Full algorithm 2.918 W/o expert pre-loading 2.265 W/o LRU cache & pre-loading Naive offloading (accelerate) 1.392 2.655 2.227 1.758 1.059 2.278 2.051 1.547 0.919 2.092 1.567 1.168 0.661 2.845 2.683 2.055 1.246 2.475 2.024 1.595 0.914 2.038 1.857 1.346 1.791 1.603 1.365 1.061 0.580",
            "Several recent works modify transformer architecture by introducing sparse Mixture-of-Experts blocks (Jacobs et al., 1991; Shazeer et al., 2017). MoE blocks contain multiple âexpertsâ (layers), as well as a âgating functionâ that selects which experts are used on a given input. As a result, the MoE block uses a small portion of all âexpertsâ for any single forward pass, allowing for more compute-efficient training Fedus et al. (2021); Du et al. (2022). Notably, MoEs are among the largest Fedus et al. (2021) and among the best Mixtral AI team (2023) of available LLMs. While Mixture-of-Experts models can be more efficient than their dense counterparts, many techniques for efficient LLM inference were not designed with MoE in mind and perform suboptimally on modern large language models that use mixture-of-experts layers.\n1When deployed in 16-bit precision, Falcon-180B needs approximately 360GB, while LLaMA-2 70B requires 140GB of combined accelerator memory."
        ],
        "response": "Offloading on the A100 server is used primarily as a reference point for other hardware setups, as the A100 can run the model without offloading. It is employed to evaluate the effectiveness of the proposed MoE-specific offloading strategy, which includes LRU caching and speculative expert loading, to compare performance against naive offloading methods [source: http://arxiv.org/pdf/2312.17238].",
        "reference": "Offloading is used on the A100 server for accelerating MoE-based language models when there is resource-constricted hardware and the goal is to enable broader access to these powerful models for research and development."
    },
    {
        "user_input": "How does Mixtral compare to Llama 2 70B in code benchmarks?",
        "retrieved_contexts": [
            "Table 2: Comparison of Mixtral with Llama. Mixtral outperforms or matches Llama 2 70B performance on almost all popular benchmarks while using 5x fewer active parameters during inference.\n70 Mixtral 8x7B. âMixtral 8x7B Mixtral 8x7B 355 =o = Es & E60! Mistral 78 % 2681 Mistral 78 3 3 s0 5 = A % 66 50 g 4 45 64 78 138 348708 78 138 348708 78 138 348 70B S66 Mixtral 8x7B 50 Mixtral 8x7B 5 = 564 340 g al Mistral 78 ee Mistral 78 3 5 Â§ 30 5 eo â= Mistral Â° 20 âe LlaMA2 78 (138 348 70B 7B (138 348 708 7B Â«13B 34B 708 Active Params Active Params Active Params\nFigure 3: Results on MMLU, commonsense reasoning, world knowledge and reading comprehension, math and code for Mistral (7B/8x7B) vs Llama 2 (7B/13B/70B). Mixtral largely outperforms Llama 2 70B on all benchmarks, except on reading comprehension benchmarks while using 5x lower active parameters. It is also vastly superior to Llama 2 70B on code and math.",
            "Detailed results for Mixtral, Mistral 7B and Llama 2 7B/13B/70B and Llama 1 34B2 are reported in Table 2. Figure 2 compares the performance of Mixtral with the Llama models in different categories. Mixtral surpasses Llama 2 70B across most metrics. In particular, Mixtral displays a superior performance in code and mathematics benchmarks.\nSize and Efficiency. We compare our performance to the Llama 2 family, aiming to understand Mixtral modelsâ efficiency in the cost-performance spectrum (see Figure 3). As a sparse Mixture- of-Experts model, Mixtral only uses 13B active parameters for each token. With 5x lower active parameters, Mixtral is able to outperform Llama 2 70B across most categories.\nNote that this analysis focuses on the active parameter count (see Section 2.1), which is directly proportional to the inference compute cost, but does not consider the memory costs and hardware utilization. The memory costs for serving Mixtral are proportional to its sparse parameter count, 47B, which is still smaller than Llama 2 70B. As for device utilization, we note that the SMoEs layer introduces additional overhead due to the routing mechanism and due to the increased memory loads when running more than one expert per device. They are more suitable for batched workloads where one can reach a good degree of arithmetic intensity.",
            "Llama 2 70B Mixtral 8x7B BBQ accuracy 51.5% 56.0% BOLD sentiment score (avg Â± std) gender profession religious_ideology political_ideology race 0.293 Â± 0.073 0.218 Â± 0.073 0.188 Â± 0.133 0.149 Â± 0.140 0.232 Â± 0.049 0.323 Â±0.045 0.243 Â± 0.087 0.144 Â± 0.089 0.186 Â± 0.146 0.232 Â± 0.052\nFigure 5: Bias Benchmarks. Compared Llama 2 70B, Mixtral presents less bias (higher accuracy on BBQ, lower std on BOLD) and displays more positive sentiment (higher avg on BOLD).\nWe benchmark Llama 2 and Mixtral on BBQ and BOLD with our evaluation framework and report the results in Table 5. Compared to Llama 2, Mixtral presents less bias on the BBQ benchmark (56.0% vs 51.5%). For each group in BOLD, a higher average sentiment score means more positive sentiments and a lower standard deviation indicates less bias within the group. Overall, Mixtral displays more positive sentiments than Llama 2, with similar variances within each group.\n# Instruction Fine-tuning",
            "Comparison with Llama 2 70B and GPT-3.5. In Table 3, we report the performance of Mixtral 8x7B compared to Llama 2 70B and GPT-3.5. We observe that Mixtral performs similarly or above the two other models. On MMLU, Mixtral obtains a better performance, despite its significantly smaller capacity (47B tokens compared to 70B). For MT Bench, we report the performance of the latest GPT-3.5-Turbo model available, gpt-3.5-turbo-1106.\n2Since Llama 2 34B was not open-sourced, we report results for Llama 1 34B.\n4",
            "Abstract\nWe introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine- tuned to follow instructions, Mixtral 8x7B â Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â chat model on human bench- marks. Both the base and instruct models are released under the Apache 2.0 license.\nCode: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/mixtral-of-experts/",
            "2Since Llama 2 34B was not open-sourced, we report results for Llama 1 34B.\n4\nLLaMA 2 70B GPT-3.5 MMLU (MCQ in 57 subjects) 69.9% 70.0% 70.6% HellaSwag (10-shot) 87.1% 85.5% 86.7% ARC Challenge (25-shot) 85.1% 85.2% 85.8% WinoGrande (5-shot) 83.2% 81.6% 81.2% MBPP (pass@1) 49.8% 52.2% 60.7% GSM-8K (5-shot) 53.6% 57.1% 58.4% MT Bench (for Instruct Models) 6.86 8.32 8.30\n# Mixtral 8x7B\nTable 3: Comparison of Mixtral with Llama 2 70B and GPT-3.5. Mixtral outperforms or matches Llama 2 70B and GPT-3.5 performance on most metrics.\nEvaluation Differences. On some benchmarks, there are some differences between our evaluation protocol and the one reported in the Llama 2 paper: 1) on MBPP, we use the hand-verified subset 2) on TriviaQA, we do not provide Wikipedia contexts.\n# 3.1 Multilingual benchmarks",
            "# 3.1 Multilingual benchmarks\nCompared to Mistral 7B, we significantly upsample the proportion of multilingual data during pretraining. The extra capacity allows Mixtral to perform well on multilingual benchmarks while maintaining a high accuracy in English. In particular, Mixtral significantly outperforms Llama 2 70B in French, German, Spanish, and Italian, as shown in Table 4.\nActive Params French Arc-c HellaS MMLU German Arc-c HellaS MMLU Spanish Arc-c HellaS MMLU Italian Arc-c HellaS MMLU 33B 70B 13B 42.9% 65.4% 49.0% 39.3% 68.1% 49.9% 49.9% 72.5% 64.3% 49.4% 70.9% 65.1% 58.2% 77.4% 70.9% 54.3% 73.0% 71.5% 55.4% 77.6% 72.5% 52.8% 75.1% 70.9% 41.1% 63.3% 48.7% 47.3% 68.7% 64.2% 45.7% 69.8% 52.3% 50.5% 74.5% 66.0%",
            "Code: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/mixtral-of-experts/\n# Introduction\nIn this paper, we present Mixtral 8x7B, a sparse mixture of experts model (SMoE) with open weights, licensed under Apache 2.0. Mixtral outperforms Llama 2 70B and GPT-3.5 on most benchmarks. As it only uses a subset of its parameters for every token, Mixtral allows faster inference speed at low batch-sizes, and higher throughput at large batch-sizes.\nMixtral is a sparse mixture-of-experts network. It is a decoder-only model where the feedforward block picks from a set of 8 distinct groups of parameters. At every layer, for every token, a router network chooses two of these groups (the âexpertsâ) to process the token and combine their output additively. This technique increases the number of parameters of a model while controlling cost and latency, as the model only uses a fraction of the total set of parameters per token.\nMixtral is pretrained with multilingual data using a context size of 32k tokens. It either matches or exceeds the performance of Llama 2 70B and GPT-3.5, over several benchmarks. In particular,\nMixture of Experts Layer i gating inputs af outputs router expert",
            "Table 4: Comparison of Mixtral with Llama on Multilingual Benchmarks. On ARC Challenge, Hellaswag, and MMLU, Mixtral outperforms Llama 2 70B on 4 languages: French, German, Spanish, and Italian.\n# 3.2 Long range performance\nTo assess the capabilities of Mixtral to tackle long context, we evaluate it on the passkey retrieval task introduced in [23], a synthetic task designed to measure the ability of the model to retrieve a passkey inserted randomly in a long prompt. Results in Figure 4 (Left) show that Mixtral achieves a 100% retrieval accuracy regardless of the context length or the position of passkey in the sequence. Figure 4 (Right) shows that the perplexity of Mixtral on a subset of the proof-pile dataset [2] decreases monotonically as the size of the context increases.\nPasskey Performance ry 0.8 0.6 04 0.2 0.0 OK 4K 8K 12K 16K 20K 24K 28K Seq Len Passkey Loc\n3.8 â Mixtral_8x7B 3.5 32 > $3.0 i] 228 fos a 2.0 0 5k 10k 15k 20k 25k 30k Context length",
            "Mixture of Experts Layer i gating inputs af outputs router expert\nFigure 1: Mixture of Experts Layer. Each input vector is assigned to 2 of the 8 experts by a router. The layerâs output is the weighted sum of the outputs of the two selected experts. In Mixtral, an expert is a standard feedforward block as in a vanilla transformer architecture.\nMixtral demonstrates superior capabilities in mathematics, code generation, and tasks that require multilingual understanding, significantly outperforming Llama 2 70B in these domains. Experiments show that Mixtral is able to successfully retrieve information from its context window of 32k tokens, regardless of the sequence length and the location of the information in the sequence."
        ],
        "response": "Mixtral 8x7B outperforms Llama 2 70B on code benchmarks, achieving higher accuracy in tasks like MBPP with 52.2% vs 49.8% for Llama 2 70B. [http://arxiv.org/pdf/2401.04088] Mixtral uses only 13B active parameters during inference, compared to Llama 2 70B's 70B parameters. [http://arxiv.org/pdf/2401.04088] This efficiency allows Mixtral to process tokens faster while maintaining superior performance in code generation and mathematics. [http://arxiv.org/pdf/2401.04088] The model's sparse Mixture-of-Experts architecture enables it to select relevant experts per token, reducing computational cost. [http://arxiv.org/pdf/2401.04088] Additionally, Mixtral matches or exceeds Llama 2 70B on most other benchmarks, including MMLU and multilingual tasks. [http://arxiv.org/pdf/2401.04088]",
        "reference": "Mixtral outperforms Llama 2 70B in code benchmarks."
    },
    {
        "user_input": "In terms of mathematics benchmarks, how does Mixtral perform compared to Llama 2 70B?",
        "retrieved_contexts": [
            "Detailed results for Mixtral, Mistral 7B and Llama 2 7B/13B/70B and Llama 1 34B2 are reported in Table 2. Figure 2 compares the performance of Mixtral with the Llama models in different categories. Mixtral surpasses Llama 2 70B across most metrics. In particular, Mixtral displays a superior performance in code and mathematics benchmarks.\nSize and Efficiency. We compare our performance to the Llama 2 family, aiming to understand Mixtral modelsâ efficiency in the cost-performance spectrum (see Figure 3). As a sparse Mixture- of-Experts model, Mixtral only uses 13B active parameters for each token. With 5x lower active parameters, Mixtral is able to outperform Llama 2 70B across most categories.\nNote that this analysis focuses on the active parameter count (see Section 2.1), which is directly proportional to the inference compute cost, but does not consider the memory costs and hardware utilization. The memory costs for serving Mixtral are proportional to its sparse parameter count, 47B, which is still smaller than Llama 2 70B. As for device utilization, we note that the SMoEs layer introduces additional overhead due to the routing mechanism and due to the increased memory loads when running more than one expert per device. They are more suitable for batched workloads where one can reach a good degree of arithmetic intensity.",
            "Table 2: Comparison of Mixtral with Llama. Mixtral outperforms or matches Llama 2 70B performance on almost all popular benchmarks while using 5x fewer active parameters during inference.\n70 Mixtral 8x7B. âMixtral 8x7B Mixtral 8x7B 355 =o = Es & E60! Mistral 78 % 2681 Mistral 78 3 3 s0 5 = A % 66 50 g 4 45 64 78 138 348708 78 138 348708 78 138 348 70B S66 Mixtral 8x7B 50 Mixtral 8x7B 5 = 564 340 g al Mistral 78 ee Mistral 78 3 5 Â§ 30 5 eo â= Mistral Â° 20 âe LlaMA2 78 (138 348 70B 7B (138 348 708 7B Â«13B 34B 708 Active Params Active Params Active Params\nFigure 3: Results on MMLU, commonsense reasoning, world knowledge and reading comprehension, math and code for Mistral (7B/8x7B) vs Llama 2 (7B/13B/70B). Mixtral largely outperforms Llama 2 70B on all benchmarks, except on reading comprehension benchmarks while using 5x lower active parameters. It is also vastly superior to Llama 2 70B on code and math.",
            "Comparison with Llama 2 70B and GPT-3.5. In Table 3, we report the performance of Mixtral 8x7B compared to Llama 2 70B and GPT-3.5. We observe that Mixtral performs similarly or above the two other models. On MMLU, Mixtral obtains a better performance, despite its significantly smaller capacity (47B tokens compared to 70B). For MT Bench, we report the performance of the latest GPT-3.5-Turbo model available, gpt-3.5-turbo-1106.\n2Since Llama 2 34B was not open-sourced, we report results for Llama 1 34B.\n4",
            "Abstract\nWe introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine- tuned to follow instructions, Mixtral 8x7B â Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â chat model on human bench- marks. Both the base and instruct models are released under the Apache 2.0 license.\nCode: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/mixtral-of-experts/",
            "Llama 2 70B Mixtral 8x7B BBQ accuracy 51.5% 56.0% BOLD sentiment score (avg Â± std) gender profession religious_ideology political_ideology race 0.293 Â± 0.073 0.218 Â± 0.073 0.188 Â± 0.133 0.149 Â± 0.140 0.232 Â± 0.049 0.323 Â±0.045 0.243 Â± 0.087 0.144 Â± 0.089 0.186 Â± 0.146 0.232 Â± 0.052\nFigure 5: Bias Benchmarks. Compared Llama 2 70B, Mixtral presents less bias (higher accuracy on BBQ, lower std on BOLD) and displays more positive sentiment (higher avg on BOLD).\nWe benchmark Llama 2 and Mixtral on BBQ and BOLD with our evaluation framework and report the results in Table 5. Compared to Llama 2, Mixtral presents less bias on the BBQ benchmark (56.0% vs 51.5%). For each group in BOLD, a higher average sentiment score means more positive sentiments and a lower standard deviation indicates less bias within the group. Overall, Mixtral displays more positive sentiments than Llama 2, with similar variances within each group.\n# Instruction Fine-tuning",
            "2Since Llama 2 34B was not open-sourced, we report results for Llama 1 34B.\n4\nLLaMA 2 70B GPT-3.5 MMLU (MCQ in 57 subjects) 69.9% 70.0% 70.6% HellaSwag (10-shot) 87.1% 85.5% 86.7% ARC Challenge (25-shot) 85.1% 85.2% 85.8% WinoGrande (5-shot) 83.2% 81.6% 81.2% MBPP (pass@1) 49.8% 52.2% 60.7% GSM-8K (5-shot) 53.6% 57.1% 58.4% MT Bench (for Instruct Models) 6.86 8.32 8.30\n# Mixtral 8x7B\nTable 3: Comparison of Mixtral with Llama 2 70B and GPT-3.5. Mixtral outperforms or matches Llama 2 70B and GPT-3.5 performance on most metrics.\nEvaluation Differences. On some benchmarks, there are some differences between our evaluation protocol and the one reported in the Llama 2 paper: 1) on MBPP, we use the hand-verified subset 2) on TriviaQA, we do not provide Wikipedia contexts.\n# 3.1 Multilingual benchmarks",
            "# 3.1 Multilingual benchmarks\nCompared to Mistral 7B, we significantly upsample the proportion of multilingual data during pretraining. The extra capacity allows Mixtral to perform well on multilingual benchmarks while maintaining a high accuracy in English. In particular, Mixtral significantly outperforms Llama 2 70B in French, German, Spanish, and Italian, as shown in Table 4.\nActive Params French Arc-c HellaS MMLU German Arc-c HellaS MMLU Spanish Arc-c HellaS MMLU Italian Arc-c HellaS MMLU 33B 70B 13B 42.9% 65.4% 49.0% 39.3% 68.1% 49.9% 49.9% 72.5% 64.3% 49.4% 70.9% 65.1% 58.2% 77.4% 70.9% 54.3% 73.0% 71.5% 55.4% 77.6% 72.5% 52.8% 75.1% 70.9% 41.1% 63.3% 48.7% 47.3% 68.7% 64.2% 45.7% 69.8% 52.3% 50.5% 74.5% 66.0%",
            "Table 4: Comparison of Mixtral with Llama on Multilingual Benchmarks. On ARC Challenge, Hellaswag, and MMLU, Mixtral outperforms Llama 2 70B on 4 languages: French, German, Spanish, and Italian.\n# 3.2 Long range performance\nTo assess the capabilities of Mixtral to tackle long context, we evaluate it on the passkey retrieval task introduced in [23], a synthetic task designed to measure the ability of the model to retrieve a passkey inserted randomly in a long prompt. Results in Figure 4 (Left) show that Mixtral achieves a 100% retrieval accuracy regardless of the context length or the position of passkey in the sequence. Figure 4 (Right) shows that the perplexity of Mixtral on a subset of the proof-pile dataset [2] decreases monotonically as the size of the context increases.\nPasskey Performance ry 0.8 0.6 04 0.2 0.0 OK 4K 8K 12K 16K 20K 24K 28K Seq Len Passkey Loc\n3.8 â Mixtral_8x7B 3.5 32 > $3.0 i] 228 fos a 2.0 0 5k 10k 15k 20k 25k 30k Context length",
            "# 6 Conclusion\nIn this paper, we introduced Mixtral 8x7B, the first mixture-of-experts network to reach a state-of-the- art performance among open-source models. Mixtral 8x7B Instruct outperforms Claude-2.1, Gem- ini Pro, and GPT-3.5 Turbo on human evaluation benchmarks. Because it only uses two experts at each time step, Mixtral only uses 13B active parameters per token while outperforming the previous best model using 70B parameters per token (Llama 2 70B). We are making our trained and fine-tuned mod- els publicly available under the Apache 2.0 license. By sharing our models, we aim to facilitate the de- velopment of new techniques and applications that can benefit a wide range of industries and domains.",
            "n-1 y= Ss Softmax(Top2(a - W,)); - SwiGLU;(a). i=0\nThis formulation is similar to the GShard architecture [21], with the exceptions that we replace all FFN sub-blocks by MoE layers while GShard replaces every other block, and that GShard uses a more elaborate gating strategy for the second expert assigned to each token.\n# 3 Results\nWe compare Mixtral to Llama, and re-run all benchmarks with our own evaluation pipeline for fair comparison. We measure performance on a wide variety of tasks categorized as follow:\nâ¢ Commonsense Reasoning (0-shot): Hellaswag [32], Winogrande [26], PIQA [3], SIQA [27], OpenbookQA [22], ARC-Easy, ARC-Challenge [8], CommonsenseQA [30]"
        ],
        "response": "Mixtral outperforms Llama 2 70B on mathematics benchmarks, as shown in Table 3. Mixtral achieves higher accuracy on GSM-8K compared to Llama 2 70B. [http://arxiv.org/pdf/2401.04088] Mixtral also surpasses Llama 2 70B on code generation benchmarks like MBPP. [http://arxiv.org/pdf/2401.04088] Despite having fewer active parameters, Mixtral maintains superior performance in mathematics and code tasks. [http://arxiv.org/pdf/2401.04088] The sparse Mixture-of-Experts architecture enables Mixtral to outperform larger models like Llama 2 70B. [http://arxiv.org/pdf/2401.04088]",
        "reference": "Mixtral performs better than Llama 2 70B in terms of mathematics benchmarks."
    },
    {
        "user_input": "What is the relationship between benchmarking the expert LRU cache and speculative loading, and the expert recall rate in the Mixtral-8x7B-Instruct model on the OpenAssistant dataset?",
        "retrieved_contexts": [
            "For this evaluation, we run Mixtral-8x7B-Instruct model on the OpenAssistant dataset (KÃ¶pf et al., 2023). We test LRU caching by running the model on recorded conversations and measuring the recall (aka âhit ratioâ from caching perspective) for different cache sizes k. Next, we test speculative loading in isolation by âguessingâ which experts should be loaded (by applying the next layerâs gating function on current layer activations), then measuring how often the actual next experts get loaded this way. A recall of 1.0 corresponds to a situation where both (2) Mixtral active experts were pre-fetched. We test speculative loading in three settings: 1, 2 and 10 layers ahead.\n# 4.2 Mixed MoE Quantization",
            "7Notably, Google Colab RAM cannot fit Mixtral-8x7B with a reasonable compression rate 8This corresponds to tensor.pin_memory() command in PyTorch.\n5\niy & cache_size =3 cache_size = 2 cache_size =4 0.84 | PIO â prefetch 1 experts ~ escent ae | PRS aa 0.2} ââ prefetch 2 experts ââ prefetch 3 experts 0.0 00 0 5 10 15 20 25 30 0 5 10 15 20 25 30 Layer # Layer # S Fd Ed Cache hit rate Bd ES Prediction recall = ES Ss &\nFigure 2: (left) LRU cache hit ratio for different cache size k; (right) speculative loading recall when pre-loading a different number of experts. Regular lines represent loading 1 layer ahead; dashed line stands for 2 layers ahead; dotted line is 10 layers ahead.\n# 4.1 Expert LRU Cache and Speculative Loading\nIn this section, we benchmark the effectiveness of the two expert offloading strategies: LRU caching and and speculative loading, as defined in Sections 3.1 and 3.2 respectively. For this evaluation, we measure âexpert recallâ â the fraction of times when an expert needed for inference was already available on GPU.",
            "Expert Offloading. As described earlier, we use LRU cache with an equal number k of cached experts per layer. For Mixtral-8x7B, we use k=2 for 12GB GPUs and k=4 for 16GB ones. We trigger speculative expert loading immediately after the system finished loading all experts for the current layer. The speculative expert loading fetches 1 â 2 most likely experts. The newly loaded experts do not replace the currently cached experts. If a speculatively loaded expert was later used during next layer inference, it will replace the least recently used expert from the next layerâs cache.\nMany consumer devices and free-tier cloud instances have limited host RAM that cannot fit the entire model7. In these cases, the experts must be split between host and device memory. To support this, our implementation of expert LRU cache splits experts between host and GPU devices. When loading and expert to the GPU cache, the system also offloads the least recently used on-device expert back to RAM so as to preserve memory parity.",
            "We illustrate an example of how LRU cache saves experts in Figure 1 (see caption). LRU is a very simple strategy that does not consider factors like expert activation frequencies, varying cache size between MoE layers, or any sequential patterns in expert activation. However, we found that even this simple strategy can significantly speed up inference for modern Mixture-of-Experts models such as Mixtral-8x7B (see Section 4 for detailed evaluation).\n# 3.2 Speculative Expert Loading\nWhile LRU caching can reduce the average expert loading time, most of the inference time is still spent waiting for the next expert to be loaded. The reason behind this is that, unlike with dense models, MoE offloading cannot effectively overlap expert loading with computation. To understand this problem, let us zoom into the process of generating a single token, layer-by-layer. The full compute workload starts by embedding the previous token via look-up, then alternates between running self-attention and MLP for each transformer block in the model. Finally, the outputs from the last transformer block are used to predict next token logits with a linear projection.",
            "Figure 1: An example of expert loading pattern in Mixtral-8x7B-Instruct for select layers. Blue cells indicate that a certain expert was active when encoding a certain token; deeper blue indicates higher gating weight. Small gray squares show which experts are cached with an LRU cache for k=2.\n# 3.1 Expert Locality and LRU caching\nAs we discussed earlier in Section 2.1, Mixture-of-Experts language models were often observed to assign individual experts to distinct sub-tasks. However, this does not mean that the model uses the same expert over long stretches of tokens. Instead, some experts are active in short sequences of 2-4 tokens, while others are often used with âgapsâ, as shown in Figure 1.\nTo take advantage of this pattern, we can keep active experts in GPU memory as a âcacheâ for future tokens. If the same experts are activated again in future, they will be available instantaneously. Naturally, the number of experts that can be stored this way if very limited by the available GPU memory. For simplicity, we choose to always keep k least recently used experts as a type of LRU cache. If k is greater than the number of active experts, the cache will save experts from multiple previous tokens. For simplicity, we keep the same number of cached experts for each MoE layer.",
            "To speed up offloading in practice, we allocate all expert parameters in a contiguous memory buffer that can be moved as a single host-to-device copy. For host-side (RAM) experts, we pin8 this memory buffer for faster communication. Our implementation additionally allocates b=4 on-device buffers used to copy and prefetch experts asynchronously, without modifying existing experts. These buffers are shared between all MoE layers to reduce memory footprint. Overall, the system requires num_layers Ã num_experts expert memory buffers split between host and device memory and b=4 temporary buffers, the size of each buffer being equal to a single expert.\n# 4 Experiments\nIn this section, we verify our earlier hypotheses about MoE behavior and benchmark the inference latency in different conditions. We focus our evaluations on Mixtral-8x7B and Mixtral-8x7B-Instruct models since they represent the current state of the art among open-access MoE models. We organize this section as follows: Section 4.1 measures the effectiveness of expert caching and pre-loading in isolation, Section 4.2 compares different model compression algorithms and verifies our hypotheses from Section 3.3. Finally, Section 4.3 measures the inference latency in several hardware setups.",
            "1When deployed in 16-bit precision, Falcon-180B needs approximately 360GB, while LLaMA-2 70B requires 140GB of combined accelerator memory.\nIn this work, we systematically develop techniques for running large MoE language models with limited GPU memory. Our main objective is inferencing (generating tokens) with Mixtral-8x7B- Instruct â a MoE-based chat assistant â on a desktop-grade hardware where only a fraction of experts fit into the accelerator memory. To that end:\nwe observe how MoE language model accesses its experts between tokens, and find several regularities: i) some experts are reused between adjacent tokens and ii) the model hidden states of early layers already âknowâ which experts are to be used at subsequent layers. â¢ we design a MoE-specific offloading strategy that takes advantage of these regularities: i) it uses LRU cache to significantly reduces GPU-RAM communication, leading to faster generation and ii) it guesses which experts are needed ahead of time to better overlap expert loading with computation.\nâ¢ we consider the specific scenario of running Mixtral-8x7B-Instruct on a T4, RTX 3060 and RTX 3080 Mobile and develop a practical combination of mixed quantization and the proposed offloading algorithm to run this model interactively at 2-3 tokens per second depending on the hardware. The source code with our implementation is available online2",
            "n-1 y= Ss Softmax(Top2(a - W,)); - SwiGLU;(a). i=0\nThis formulation is similar to the GShard architecture [21], with the exceptions that we replace all FFN sub-blocks by MoE layers while GShard replaces every other block, and that GShard uses a more elaborate gating strategy for the second expert assigned to each token.\n# 3 Results\nWe compare Mixtral to Llama, and re-run all benchmarks with our own evaluation pipeline for fair comparison. We measure performance on a wide variety of tasks categorized as follow:\nâ¢ Commonsense Reasoning (0-shot): Hellaswag [32], Winogrande [26], PIQA [3], SIQA [27], OpenbookQA [22], ARC-Easy, ARC-Challenge [8], CommonsenseQA [30]",
            "Table 1: Perplexity and model size evaluation of Mixtral-8x7B with different quantization for shared attention (Attn quant) and experts (Experts quant) layers. For comprarison, a Mistral-7B 4-bit quantized model has Wiki2 perplexity 5.03, C4 perplexity 7.56 and MMLU score 61.3%. See Section 4.2 for details. Green values correspond to the configurations we chose for full system evaluation.\n6\nAlgorithm 2-bit Experts 3-bit Experts A100 3080 Mobile 3060 T4 (Colab) A100 3080 Mobile 3060 T4 (Cloud) 3.061 Full algorithm 2.918 W/o expert pre-loading 2.265 W/o LRU cache & pre-loading Naive offloading (accelerate) 1.392 2.655 2.227 1.758 1.059 2.278 2.051 1.547 0.919 2.092 1.567 1.168 0.661 2.845 2.683 2.055 1.246 2.475 2.024 1.595 0.914 2.038 1.857 1.346 1.791 1.603 1.365 1.061 0.580",
            "# 6 Conclusion\nIn this paper, we introduced Mixtral 8x7B, the first mixture-of-experts network to reach a state-of-the- art performance among open-source models. Mixtral 8x7B Instruct outperforms Claude-2.1, Gem- ini Pro, and GPT-3.5 Turbo on human evaluation benchmarks. Because it only uses two experts at each time step, Mixtral only uses 13B active parameters per token while outperforming the previous best model using 70B parameters per token (Llama 2 70B). We are making our trained and fine-tuned mod- els publicly available under the Apache 2.0 license. By sharing our models, we aim to facilitate the de- velopment of new techniques and applications that can benefit a wide range of industries and domains."
        ],
        "response": "Benchmarking LRU cache and speculative loading measures expert recall, the fraction of times a needed expert was pre-loaded on GPU. Higher recall reduces inference latency by minimizing host-device transfers. For Mixtral-8x7B-Instruct on OpenAssistant, LRU cache hit ratio increases with cache size k, as shown in Figure 2 left. Speculative loading recall peaks when pre-loading 1-2 experts 1-2 layers ahead, per Figure 2 right. Combining both strategies optimizes expert availability, as demonstrated in Table 1's 2.918 tokens/sec result. [source: http://arxiv.org/pdf/2312.17238]",
        "reference": "The relationship between benchmarking the expert LRU cache and speculative loading and the expert recall rate in the Mixtral-8x7B-Instruct model on the OpenAssistant dataset is that both strategies are being evaluated to determine their effectiveness. The benchmarking is done to measure the expert recall, which is the fraction of times when an expert needed for inference was already available on the GPU."
    },
    {
        "user_input": "How does the use of sparse Mixture-of-Experts (MoE) in language models contribute to faster token generation?",
        "retrieved_contexts": [
            "# Denis Mazur Moscow Institute of Physics and Technology Yandex Researchcore denismazur8@gmail.com\n# Abstract\nWith the widespread adoption of Large Language Models (LLMs), many deep learning practitioners are looking for strategies of running these models more efficiently. One such strategy is to use sparse Mixture-of-Experts (MoE) â a type of model architectures where only a fraction of model layers are active for any given input. This property allows MoE-based language models to generate tokens faster than their âdenseâ counterparts, but it also increases model size due to having multiple âexpertsâ. Unfortunately, this makes state-of-the-art MoE language models difficult to run without high-end GPUs. In this work, we study the problem of running large MoE language models on consumer hardware with limited accelerator memory. We build upon parameter offloading algorithms and propose a novel strategy that accelerates offloading by taking advantage of innate properties of MoE LLMs. Using this strategy, we build can run Mixtral-8x7B with mixed quantization on desktop hardware and free-tier Google Colab instances.\n# Introduction",
            "1When deployed in 16-bit precision, Falcon-180B needs approximately 360GB, while LLaMA-2 70B requires 140GB of combined accelerator memory.\nIn this work, we systematically develop techniques for running large MoE language models with limited GPU memory. Our main objective is inferencing (generating tokens) with Mixtral-8x7B- Instruct â a MoE-based chat assistant â on a desktop-grade hardware where only a fraction of experts fit into the accelerator memory. To that end:\nwe observe how MoE language model accesses its experts between tokens, and find several regularities: i) some experts are reused between adjacent tokens and ii) the model hidden states of early layers already âknowâ which experts are to be used at subsequent layers. â¢ we design a MoE-specific offloading strategy that takes advantage of these regularities: i) it uses LRU cache to significantly reduces GPU-RAM communication, leading to faster generation and ii) it guesses which experts are needed ahead of time to better overlap expert loading with computation.\nâ¢ we consider the specific scenario of running Mixtral-8x7B-Instruct on a T4, RTX 3060 and RTX 3080 Mobile and develop a practical combination of mixed quantization and the proposed offloading algorithm to run this model interactively at 2-3 tokens per second depending on the hardware. The source code with our implementation is available online2",
            "Shazeer et al. (2017) builds on this idea to train a sparsely gated Mixture-of-Experts to serve as a language model. The full model consists of a recurrent neural network backbone and a MoE module with up to 131072 experts. When processing a given token, a linear gating function select 4 most suitable experts based on the latest hidden state. The resulting model (including the gating function and experts) is trained end-to-end to minimize cross-entropy, with an additional regularizer to promote equal expert utilization. Shazeer et al. (2017) observed that the MoE model not only improves perplexity, but also learns interpretable expert specializations: some experts would âspecializeâ on prepositions, while others learn to express a particular concept (e.g. speed).",
            "Since then, several lines of work explore Mixture-of-Experts with Transformer-based language models for machine translation Lepikhin et al. (2020), masked language modeling Fedus et al. (2021), general-purpose LLMs Du et al. (2022) and others. Most of these models follow traditional (dense) Transformer architecture for embeddings and attention layers, and only use Mixture for the feedforward (MLP) blocks and use a linear token-level gating function. A common observation across most of these works is that MoE models are cheaper to train and inference Fedus et al. (2021); Lepikhin et al. (2020), but require more parameters than a dense model with equivalent perplexity. Pre-trained Mixture-of-Experts LLMs have been openly available for over a year3. However, these models seem to have gained less traction than equivalent dense models, arguable because their sheer model size (over a trillion parameters) makes them difficult to use. Most recently, Mistral AI released a family of sparse Mixture of Experts models called Mixtral-8x7B with near state-of-the-art performance Mixtral AI team (2023). This model has already inspired several follow-up works and practical applications, but it still requires a high-end GPU accelerator.\n# 2.2 Post-training Quantization of LLMs",
            "# 5 Conclusion and Future Work\nIn this work, we explore strategies for accelerating Mixture-of-Experts based language models on consumer hardware with limited GPU memory. We propose a MoE-centric approach to offloading\n7\nand explore how mixed quantization affects perplexity and performance on language understanding tasks. We evaluate the proposed strategies and show that they produce a significant increase in generation speed compared to naÂ¨ve approaches on consumer-grade hardware, including free-tier Google Colab.\nOur method provides a practical solution for inferencing large MoE language models on resource- constricted hardware, enabling broader access to these powerful models for research and development. As future work, we plan to explore further offloading strategies, based on speculative expert predic- tion.\n# Acknowledgements\nAuthors would like to acknowledge mobicham@ for helpful discussions on Mixtral quantization.\n# References\nAminabadi, R. Y., Rajbhandari, S., Awan, A. A., Li, C., Li, D., Zheng, E., Ruwase, O., Smith, S., Zhang, M., Rasley, J., and He, Y. Deepspeed-inference: Enabling efficient inference of transformer models at unprecedented scale. In Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis, SC â22. IEEE Press, 2022. ISBN 9784665454445.",
            "Several recent works modify transformer architecture by introducing sparse Mixture-of-Experts blocks (Jacobs et al., 1991; Shazeer et al., 2017). MoE blocks contain multiple âexpertsâ (layers), as well as a âgating functionâ that selects which experts are used on a given input. As a result, the MoE block uses a small portion of all âexpertsâ for any single forward pass, allowing for more compute-efficient training Fedus et al. (2021); Du et al. (2022). Notably, MoEs are among the largest Fedus et al. (2021) and among the best Mixtral AI team (2023) of available LLMs. While Mixture-of-Experts models can be more efficient than their dense counterparts, many techniques for efficient LLM inference were not designed with MoE in mind and perform suboptimally on modern large language models that use mixture-of-experts layers.\n1When deployed in 16-bit precision, Falcon-180B needs approximately 360GB, while LLaMA-2 70B requires 140GB of combined accelerator memory.",
            "Code: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/mixtral-of-experts/\n# Introduction\nIn this paper, we present Mixtral 8x7B, a sparse mixture of experts model (SMoE) with open weights, licensed under Apache 2.0. Mixtral outperforms Llama 2 70B and GPT-3.5 on most benchmarks. As it only uses a subset of its parameters for every token, Mixtral allows faster inference speed at low batch-sizes, and higher throughput at large batch-sizes.\nMixtral is a sparse mixture-of-experts network. It is a decoder-only model where the feedforward block picks from a set of 8 distinct groups of parameters. At every layer, for every token, a router network chooses two of these groups (the âexpertsâ) to process the token and combine their output additively. This technique increases the number of parameters of a model while controlling cost and latency, as the model only uses a fraction of the total set of parameters per token.\nMixtral is pretrained with multilingual data using a context size of 32k tokens. It either matches or exceeds the performance of Llama 2 70B and GPT-3.5, over several benchmarks. In particular,\nMixture of Experts Layer i gating inputs af outputs router expert",
            "The generative inference workload consists of two phases: 1) encoding the input prompt and 2) generating tokens conditioned on that prompt. The key difference between these two phases is that prompt tokens are encoded in parallel (layer-by-layer), whereas the generation runs sequentially (token-by-token and layer-by-layer). In general, phase 1 works relatively well with existing Mixture- of-Experts algorithms, since each layer can only be loaded once for the entire prompt. In turn, when generating tokens, one must load layer once per each token generated. In practice, this means that inference speed is limited by how fast one can fetch parameters from system memory.\nBelow, we look for patterns in how the MoE model loads its experts and propose ways to exploit these patterns to speed up inference time.\n4To learn more about these methods, please refer to surveys such as Gholami et al. (2021); Liang et al. (2021) 5As opposed to running a processing a large batch of texts over many seconds, as in Sheng et al. (2023)\n3\nSelected experts for Mixtral-8x7B-Instruct woe 0 (top) and 15 ae =n a oa ao a âme: a n: ee Layer 15 expert # Layer 0 expert # MAUR STARR O However about |= and 4 training data owerful language model based trained Trans former f architecture",
            "[10] Jwala Dhamala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada Pruksachatkun, Kai-Wei Chang, and Rahul Gupta. Bold: Dataset and metrics for measuring biases in open-ended language generation. In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency, pages 862â872, 2021.\n[11] Artyom Eliseev and Denis Mazur. Fast inference of mixture-of-experts language models with offloading. arXiv preprint arXiv:2312.17238, 2023.\n[12] William Fedus, Jeff Dean, and Barret Zoph. A review of sparse expert models in deep learning. arXiv preprint arXiv:2209.01667, 2022.\n[13] Trevor Gale, Deepak Narayanan, Cliff Young, and Matei Zaharia. Megablocks: Efficient sparse training with mixture-of-experts. arXiv preprint arXiv:2211.15841, 2022.",
            "We illustrate an example of how LRU cache saves experts in Figure 1 (see caption). LRU is a very simple strategy that does not consider factors like expert activation frequencies, varying cache size between MoE layers, or any sequential patterns in expert activation. However, we found that even this simple strategy can significantly speed up inference for modern Mixture-of-Experts models such as Mixtral-8x7B (see Section 4 for detailed evaluation).\n# 3.2 Speculative Expert Loading\nWhile LRU caching can reduce the average expert loading time, most of the inference time is still spent waiting for the next expert to be loaded. The reason behind this is that, unlike with dense models, MoE offloading cannot effectively overlap expert loading with computation. To understand this problem, let us zoom into the process of generating a single token, layer-by-layer. The full compute workload starts by embedding the previous token via look-up, then alternates between running self-attention and MLP for each transformer block in the model. Finally, the outputs from the last transformer block are used to predict next token logits with a linear projection."
        ],
        "response": "Sparse Mixture-of-Experts (MoE) models only activate a fraction of experts per token, reducing computational load and enabling faster token generation. This sparsity allows MoE models to achieve higher throughput at low batch sizes compared to dense models. The gating function selects the most relevant experts for each token, minimizing unnecessary computations. MoE models like Mixtral-8x7B leverage this property to accelerate inference, as only a subset of parameters is processed per token. However, the increased model size requires efficient offloading strategies for consumer hardware. Techniques like LRU caching and speculative expert loading further optimize speed by reducing GPU-RAM communication. These methods exploit regularities in expert reuse, allowing faster generation even with limited memory. Mixtral-8x7B demonstrates this by achieving 2-3 tokens per second on hardware like RTX 3080 Mobile. [http://arxiv.org/pdf/2312.17238]",
        "reference": "The use of sparse Mixture-of-Experts (MoE) in language models contributes to faster token generation because only a fraction of model layers are active for any given input. This means that MoE-based language models can generate tokens faster than their \"dense\" counterparts. The MoE blocks in the model contain multiple \"experts\" (layers) and a \"gating function\" that selects which experts are used for a given input. By using a small portion of all \"experts\" for each forward pass, the MoE block allows for more compute-efficient training, leading to faster token generation."
    },
    {
        "user_input": "What impact does the use of sparse Mixture-of-Experts (MoE) have on the size of language models?",
        "retrieved_contexts": [
            "# Denis Mazur Moscow Institute of Physics and Technology Yandex Researchcore denismazur8@gmail.com\n# Abstract\nWith the widespread adoption of Large Language Models (LLMs), many deep learning practitioners are looking for strategies of running these models more efficiently. One such strategy is to use sparse Mixture-of-Experts (MoE) â a type of model architectures where only a fraction of model layers are active for any given input. This property allows MoE-based language models to generate tokens faster than their âdenseâ counterparts, but it also increases model size due to having multiple âexpertsâ. Unfortunately, this makes state-of-the-art MoE language models difficult to run without high-end GPUs. In this work, we study the problem of running large MoE language models on consumer hardware with limited accelerator memory. We build upon parameter offloading algorithms and propose a novel strategy that accelerates offloading by taking advantage of innate properties of MoE LLMs. Using this strategy, we build can run Mixtral-8x7B with mixed quantization on desktop hardware and free-tier Google Colab instances.\n# Introduction",
            "Several recent works modify transformer architecture by introducing sparse Mixture-of-Experts blocks (Jacobs et al., 1991; Shazeer et al., 2017). MoE blocks contain multiple âexpertsâ (layers), as well as a âgating functionâ that selects which experts are used on a given input. As a result, the MoE block uses a small portion of all âexpertsâ for any single forward pass, allowing for more compute-efficient training Fedus et al. (2021); Du et al. (2022). Notably, MoEs are among the largest Fedus et al. (2021) and among the best Mixtral AI team (2023) of available LLMs. While Mixture-of-Experts models can be more efficient than their dense counterparts, many techniques for efficient LLM inference were not designed with MoE in mind and perform suboptimally on modern large language models that use mixture-of-experts layers.\n1When deployed in 16-bit precision, Falcon-180B needs approximately 360GB, while LLaMA-2 70B requires 140GB of combined accelerator memory.",
            "Since then, several lines of work explore Mixture-of-Experts with Transformer-based language models for machine translation Lepikhin et al. (2020), masked language modeling Fedus et al. (2021), general-purpose LLMs Du et al. (2022) and others. Most of these models follow traditional (dense) Transformer architecture for embeddings and attention layers, and only use Mixture for the feedforward (MLP) blocks and use a linear token-level gating function. A common observation across most of these works is that MoE models are cheaper to train and inference Fedus et al. (2021); Lepikhin et al. (2020), but require more parameters than a dense model with equivalent perplexity. Pre-trained Mixture-of-Experts LLMs have been openly available for over a year3. However, these models seem to have gained less traction than equivalent dense models, arguable because their sheer model size (over a trillion parameters) makes them difficult to use. Most recently, Mistral AI released a family of sparse Mixture of Experts models called Mixtral-8x7B with near state-of-the-art performance Mixtral AI team (2023). This model has already inspired several follow-up works and practical applications, but it still requires a high-end GPU accelerator.\n# 2.2 Post-training Quantization of LLMs",
            "Shazeer et al. (2017) builds on this idea to train a sparsely gated Mixture-of-Experts to serve as a language model. The full model consists of a recurrent neural network backbone and a MoE module with up to 131072 experts. When processing a given token, a linear gating function select 4 most suitable experts based on the latest hidden state. The resulting model (including the gating function and experts) is trained end-to-end to minimize cross-entropy, with an additional regularizer to promote equal expert utilization. Shazeer et al. (2017) observed that the MoE model not only improves perplexity, but also learns interpretable expert specializations: some experts would âspecializeâ on prepositions, while others learn to express a particular concept (e.g. speed).",
            "# 2 Background & Related Work\n# 2.1 Mixture-of-Experts\nThe recent surge in MoE language models builds on a relatively old idea (Jacobs et al., 1991; Jordan & Jacobs, 1994) of training ensembles of specialized models (âexpertsâ) and a gating function to select the right expert for the task. To achieve specialization, Mixture-of-Experts learn by simultaneously i) training the gating function to choose the best experts and ii) training the experts themselves on samples assigned to them by the gating function. Since then, many different MoE variants emerged, including mixture of SVM models (Collobert et al., 2002), Dirichlet processes (Shahbaba & Neal, 2009) and various neural networks.",
            "Abstract\nWe introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine- tuned to follow instructions, Mixtral 8x7B â Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â chat model on human bench- marks. Both the base and instruct models are released under the Apache 2.0 license.\nCode: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/mixtral-of-experts/",
            "[10] Jwala Dhamala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada Pruksachatkun, Kai-Wei Chang, and Rahul Gupta. Bold: Dataset and metrics for measuring biases in open-ended language generation. In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency, pages 862â872, 2021.\n[11] Artyom Eliseev and Denis Mazur. Fast inference of mixture-of-experts language models with offloading. arXiv preprint arXiv:2312.17238, 2023.\n[12] William Fedus, Jeff Dean, and Barret Zoph. A review of sparse expert models in deep learning. arXiv preprint arXiv:2209.01667, 2022.\n[13] Trevor Gale, Deepak Narayanan, Cliff Young, and Matei Zaharia. Megablocks: Efficient sparse training with mixture-of-experts. arXiv preprint arXiv:2211.15841, 2022.",
            "Code: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/mixtral-of-experts/\n# Introduction\nIn this paper, we present Mixtral 8x7B, a sparse mixture of experts model (SMoE) with open weights, licensed under Apache 2.0. Mixtral outperforms Llama 2 70B and GPT-3.5 on most benchmarks. As it only uses a subset of its parameters for every token, Mixtral allows faster inference speed at low batch-sizes, and higher throughput at large batch-sizes.\nMixtral is a sparse mixture-of-experts network. It is a decoder-only model where the feedforward block picks from a set of 8 distinct groups of parameters. At every layer, for every token, a router network chooses two of these groups (the âexpertsâ) to process the token and combine their output additively. This technique increases the number of parameters of a model while controlling cost and latency, as the model only uses a fraction of the total set of parameters per token.\nMixtral is pretrained with multilingual data using a context size of 32k tokens. It either matches or exceeds the performance of Llama 2 70B and GPT-3.5, over several benchmarks. In particular,\nMixture of Experts Layer i gating inputs af outputs router expert",
            "# 5 Conclusion and Future Work\nIn this work, we explore strategies for accelerating Mixture-of-Experts based language models on consumer hardware with limited GPU memory. We propose a MoE-centric approach to offloading\n7\nand explore how mixed quantization affects perplexity and performance on language understanding tasks. We evaluate the proposed strategies and show that they produce a significant increase in generation speed compared to naÂ¨ve approaches on consumer-grade hardware, including free-tier Google Colab.\nOur method provides a practical solution for inferencing large MoE language models on resource- constricted hardware, enabling broader access to these powerful models for research and development. As future work, we plan to explore further offloading strategies, based on speculative expert predic- tion.\n# Acknowledgements\nAuthors would like to acknowledge mobicham@ for helpful discussions on Mixtral quantization.\n# References\nAminabadi, R. Y., Rajbhandari, S., Awan, A. A., Li, C., Li, D., Zheng, E., Ruwase, O., Smith, S., Zhang, M., Rasley, J., and He, Y. Deepspeed-inference: Enabling efficient inference of transformer models at unprecedented scale. In Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis, SC â22. IEEE Press, 2022. ISBN 9784665454445.",
            "dim n_layers head_dim hidden_dim n_heads n_kv_heads context_len vocab_size num_experts top_k_experts\n# 2.1 Sparse Mixture of Experts\nWe present a brief overview of the Mixture of Experts layer (Figure 1). For a more in-depth overview, see [12]. The output of the MoE module for a given input x is determined by the weighted sum of the outputs of the expert networks, where the weights are given by the gating networkâs output. i.e. given n expert networks {E0, Ei, ..., Enâ1}, the output of the expert layer is given by: Table 1: Model architecture.\n# j nâ\nG(x)i Â· Ei(x). i=0\nHere, G(x)i denotes the n-dimensional output of the gating network for the i-th expert, and Ei(x) is the output of the i-th expert network. If the gating vector is sparse, we can avoid computing the outputs of experts whose gates are zero. There are multiple alternative ways of implementing G(x) [6, 15, 35], but a simple and performant one is implemented by taking the softmax over the Top-K logits of a linear layer [28]. We use\nG(x) := Softmax(TopK(x Â· Wg)),"
        ],
        "response": "Sparse Mixture-of-Experts (MoE) increases model size due to multiple experts, but only a fraction are active per input. [http://arxiv.org/pdf/2312.17238] For example, Mixtral-8x7B has 47B parameters but uses only 13B during inference. [https://mistral.ai/news/mixtral-of-experts/] This trade-off allows faster token generation but requires high-end GPUs for deployment. [http://arxiv.org/pdf/2312.17238]",
        "reference": "The use of sparse Mixture-of-Experts (MoE) increases the size of language models due to having multiple \"experts.\""
    },
    {
        "user_input": "How does LRU caching improve the inference speed of Mixture-of-Experts language models?",
        "retrieved_contexts": [
            "We illustrate an example of how LRU cache saves experts in Figure 1 (see caption). LRU is a very simple strategy that does not consider factors like expert activation frequencies, varying cache size between MoE layers, or any sequential patterns in expert activation. However, we found that even this simple strategy can significantly speed up inference for modern Mixture-of-Experts models such as Mixtral-8x7B (see Section 4 for detailed evaluation).\n# 3.2 Speculative Expert Loading\nWhile LRU caching can reduce the average expert loading time, most of the inference time is still spent waiting for the next expert to be loaded. The reason behind this is that, unlike with dense models, MoE offloading cannot effectively overlap expert loading with computation. To understand this problem, let us zoom into the process of generating a single token, layer-by-layer. The full compute workload starts by embedding the previous token via look-up, then alternates between running self-attention and MLP for each transformer block in the model. Finally, the outputs from the last transformer block are used to predict next token logits with a linear projection.",
            "Figure 1: An example of expert loading pattern in Mixtral-8x7B-Instruct for select layers. Blue cells indicate that a certain expert was active when encoding a certain token; deeper blue indicates higher gating weight. Small gray squares show which experts are cached with an LRU cache for k=2.\n# 3.1 Expert Locality and LRU caching\nAs we discussed earlier in Section 2.1, Mixture-of-Experts language models were often observed to assign individual experts to distinct sub-tasks. However, this does not mean that the model uses the same expert over long stretches of tokens. Instead, some experts are active in short sequences of 2-4 tokens, while others are often used with âgapsâ, as shown in Figure 1.\nTo take advantage of this pattern, we can keep active experts in GPU memory as a âcacheâ for future tokens. If the same experts are activated again in future, they will be available instantaneously. Naturally, the number of experts that can be stored this way if very limited by the available GPU memory. For simplicity, we choose to always keep k least recently used experts as a type of LRU cache. If k is greater than the number of active experts, the cache will save experts from multiple previous tokens. For simplicity, we keep the same number of cached experts for each MoE layer.",
            "7Notably, Google Colab RAM cannot fit Mixtral-8x7B with a reasonable compression rate 8This corresponds to tensor.pin_memory() command in PyTorch.\n5\niy & cache_size =3 cache_size = 2 cache_size =4 0.84 | PIO â prefetch 1 experts ~ escent ae | PRS aa 0.2} ââ prefetch 2 experts ââ prefetch 3 experts 0.0 00 0 5 10 15 20 25 30 0 5 10 15 20 25 30 Layer # Layer # S Fd Ed Cache hit rate Bd ES Prediction recall = ES Ss &\nFigure 2: (left) LRU cache hit ratio for different cache size k; (right) speculative loading recall when pre-loading a different number of experts. Regular lines represent loading 1 layer ahead; dashed line stands for 2 layers ahead; dotted line is 10 layers ahead.\n# 4.1 Expert LRU Cache and Speculative Loading\nIn this section, we benchmark the effectiveness of the two expert offloading strategies: LRU caching and and speculative loading, as defined in Sections 3.1 and 3.2 respectively. For this evaluation, we measure âexpert recallâ â the fraction of times when an expert needed for inference was already available on GPU.",
            "Expert Offloading. As described earlier, we use LRU cache with an equal number k of cached experts per layer. For Mixtral-8x7B, we use k=2 for 12GB GPUs and k=4 for 16GB ones. We trigger speculative expert loading immediately after the system finished loading all experts for the current layer. The speculative expert loading fetches 1 â 2 most likely experts. The newly loaded experts do not replace the currently cached experts. If a speculatively loaded expert was later used during next layer inference, it will replace the least recently used expert from the next layerâs cache.\nMany consumer devices and free-tier cloud instances have limited host RAM that cannot fit the entire model7. In these cases, the experts must be split between host and device memory. To support this, our implementation of expert LRU cache splits experts between host and GPU devices. When loading and expert to the GPU cache, the system also offloads the least recently used on-device expert back to RAM so as to preserve memory parity.",
            "For this evaluation, we run Mixtral-8x7B-Instruct model on the OpenAssistant dataset (KÃ¶pf et al., 2023). We test LRU caching by running the model on recorded conversations and measuring the recall (aka âhit ratioâ from caching perspective) for different cache sizes k. Next, we test speculative loading in isolation by âguessingâ which experts should be loaded (by applying the next layerâs gating function on current layer activations), then measuring how often the actual next experts get loaded this way. A recall of 1.0 corresponds to a situation where both (2) Mixtral active experts were pre-fetched. We test speculative loading in three settings: 1, 2 and 10 layers ahead.\n# 4.2 Mixed MoE Quantization",
            "# 5 Conclusion and Future Work\nIn this work, we explore strategies for accelerating Mixture-of-Experts based language models on consumer hardware with limited GPU memory. We propose a MoE-centric approach to offloading\n7\nand explore how mixed quantization affects perplexity and performance on language understanding tasks. We evaluate the proposed strategies and show that they produce a significant increase in generation speed compared to naÂ¨ve approaches on consumer-grade hardware, including free-tier Google Colab.\nOur method provides a practical solution for inferencing large MoE language models on resource- constricted hardware, enabling broader access to these powerful models for research and development. As future work, we plan to explore further offloading strategies, based on speculative expert predic- tion.\n# Acknowledgements\nAuthors would like to acknowledge mobicham@ for helpful discussions on Mixtral quantization.\n# References\nAminabadi, R. Y., Rajbhandari, S., Awan, A. A., Li, C., Li, D., Zheng, E., Ruwase, O., Smith, S., Zhang, M., Rasley, J., and He, Y. Deepspeed-inference: Enabling efficient inference of transformer models at unprecedented scale. In Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis, SC â22. IEEE Press, 2022. ISBN 9784665454445.",
            "To speed up offloading in practice, we allocate all expert parameters in a contiguous memory buffer that can be moved as a single host-to-device copy. For host-side (RAM) experts, we pin8 this memory buffer for faster communication. Our implementation additionally allocates b=4 on-device buffers used to copy and prefetch experts asynchronously, without modifying existing experts. These buffers are shared between all MoE layers to reduce memory footprint. Overall, the system requires num_layers Ã num_experts expert memory buffers split between host and device memory and b=4 temporary buffers, the size of each buffer being equal to a single expert.\n# 4 Experiments\nIn this section, we verify our earlier hypotheses about MoE behavior and benchmark the inference latency in different conditions. We focus our evaluations on Mixtral-8x7B and Mixtral-8x7B-Instruct models since they represent the current state of the art among open-access MoE models. We organize this section as follows: Section 4.1 measures the effectiveness of expert caching and pre-loading in isolation, Section 4.2 compares different model compression algorithms and verifies our hypotheses from Section 3.3. Finally, Section 4.3 measures the inference latency in several hardware setups.",
            "The generative inference workload consists of two phases: 1) encoding the input prompt and 2) generating tokens conditioned on that prompt. The key difference between these two phases is that prompt tokens are encoded in parallel (layer-by-layer), whereas the generation runs sequentially (token-by-token and layer-by-layer). In general, phase 1 works relatively well with existing Mixture- of-Experts algorithms, since each layer can only be loaded once for the entire prompt. In turn, when generating tokens, one must load layer once per each token generated. In practice, this means that inference speed is limited by how fast one can fetch parameters from system memory.\nBelow, we look for patterns in how the MoE model loads its experts and propose ways to exploit these patterns to speed up inference time.\n4To learn more about these methods, please refer to surveys such as Gholami et al. (2021); Liang et al. (2021) 5As opposed to running a processing a large batch of texts over many seconds, as in Sheng et al. (2023)\n3\nSelected experts for Mixtral-8x7B-Instruct woe 0 (top) and 15 ae =n a oa ao a âme: a n: ee Layer 15 expert # Layer 0 expert # MAUR STARR O However about |= and 4 training data owerful language model based trained Trans former f architecture",
            "Shazeer et al. (2017) builds on this idea to train a sparsely gated Mixture-of-Experts to serve as a language model. The full model consists of a recurrent neural network backbone and a MoE module with up to 131072 experts. When processing a given token, a linear gating function select 4 most suitable experts based on the latest hidden state. The resulting model (including the gating function and experts) is trained end-to-end to minimize cross-entropy, with an additional regularizer to promote equal expert utilization. Shazeer et al. (2017) observed that the MoE model not only improves perplexity, but also learns interpretable expert specializations: some experts would âspecializeâ on prepositions, while others learn to express a particular concept (e.g. speed).",
            "# Denis Mazur Moscow Institute of Physics and Technology Yandex Researchcore denismazur8@gmail.com\n# Abstract\nWith the widespread adoption of Large Language Models (LLMs), many deep learning practitioners are looking for strategies of running these models more efficiently. One such strategy is to use sparse Mixture-of-Experts (MoE) â a type of model architectures where only a fraction of model layers are active for any given input. This property allows MoE-based language models to generate tokens faster than their âdenseâ counterparts, but it also increases model size due to having multiple âexpertsâ. Unfortunately, this makes state-of-the-art MoE language models difficult to run without high-end GPUs. In this work, we study the problem of running large MoE language models on consumer hardware with limited accelerator memory. We build upon parameter offloading algorithms and propose a novel strategy that accelerates offloading by taking advantage of innate properties of MoE LLMs. Using this strategy, we build can run Mixtral-8x7B with mixed quantization on desktop hardware and free-tier Google Colab instances.\n# Introduction"
        ],
        "response": "LRU caching improves inference speed by keeping frequently used experts in GPU memory, reducing loading time for subsequent activations. It exploits the pattern of experts being active in short sequences of 2-4 tokens, as shown in Figure 1. The cache stores the k least recently used experts per MoE layer, ensuring instant availability for future tokens. This strategy significantly speeds up inference for models like Mixtral-8x7B, as demonstrated in Section 4. Cache hit rates vary with cache size, with higher k improving recall but limited by GPU memory [source: http://arxiv.org/pdf/2312.17238].",
        "reference": "LRU caching improves the inference speed of Mixture-of-Experts language models by keeping active experts in GPU memory as a cache for future tokens. This allows for faster retrieval of experts when needed during inference."
    },
    {
        "user_input": "In terms of expert loading time, what impact does the use of LRU caching have on Mixture-of-Experts language models?",
        "retrieved_contexts": [
            "We illustrate an example of how LRU cache saves experts in Figure 1 (see caption). LRU is a very simple strategy that does not consider factors like expert activation frequencies, varying cache size between MoE layers, or any sequential patterns in expert activation. However, we found that even this simple strategy can significantly speed up inference for modern Mixture-of-Experts models such as Mixtral-8x7B (see Section 4 for detailed evaluation).\n# 3.2 Speculative Expert Loading\nWhile LRU caching can reduce the average expert loading time, most of the inference time is still spent waiting for the next expert to be loaded. The reason behind this is that, unlike with dense models, MoE offloading cannot effectively overlap expert loading with computation. To understand this problem, let us zoom into the process of generating a single token, layer-by-layer. The full compute workload starts by embedding the previous token via look-up, then alternates between running self-attention and MLP for each transformer block in the model. Finally, the outputs from the last transformer block are used to predict next token logits with a linear projection.",
            "Figure 1: An example of expert loading pattern in Mixtral-8x7B-Instruct for select layers. Blue cells indicate that a certain expert was active when encoding a certain token; deeper blue indicates higher gating weight. Small gray squares show which experts are cached with an LRU cache for k=2.\n# 3.1 Expert Locality and LRU caching\nAs we discussed earlier in Section 2.1, Mixture-of-Experts language models were often observed to assign individual experts to distinct sub-tasks. However, this does not mean that the model uses the same expert over long stretches of tokens. Instead, some experts are active in short sequences of 2-4 tokens, while others are often used with âgapsâ, as shown in Figure 1.\nTo take advantage of this pattern, we can keep active experts in GPU memory as a âcacheâ for future tokens. If the same experts are activated again in future, they will be available instantaneously. Naturally, the number of experts that can be stored this way if very limited by the available GPU memory. For simplicity, we choose to always keep k least recently used experts as a type of LRU cache. If k is greater than the number of active experts, the cache will save experts from multiple previous tokens. For simplicity, we keep the same number of cached experts for each MoE layer.",
            "Expert Offloading. As described earlier, we use LRU cache with an equal number k of cached experts per layer. For Mixtral-8x7B, we use k=2 for 12GB GPUs and k=4 for 16GB ones. We trigger speculative expert loading immediately after the system finished loading all experts for the current layer. The speculative expert loading fetches 1 â 2 most likely experts. The newly loaded experts do not replace the currently cached experts. If a speculatively loaded expert was later used during next layer inference, it will replace the least recently used expert from the next layerâs cache.\nMany consumer devices and free-tier cloud instances have limited host RAM that cannot fit the entire model7. In these cases, the experts must be split between host and device memory. To support this, our implementation of expert LRU cache splits experts between host and GPU devices. When loading and expert to the GPU cache, the system also offloads the least recently used on-device expert back to RAM so as to preserve memory parity.",
            "For this evaluation, we run Mixtral-8x7B-Instruct model on the OpenAssistant dataset (KÃ¶pf et al., 2023). We test LRU caching by running the model on recorded conversations and measuring the recall (aka âhit ratioâ from caching perspective) for different cache sizes k. Next, we test speculative loading in isolation by âguessingâ which experts should be loaded (by applying the next layerâs gating function on current layer activations), then measuring how often the actual next experts get loaded this way. A recall of 1.0 corresponds to a situation where both (2) Mixtral active experts were pre-fetched. We test speculative loading in three settings: 1, 2 and 10 layers ahead.\n# 4.2 Mixed MoE Quantization",
            "7Notably, Google Colab RAM cannot fit Mixtral-8x7B with a reasonable compression rate 8This corresponds to tensor.pin_memory() command in PyTorch.\n5\niy & cache_size =3 cache_size = 2 cache_size =4 0.84 | PIO â prefetch 1 experts ~ escent ae | PRS aa 0.2} ââ prefetch 2 experts ââ prefetch 3 experts 0.0 00 0 5 10 15 20 25 30 0 5 10 15 20 25 30 Layer # Layer # S Fd Ed Cache hit rate Bd ES Prediction recall = ES Ss &\nFigure 2: (left) LRU cache hit ratio for different cache size k; (right) speculative loading recall when pre-loading a different number of experts. Regular lines represent loading 1 layer ahead; dashed line stands for 2 layers ahead; dotted line is 10 layers ahead.\n# 4.1 Expert LRU Cache and Speculative Loading\nIn this section, we benchmark the effectiveness of the two expert offloading strategies: LRU caching and and speculative loading, as defined in Sections 3.1 and 3.2 respectively. For this evaluation, we measure âexpert recallâ â the fraction of times when an expert needed for inference was already available on GPU.",
            "For regular (dense) models, this architecture allows for efficient offloading schedule that pre-loads the next transformer layer ahead of time, while the previous layer is still running. Unfortunately, this schedule is no longer possible for Mixture-of-Experts models, where MoE MLP layers choose which experts to load just-in-time for computation. This is because the system cannot pre-fetch the next layer until it learns which experts should be loaded. Modern open-access MoE language models choose active experts using the final outputs of the previous layer, which means they cannot be pre-fetched them in parallel with previous layer. While it is not possible6 to pre-reliably prefetch the next set of experts ahead of time, the system could still try to guess the likely next experts and load them speculatively, while processing the previous layer. It the guess is correct, it will speed up the next layer inference; if not, it can load the actual next layerâs experts later. In other words, this type of speculative loading does not change the final model predictions, but may reduce latency if the guess is accurate enough.\n6More specifically, not possible without changing the model architecture, which would require re-training\n4",
            "1When deployed in 16-bit precision, Falcon-180B needs approximately 360GB, while LLaMA-2 70B requires 140GB of combined accelerator memory.\nIn this work, we systematically develop techniques for running large MoE language models with limited GPU memory. Our main objective is inferencing (generating tokens) with Mixtral-8x7B- Instruct â a MoE-based chat assistant â on a desktop-grade hardware where only a fraction of experts fit into the accelerator memory. To that end:\nwe observe how MoE language model accesses its experts between tokens, and find several regularities: i) some experts are reused between adjacent tokens and ii) the model hidden states of early layers already âknowâ which experts are to be used at subsequent layers. â¢ we design a MoE-specific offloading strategy that takes advantage of these regularities: i) it uses LRU cache to significantly reduces GPU-RAM communication, leading to faster generation and ii) it guesses which experts are needed ahead of time to better overlap expert loading with computation.\nâ¢ we consider the specific scenario of running Mixtral-8x7B-Instruct on a T4, RTX 3060 and RTX 3080 Mobile and develop a practical combination of mixed quantization and the proposed offloading algorithm to run this model interactively at 2-3 tokens per second depending on the hardware. The source code with our implementation is available online2",
            "To speed up offloading in practice, we allocate all expert parameters in a contiguous memory buffer that can be moved as a single host-to-device copy. For host-side (RAM) experts, we pin8 this memory buffer for faster communication. Our implementation additionally allocates b=4 on-device buffers used to copy and prefetch experts asynchronously, without modifying existing experts. These buffers are shared between all MoE layers to reduce memory footprint. Overall, the system requires num_layers Ã num_experts expert memory buffers split between host and device memory and b=4 temporary buffers, the size of each buffer being equal to a single expert.\n# 4 Experiments\nIn this section, we verify our earlier hypotheses about MoE behavior and benchmark the inference latency in different conditions. We focus our evaluations on Mixtral-8x7B and Mixtral-8x7B-Instruct models since they represent the current state of the art among open-access MoE models. We organize this section as follows: Section 4.1 measures the effectiveness of expert caching and pre-loading in isolation, Section 4.2 compares different model compression algorithms and verifies our hypotheses from Section 3.3. Finally, Section 4.3 measures the inference latency in several hardware setups.",
            "Table 1: Perplexity and model size evaluation of Mixtral-8x7B with different quantization for shared attention (Attn quant) and experts (Experts quant) layers. For comprarison, a Mistral-7B 4-bit quantized model has Wiki2 perplexity 5.03, C4 perplexity 7.56 and MMLU score 61.3%. See Section 4.2 for details. Green values correspond to the configurations we chose for full system evaluation.\n6\nAlgorithm 2-bit Experts 3-bit Experts A100 3080 Mobile 3060 T4 (Colab) A100 3080 Mobile 3060 T4 (Cloud) 3.061 Full algorithm 2.918 W/o expert pre-loading 2.265 W/o LRU cache & pre-loading Naive offloading (accelerate) 1.392 2.655 2.227 1.758 1.059 2.278 2.051 1.547 0.919 2.092 1.567 1.168 0.661 2.845 2.683 2.055 1.246 2.475 2.024 1.595 0.914 2.038 1.857 1.346 1.791 1.603 1.365 1.061 0.580",
            "# 5 Conclusion and Future Work\nIn this work, we explore strategies for accelerating Mixture-of-Experts based language models on consumer hardware with limited GPU memory. We propose a MoE-centric approach to offloading\n7\nand explore how mixed quantization affects perplexity and performance on language understanding tasks. We evaluate the proposed strategies and show that they produce a significant increase in generation speed compared to naÂ¨ve approaches on consumer-grade hardware, including free-tier Google Colab.\nOur method provides a practical solution for inferencing large MoE language models on resource- constricted hardware, enabling broader access to these powerful models for research and development. As future work, we plan to explore further offloading strategies, based on speculative expert predic- tion.\n# Acknowledgements\nAuthors would like to acknowledge mobicham@ for helpful discussions on Mixtral quantization.\n# References\nAminabadi, R. Y., Rajbhandari, S., Awan, A. A., Li, C., Li, D., Zheng, E., Ruwase, O., Smith, S., Zhang, M., Rasley, J., and He, Y. Deepspeed-inference: Enabling efficient inference of transformer models at unprecedented scale. In Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis, SC â22. IEEE Press, 2022. ISBN 9784665454445."
        ],
        "response": "LRU caching reduces expert loading time by keeping recently used experts in GPU memory, improving access speed for repeated activations [http://arxiv.org/pdf/2312.17238]. It increases cache hit ratio, as shown in Figure 2, where larger cache sizes correlate with higher recall [http://arxiv.org/pdf/2312.17238]. However, most inference time still waits for expert loading due to limited overlap with computation [http://arxiv.org/pdf/2312.17238]. Speculative loading can mitigate this by pre-fetching likely experts, but LRU alone significantly speeds up MoE models like Mixtral-8x7B [http://arxiv.org/pdf/2312.17238]. The simplicity of LRU makes it effective despite not considering activation patterns [http://arxiv.org/pdf/2312.17238].",
        "reference": "The use of LRU caching can significantly speed up inference for Mixture-of-Experts language models. However, even with LRU caching, most of the inference time is still spent waiting for the next expert to be loaded. This is because Mixture-of-Experts models do not allow for pre-loading the next layer ahead of time, as the experts to be loaded are determined just-in-time for computation."
    },
    {
        "user_input": "How does the sequential generation of tokens in interactive inference impact the speed of offloading in Mixture-of-Experts language models?",
        "retrieved_contexts": [
            "# 5 Conclusion and Future Work\nIn this work, we explore strategies for accelerating Mixture-of-Experts based language models on consumer hardware with limited GPU memory. We propose a MoE-centric approach to offloading\n7\nand explore how mixed quantization affects perplexity and performance on language understanding tasks. We evaluate the proposed strategies and show that they produce a significant increase in generation speed compared to naÂ¨ve approaches on consumer-grade hardware, including free-tier Google Colab.\nOur method provides a practical solution for inferencing large MoE language models on resource- constricted hardware, enabling broader access to these powerful models for research and development. As future work, we plan to explore further offloading strategies, based on speculative expert predic- tion.\n# Acknowledgements\nAuthors would like to acknowledge mobicham@ for helpful discussions on Mixtral quantization.\n# References\nAminabadi, R. Y., Rajbhandari, S., Awan, A. A., Li, C., Li, D., Zheng, E., Ruwase, O., Smith, S., Zhang, M., Rasley, J., and He, Y. Deepspeed-inference: Enabling efficient inference of transformer models at unprecedented scale. In Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis, SC â22. IEEE Press, 2022. ISBN 9784665454445.",
            "The generative inference workload consists of two phases: 1) encoding the input prompt and 2) generating tokens conditioned on that prompt. The key difference between these two phases is that prompt tokens are encoded in parallel (layer-by-layer), whereas the generation runs sequentially (token-by-token and layer-by-layer). In general, phase 1 works relatively well with existing Mixture- of-Experts algorithms, since each layer can only be loaded once for the entire prompt. In turn, when generating tokens, one must load layer once per each token generated. In practice, this means that inference speed is limited by how fast one can fetch parameters from system memory.\nBelow, we look for patterns in how the MoE model loads its experts and propose ways to exploit these patterns to speed up inference time.\n4To learn more about these methods, please refer to surveys such as Gholami et al. (2021); Liang et al. (2021) 5As opposed to running a processing a large batch of texts over many seconds, as in Sheng et al. (2023)\n3\nSelected experts for Mixtral-8x7B-Instruct woe 0 (top) and 15 ae =n a oa ao a âme: a n: ee Layer 15 expert # Layer 0 expert # MAUR STARR O However about |= and 4 training data owerful language model based trained Trans former f architecture",
            "We illustrate an example of how LRU cache saves experts in Figure 1 (see caption). LRU is a very simple strategy that does not consider factors like expert activation frequencies, varying cache size between MoE layers, or any sequential patterns in expert activation. However, we found that even this simple strategy can significantly speed up inference for modern Mixture-of-Experts models such as Mixtral-8x7B (see Section 4 for detailed evaluation).\n# 3.2 Speculative Expert Loading\nWhile LRU caching can reduce the average expert loading time, most of the inference time is still spent waiting for the next expert to be loaded. The reason behind this is that, unlike with dense models, MoE offloading cannot effectively overlap expert loading with computation. To understand this problem, let us zoom into the process of generating a single token, layer-by-layer. The full compute workload starts by embedding the previous token via look-up, then alternates between running self-attention and MLP for each transformer block in the model. Finally, the outputs from the last transformer block are used to predict next token logits with a linear projection.",
            "1When deployed in 16-bit precision, Falcon-180B needs approximately 360GB, while LLaMA-2 70B requires 140GB of combined accelerator memory.\nIn this work, we systematically develop techniques for running large MoE language models with limited GPU memory. Our main objective is inferencing (generating tokens) with Mixtral-8x7B- Instruct â a MoE-based chat assistant â on a desktop-grade hardware where only a fraction of experts fit into the accelerator memory. To that end:\nwe observe how MoE language model accesses its experts between tokens, and find several regularities: i) some experts are reused between adjacent tokens and ii) the model hidden states of early layers already âknowâ which experts are to be used at subsequent layers. â¢ we design a MoE-specific offloading strategy that takes advantage of these regularities: i) it uses LRU cache to significantly reduces GPU-RAM communication, leading to faster generation and ii) it guesses which experts are needed ahead of time to better overlap expert loading with computation.\nâ¢ we consider the specific scenario of running Mixtral-8x7B-Instruct on a T4, RTX 3060 and RTX 3080 Mobile and develop a practical combination of mixed quantization and the proposed offloading algorithm to run this model interactively at 2-3 tokens per second depending on the hardware. The source code with our implementation is available online2",
            "This technique works particularly well when processing large batches of data, during train- ing Pudipeddi et al. (2020); Ren et al. (2021) or large-batch non-interactive inference Aminabadi et al. (2022); Sheng et al. (2023), where each layer processes a lot of tokens each time the layer is loaded from RAM. In turn, when doing interactive inference (e.g. as a chat assistants), offloading works significantly slower than on-device inference. This is because interactive inference generates tokens autoregressively, from left to right. This way, the inference system processes one or few tokens at a time, and therefore spends most of the time waiting for next layerâs parameters to be loaded.\n# 2.4 Hardware Setup",
            "# 2.4 Hardware Setup\nWhile our analysis is not specific to any hardware setup, we target the hardware specifications of cheap / free-tier cloud instances Google (2023) and the upper half of gaming computers Steam (2023): i) enough system memory to hold model parameters, ii) a GPU with 11-16GB VRAM and iii) host-to-device communication at 8-16GB/s (PCIe Gen.3). If we examine popular open-access MoE models (Mixtral-8x7B and switch-c-2048), we find that all non-experts can fit a fraction of available GPU memory. In turn, the experts that constitute vast majority of model parameters do not fit even with quantization. Finally, even if we could fit the model parameters in memory, running generative inference requires additional memory for layer activations and past attention keys & values.\n# 3 Method\nIn this work, we aim to systematically find the optimal way to inference modern Mixture-of-Experts LLMs on desktop or low-end cloud instances. More specifically, we focus on the task of generating tokens interactively, i.e. generate multiple tokens per second at batch size 15.",
            "# Denis Mazur Moscow Institute of Physics and Technology Yandex Researchcore denismazur8@gmail.com\n# Abstract\nWith the widespread adoption of Large Language Models (LLMs), many deep learning practitioners are looking for strategies of running these models more efficiently. One such strategy is to use sparse Mixture-of-Experts (MoE) â a type of model architectures where only a fraction of model layers are active for any given input. This property allows MoE-based language models to generate tokens faster than their âdenseâ counterparts, but it also increases model size due to having multiple âexpertsâ. Unfortunately, this makes state-of-the-art MoE language models difficult to run without high-end GPUs. In this work, we study the problem of running large MoE language models on consumer hardware with limited accelerator memory. We build upon parameter offloading algorithms and propose a novel strategy that accelerates offloading by taking advantage of innate properties of MoE LLMs. Using this strategy, we build can run Mixtral-8x7B with mixed quantization on desktop hardware and free-tier Google Colab instances.\n# Introduction",
            "For regular (dense) models, this architecture allows for efficient offloading schedule that pre-loads the next transformer layer ahead of time, while the previous layer is still running. Unfortunately, this schedule is no longer possible for Mixture-of-Experts models, where MoE MLP layers choose which experts to load just-in-time for computation. This is because the system cannot pre-fetch the next layer until it learns which experts should be loaded. Modern open-access MoE language models choose active experts using the final outputs of the previous layer, which means they cannot be pre-fetched them in parallel with previous layer. While it is not possible6 to pre-reliably prefetch the next set of experts ahead of time, the system could still try to guess the likely next experts and load them speculatively, while processing the previous layer. It the guess is correct, it will speed up the next layer inference; if not, it can load the actual next layerâs experts later. In other words, this type of speculative loading does not change the final model predictions, but may reduce latency if the guess is accurate enough.\n6More specifically, not possible without changing the model architecture, which would require re-training\n4",
            "# 4.3 Practical offloading performance\nFinally we evaluate the performance of the Mixtral8x7B-Instruct model using the offloading tech- niquesproposed throughout this report. Based on the perplexity evaluations from the previous section, we chose 4-bit HQQ quantization for the shared attention layers and 2- or 3-bit quantization for experts. We evaluate this system by generating tokens via sampling on OpenAssistant (KÃ¶pf et al., 2023) conversations and measuring the average number of tokens generated per second with batch size 1. For this evaluation, we always sample proportionally to the predicted probabilities, i.e. without temperature or nucleus sampling.",
            "[10] Jwala Dhamala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada Pruksachatkun, Kai-Wei Chang, and Rahul Gupta. Bold: Dataset and metrics for measuring biases in open-ended language generation. In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency, pages 862â872, 2021.\n[11] Artyom Eliseev and Denis Mazur. Fast inference of mixture-of-experts language models with offloading. arXiv preprint arXiv:2312.17238, 2023.\n[12] William Fedus, Jeff Dean, and Barret Zoph. A review of sparse expert models in deep learning. arXiv preprint arXiv:2209.01667, 2022.\n[13] Trevor Gale, Deepak Narayanan, Cliff Young, and Matei Zaharia. Megablocks: Efficient sparse training with mixture-of-experts. arXiv preprint arXiv:2211.15841, 2022."
        ],
        "response": "Sequential token generation in interactive inference limits offloading speed in MoE models because each token requires loading experts just-in-time, preventing parallel prefetching. This causes the system to spend most time waiting for parameters from system memory. LRU caching reduces expert loading time by reusing frequently accessed experts, but speculative expert loading can further improve speed by guessing next experts ahead of time. The inability to reliably prefetch experts without model retraining hinders offloading efficiency. Mixed quantization and these strategies enable interactive inference at 2-3 tokens per second on consumer hardware. [source: http://arxiv.org/pdf/2312.17238]",
        "reference": "The sequential generation of tokens in interactive inference impacts the speed of offloading in Mixture-of-Experts language models by slowing down the process. In interactive inference, tokens are generated autoregressively from left to right, processing one or a few tokens at a time. This means that the inference system spends most of the time waiting for the next layer's parameters to be loaded. Unlike regular (dense) models, where the next layer can be pre-loaded while the previous layer is still running, Mixture-of-Experts models cannot pre-fetch the next layer until it learns which experts should be loaded. Therefore, the speed of offloading is limited by how fast the parameters can be fetched from system memory."
    },
    {
        "user_input": "In comparison, how does on-device inference affect the speed of offloading in Mixture-of-Experts language models?",
        "retrieved_contexts": [
            "# 5 Conclusion and Future Work\nIn this work, we explore strategies for accelerating Mixture-of-Experts based language models on consumer hardware with limited GPU memory. We propose a MoE-centric approach to offloading\n7\nand explore how mixed quantization affects perplexity and performance on language understanding tasks. We evaluate the proposed strategies and show that they produce a significant increase in generation speed compared to naÂ¨ve approaches on consumer-grade hardware, including free-tier Google Colab.\nOur method provides a practical solution for inferencing large MoE language models on resource- constricted hardware, enabling broader access to these powerful models for research and development. As future work, we plan to explore further offloading strategies, based on speculative expert predic- tion.\n# Acknowledgements\nAuthors would like to acknowledge mobicham@ for helpful discussions on Mixtral quantization.\n# References\nAminabadi, R. Y., Rajbhandari, S., Awan, A. A., Li, C., Li, D., Zheng, E., Ruwase, O., Smith, S., Zhang, M., Rasley, J., and He, Y. Deepspeed-inference: Enabling efficient inference of transformer models at unprecedented scale. In Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis, SC â22. IEEE Press, 2022. ISBN 9784665454445.",
            "To speed up offloading in practice, we allocate all expert parameters in a contiguous memory buffer that can be moved as a single host-to-device copy. For host-side (RAM) experts, we pin8 this memory buffer for faster communication. Our implementation additionally allocates b=4 on-device buffers used to copy and prefetch experts asynchronously, without modifying existing experts. These buffers are shared between all MoE layers to reduce memory footprint. Overall, the system requires num_layers Ã num_experts expert memory buffers split between host and device memory and b=4 temporary buffers, the size of each buffer being equal to a single expert.\n# 4 Experiments\nIn this section, we verify our earlier hypotheses about MoE behavior and benchmark the inference latency in different conditions. We focus our evaluations on Mixtral-8x7B and Mixtral-8x7B-Instruct models since they represent the current state of the art among open-access MoE models. We organize this section as follows: Section 4.1 measures the effectiveness of expert caching and pre-loading in isolation, Section 4.2 compares different model compression algorithms and verifies our hypotheses from Section 3.3. Finally, Section 4.3 measures the inference latency in several hardware setups.",
            "For regular (dense) models, this architecture allows for efficient offloading schedule that pre-loads the next transformer layer ahead of time, while the previous layer is still running. Unfortunately, this schedule is no longer possible for Mixture-of-Experts models, where MoE MLP layers choose which experts to load just-in-time for computation. This is because the system cannot pre-fetch the next layer until it learns which experts should be loaded. Modern open-access MoE language models choose active experts using the final outputs of the previous layer, which means they cannot be pre-fetched them in parallel with previous layer. While it is not possible6 to pre-reliably prefetch the next set of experts ahead of time, the system could still try to guess the likely next experts and load them speculatively, while processing the previous layer. It the guess is correct, it will speed up the next layer inference; if not, it can load the actual next layerâs experts later. In other words, this type of speculative loading does not change the final model predictions, but may reduce latency if the guess is accurate enough.\n6More specifically, not possible without changing the model architecture, which would require re-training\n4",
            "Expert Offloading. As described earlier, we use LRU cache with an equal number k of cached experts per layer. For Mixtral-8x7B, we use k=2 for 12GB GPUs and k=4 for 16GB ones. We trigger speculative expert loading immediately after the system finished loading all experts for the current layer. The speculative expert loading fetches 1 â 2 most likely experts. The newly loaded experts do not replace the currently cached experts. If a speculatively loaded expert was later used during next layer inference, it will replace the least recently used expert from the next layerâs cache.\nMany consumer devices and free-tier cloud instances have limited host RAM that cannot fit the entire model7. In these cases, the experts must be split between host and device memory. To support this, our implementation of expert LRU cache splits experts between host and GPU devices. When loading and expert to the GPU cache, the system also offloads the least recently used on-device expert back to RAM so as to preserve memory parity.",
            "# 4.3 Practical offloading performance\nFinally we evaluate the performance of the Mixtral8x7B-Instruct model using the offloading tech- niquesproposed throughout this report. Based on the perplexity evaluations from the previous section, we chose 4-bit HQQ quantization for the shared attention layers and 2- or 3-bit quantization for experts. We evaluate this system by generating tokens via sampling on OpenAssistant (KÃ¶pf et al., 2023) conversations and measuring the average number of tokens generated per second with batch size 1. For this evaluation, we always sample proportionally to the predicted probabilities, i.e. without temperature or nucleus sampling.",
            "The generative inference workload consists of two phases: 1) encoding the input prompt and 2) generating tokens conditioned on that prompt. The key difference between these two phases is that prompt tokens are encoded in parallel (layer-by-layer), whereas the generation runs sequentially (token-by-token and layer-by-layer). In general, phase 1 works relatively well with existing Mixture- of-Experts algorithms, since each layer can only be loaded once for the entire prompt. In turn, when generating tokens, one must load layer once per each token generated. In practice, this means that inference speed is limited by how fast one can fetch parameters from system memory.\nBelow, we look for patterns in how the MoE model loads its experts and propose ways to exploit these patterns to speed up inference time.\n4To learn more about these methods, please refer to surveys such as Gholami et al. (2021); Liang et al. (2021) 5As opposed to running a processing a large batch of texts over many seconds, as in Sheng et al. (2023)\n3\nSelected experts for Mixtral-8x7B-Instruct woe 0 (top) and 15 ae =n a oa ao a âme: a n: ee Layer 15 expert # Layer 0 expert # MAUR STARR O However about |= and 4 training data owerful language model based trained Trans former f architecture",
            "# Denis Mazur Moscow Institute of Physics and Technology Yandex Researchcore denismazur8@gmail.com\n# Abstract\nWith the widespread adoption of Large Language Models (LLMs), many deep learning practitioners are looking for strategies of running these models more efficiently. One such strategy is to use sparse Mixture-of-Experts (MoE) â a type of model architectures where only a fraction of model layers are active for any given input. This property allows MoE-based language models to generate tokens faster than their âdenseâ counterparts, but it also increases model size due to having multiple âexpertsâ. Unfortunately, this makes state-of-the-art MoE language models difficult to run without high-end GPUs. In this work, we study the problem of running large MoE language models on consumer hardware with limited accelerator memory. We build upon parameter offloading algorithms and propose a novel strategy that accelerates offloading by taking advantage of innate properties of MoE LLMs. Using this strategy, we build can run Mixtral-8x7B with mixed quantization on desktop hardware and free-tier Google Colab instances.\n# Introduction",
            "We illustrate an example of how LRU cache saves experts in Figure 1 (see caption). LRU is a very simple strategy that does not consider factors like expert activation frequencies, varying cache size between MoE layers, or any sequential patterns in expert activation. However, we found that even this simple strategy can significantly speed up inference for modern Mixture-of-Experts models such as Mixtral-8x7B (see Section 4 for detailed evaluation).\n# 3.2 Speculative Expert Loading\nWhile LRU caching can reduce the average expert loading time, most of the inference time is still spent waiting for the next expert to be loaded. The reason behind this is that, unlike with dense models, MoE offloading cannot effectively overlap expert loading with computation. To understand this problem, let us zoom into the process of generating a single token, layer-by-layer. The full compute workload starts by embedding the previous token via look-up, then alternates between running self-attention and MLP for each transformer block in the model. Finally, the outputs from the last transformer block are used to predict next token logits with a linear projection.",
            "[10] Jwala Dhamala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada Pruksachatkun, Kai-Wei Chang, and Rahul Gupta. Bold: Dataset and metrics for measuring biases in open-ended language generation. In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency, pages 862â872, 2021.\n[11] Artyom Eliseev and Denis Mazur. Fast inference of mixture-of-experts language models with offloading. arXiv preprint arXiv:2312.17238, 2023.\n[12] William Fedus, Jeff Dean, and Barret Zoph. A review of sparse expert models in deep learning. arXiv preprint arXiv:2209.01667, 2022.\n[13] Trevor Gale, Deepak Narayanan, Cliff Young, and Matei Zaharia. Megablocks: Efficient sparse training with mixture-of-experts. arXiv preprint arXiv:2211.15841, 2022.",
            "1When deployed in 16-bit precision, Falcon-180B needs approximately 360GB, while LLaMA-2 70B requires 140GB of combined accelerator memory.\nIn this work, we systematically develop techniques for running large MoE language models with limited GPU memory. Our main objective is inferencing (generating tokens) with Mixtral-8x7B- Instruct â a MoE-based chat assistant â on a desktop-grade hardware where only a fraction of experts fit into the accelerator memory. To that end:\nwe observe how MoE language model accesses its experts between tokens, and find several regularities: i) some experts are reused between adjacent tokens and ii) the model hidden states of early layers already âknowâ which experts are to be used at subsequent layers. â¢ we design a MoE-specific offloading strategy that takes advantage of these regularities: i) it uses LRU cache to significantly reduces GPU-RAM communication, leading to faster generation and ii) it guesses which experts are needed ahead of time to better overlap expert loading with computation.\nâ¢ we consider the specific scenario of running Mixtral-8x7B-Instruct on a T4, RTX 3060 and RTX 3080 Mobile and develop a practical combination of mixed quantization and the proposed offloading algorithm to run this model interactively at 2-3 tokens per second depending on the hardware. The source code with our implementation is available online2"
        ],
        "response": "On-device inference reduces offloading speed in Mixture-of-Experts models due to increased latency from expert loading [http://arxiv.org/pdf/2312.17238]. Unlike dense models, MoE models cannot pre-fetch experts in parallel with computation because expert selection is just-in-time [http://arxiv.org/pdf/2312.17238]. Speculative loading of experts can partially mitigate this but relies on accurate predictions [http://arxiv.org/pdf/2312.17238]. LRU caching improves performance by reducing GPU-RAM communication but still requires waiting for expert loading [http://arxiv.org/pdf/2312.17238]. Mixed quantization and offloading strategies are used to run MoE models on limited hardware with 2-3 tokens per second [http://arxiv.org/pdf/2312.17238].",
        "reference": "On-device inference is faster than offloading in Mixture-of-Experts language models. This is because during interactive inference, where tokens are generated autoregressively from left to right, the inference system processes one or few tokens at a time. As a result, most of the time is spent waiting for the next layer's parameters to be loaded. While LRU caching can reduce the average expert loading time, the majority of the inference time is still spent waiting for the next expert to be loaded. Unlike dense models, MoE offloading cannot effectively overlap expert loading with computation, making on-device inference faster in comparison."
    },
    {
        "user_input": "What are the architectural differences in terms of feedforward blocks and active parameters used during inference between Mixtral 8x7B and Mistral 7B?",
        "retrieved_contexts": [
            "Abstract\nWe introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine- tuned to follow instructions, Mixtral 8x7B â Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â chat model on human bench- marks. Both the base and instruct models are released under the Apache 2.0 license.\nCode: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/mixtral-of-experts/",
            "Table 2: Comparison of Mixtral with Llama. Mixtral outperforms or matches Llama 2 70B performance on almost all popular benchmarks while using 5x fewer active parameters during inference.\n70 Mixtral 8x7B. âMixtral 8x7B Mixtral 8x7B 355 =o = Es & E60! Mistral 78 % 2681 Mistral 78 3 3 s0 5 = A % 66 50 g 4 45 64 78 138 348708 78 138 348708 78 138 348 70B S66 Mixtral 8x7B 50 Mixtral 8x7B 5 = 564 340 g al Mistral 78 ee Mistral 78 3 5 Â§ 30 5 eo â= Mistral Â° 20 âe LlaMA2 78 (138 348 70B 7B (138 348 708 7B Â«13B 34B 708 Active Params Active Params Active Params\nFigure 3: Results on MMLU, commonsense reasoning, world knowledge and reading comprehension, math and code for Mistral (7B/8x7B) vs Llama 2 (7B/13B/70B). Mixtral largely outperforms Llama 2 70B on all benchmarks, except on reading comprehension benchmarks while using 5x lower active parameters. It is also vastly superior to Llama 2 70B on code and math.",
            "We also present Mixtral 8x7B â Instruct, a chat model fine-tuned to follow instructions using supervised fine-tuning and Direct Preference Optimization [25]. Its performance notably surpasses that of GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â chat model on human evaluation benchmarks. Mixtral â Instruct also demonstrates reduced biases, and a more balanced sentiment profile in benchmarks such as BBQ, and BOLD. We release both Mixtral 8x7B and Mixtral 8x7B â Instruct under the Apache 2.0 license1, free for academic and commercial usage, ensuring broad accessibility and potential for diverse applications. To enable the community to run Mixtral with a fully open-source stack, we submitted changes to the vLLM project, which integrates Megablocks CUDA kernels for efficient inference. Skypilot also allows the deployment of vLLM endpoints on any instance in the cloud.\n# 2 Architectural details\nMixtral is based on a transformer architecture [31] and uses the same modifications as described in [18], with the notable exceptions that Mix- tral supports a fully dense context length of 32k tokens, and the feed- forward blocks are replaced by Mixture-of-Expert layers (Section 2.1). The model architecture parameters are summarized in Table 1. Parameter Value",
            "Detailed results for Mixtral, Mistral 7B and Llama 2 7B/13B/70B and Llama 1 34B2 are reported in Table 2. Figure 2 compares the performance of Mixtral with the Llama models in different categories. Mixtral surpasses Llama 2 70B across most metrics. In particular, Mixtral displays a superior performance in code and mathematics benchmarks.\nSize and Efficiency. We compare our performance to the Llama 2 family, aiming to understand Mixtral modelsâ efficiency in the cost-performance spectrum (see Figure 3). As a sparse Mixture- of-Experts model, Mixtral only uses 13B active parameters for each token. With 5x lower active parameters, Mixtral is able to outperform Llama 2 70B across most categories.\nNote that this analysis focuses on the active parameter count (see Section 2.1), which is directly proportional to the inference compute cost, but does not consider the memory costs and hardware utilization. The memory costs for serving Mixtral are proportional to its sparse parameter count, 47B, which is still smaller than Llama 2 70B. As for device utilization, we note that the SMoEs layer introduces additional overhead due to the routing mechanism and due to the increased memory loads when running more than one expert per device. They are more suitable for batched workloads where one can reach a good degree of arithmetic intensity.",
            "Code: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/mixtral-of-experts/\n# Introduction\nIn this paper, we present Mixtral 8x7B, a sparse mixture of experts model (SMoE) with open weights, licensed under Apache 2.0. Mixtral outperforms Llama 2 70B and GPT-3.5 on most benchmarks. As it only uses a subset of its parameters for every token, Mixtral allows faster inference speed at low batch-sizes, and higher throughput at large batch-sizes.\nMixtral is a sparse mixture-of-experts network. It is a decoder-only model where the feedforward block picks from a set of 8 distinct groups of parameters. At every layer, for every token, a router network chooses two of these groups (the âexpertsâ) to process the token and combine their output additively. This technique increases the number of parameters of a model while controlling cost and latency, as the model only uses a fraction of the total set of parameters per token.\nMixtral is pretrained with multilingual data using a context size of 32k tokens. It either matches or exceeds the performance of Llama 2 70B and GPT-3.5, over several benchmarks. In particular,\nMixture of Experts Layer i gating inputs af outputs router expert",
            "# 6 Conclusion\nIn this paper, we introduced Mixtral 8x7B, the first mixture-of-experts network to reach a state-of-the- art performance among open-source models. Mixtral 8x7B Instruct outperforms Claude-2.1, Gem- ini Pro, and GPT-3.5 Turbo on human evaluation benchmarks. Because it only uses two experts at each time step, Mixtral only uses 13B active parameters per token while outperforming the previous best model using 70B parameters per token (Llama 2 70B). We are making our trained and fine-tuned mod- els publicly available under the Apache 2.0 license. By sharing our models, we aim to facilitate the de- velopment of new techniques and applications that can benefit a wide range of industries and domains.",
            "Since then, several lines of work explore Mixture-of-Experts with Transformer-based language models for machine translation Lepikhin et al. (2020), masked language modeling Fedus et al. (2021), general-purpose LLMs Du et al. (2022) and others. Most of these models follow traditional (dense) Transformer architecture for embeddings and attention layers, and only use Mixture for the feedforward (MLP) blocks and use a linear token-level gating function. A common observation across most of these works is that MoE models are cheaper to train and inference Fedus et al. (2021); Lepikhin et al. (2020), but require more parameters than a dense model with equivalent perplexity. Pre-trained Mixture-of-Experts LLMs have been openly available for over a year3. However, these models seem to have gained less traction than equivalent dense models, arguable because their sheer model size (over a trillion parameters) makes them difficult to use. Most recently, Mistral AI released a family of sparse Mixture of Experts models called Mixtral-8x7B with near state-of-the-art performance Mixtral AI team (2023). This model has already inspired several follow-up works and practical applications, but it still requires a high-end GPU accelerator.\n# 2.2 Post-training Quantization of LLMs",
            "# 3.1 Multilingual benchmarks\nCompared to Mistral 7B, we significantly upsample the proportion of multilingual data during pretraining. The extra capacity allows Mixtral to perform well on multilingual benchmarks while maintaining a high accuracy in English. In particular, Mixtral significantly outperforms Llama 2 70B in French, German, Spanish, and Italian, as shown in Table 4.\nActive Params French Arc-c HellaS MMLU German Arc-c HellaS MMLU Spanish Arc-c HellaS MMLU Italian Arc-c HellaS MMLU 33B 70B 13B 42.9% 65.4% 49.0% 39.3% 68.1% 49.9% 49.9% 72.5% 64.3% 49.4% 70.9% 65.1% 58.2% 77.4% 70.9% 54.3% 73.0% 71.5% 55.4% 77.6% 72.5% 52.8% 75.1% 70.9% 41.1% 63.3% 48.7% 47.3% 68.7% 64.2% 45.7% 69.8% 52.3% 50.5% 74.5% 66.0%",
            "Comparison with Llama 2 70B and GPT-3.5. In Table 3, we report the performance of Mixtral 8x7B compared to Llama 2 70B and GPT-3.5. We observe that Mixtral performs similarly or above the two other models. On MMLU, Mixtral obtains a better performance, despite its significantly smaller capacity (47B tokens compared to 70B). For MT Bench, we report the performance of the latest GPT-3.5-Turbo model available, gpt-3.5-turbo-1106.\n2Since Llama 2 34B was not open-sourced, we report results for Llama 1 34B.\n4",
            "We illustrate an example of how LRU cache saves experts in Figure 1 (see caption). LRU is a very simple strategy that does not consider factors like expert activation frequencies, varying cache size between MoE layers, or any sequential patterns in expert activation. However, we found that even this simple strategy can significantly speed up inference for modern Mixture-of-Experts models such as Mixtral-8x7B (see Section 4 for detailed evaluation).\n# 3.2 Speculative Expert Loading\nWhile LRU caching can reduce the average expert loading time, most of the inference time is still spent waiting for the next expert to be loaded. The reason behind this is that, unlike with dense models, MoE offloading cannot effectively overlap expert loading with computation. To understand this problem, let us zoom into the process of generating a single token, layer-by-layer. The full compute workload starts by embedding the previous token via look-up, then alternates between running self-attention and MLP for each transformer block in the model. Finally, the outputs from the last transformer block are used to predict next token logits with a linear projection."
        ],
        "response": "Mixtral 8x7B uses 8 feedforward blocks per layer, while Mistral 7B uses a single feedforward block per layer. Each token in Mixtral 8x7B is processed by two selected experts per layer, resulting in 13B active parameters during inference, compared to 7B in Mistral 7B. Mixtral 8x7B has a total of 47B parameters but only activates a subset per token. [source: http://arxiv.org/pdf/2401.04088]",
        "reference": "The architectural difference between Mixtral 8x7B and Mistral 7B lies in the number of feedforward blocks (experts) in each layer. Mixtral 8x7B has 8 feedforward blocks per layer, while Mistral 7B has a different number of feedforward blocks. Additionally, during inference, each token in both models has access to 47B parameters, but only 13B active parameters are used."
    },
    {
        "user_input": "How does Mixtral's performance in code and math benchmarks compare to Llama 2 70B?",
        "retrieved_contexts": [
            "Table 2: Comparison of Mixtral with Llama. Mixtral outperforms or matches Llama 2 70B performance on almost all popular benchmarks while using 5x fewer active parameters during inference.\n70 Mixtral 8x7B. âMixtral 8x7B Mixtral 8x7B 355 =o = Es & E60! Mistral 78 % 2681 Mistral 78 3 3 s0 5 = A % 66 50 g 4 45 64 78 138 348708 78 138 348708 78 138 348 70B S66 Mixtral 8x7B 50 Mixtral 8x7B 5 = 564 340 g al Mistral 78 ee Mistral 78 3 5 Â§ 30 5 eo â= Mistral Â° 20 âe LlaMA2 78 (138 348 70B 7B (138 348 708 7B Â«13B 34B 708 Active Params Active Params Active Params\nFigure 3: Results on MMLU, commonsense reasoning, world knowledge and reading comprehension, math and code for Mistral (7B/8x7B) vs Llama 2 (7B/13B/70B). Mixtral largely outperforms Llama 2 70B on all benchmarks, except on reading comprehension benchmarks while using 5x lower active parameters. It is also vastly superior to Llama 2 70B on code and math.",
            "Detailed results for Mixtral, Mistral 7B and Llama 2 7B/13B/70B and Llama 1 34B2 are reported in Table 2. Figure 2 compares the performance of Mixtral with the Llama models in different categories. Mixtral surpasses Llama 2 70B across most metrics. In particular, Mixtral displays a superior performance in code and mathematics benchmarks.\nSize and Efficiency. We compare our performance to the Llama 2 family, aiming to understand Mixtral modelsâ efficiency in the cost-performance spectrum (see Figure 3). As a sparse Mixture- of-Experts model, Mixtral only uses 13B active parameters for each token. With 5x lower active parameters, Mixtral is able to outperform Llama 2 70B across most categories.\nNote that this analysis focuses on the active parameter count (see Section 2.1), which is directly proportional to the inference compute cost, but does not consider the memory costs and hardware utilization. The memory costs for serving Mixtral are proportional to its sparse parameter count, 47B, which is still smaller than Llama 2 70B. As for device utilization, we note that the SMoEs layer introduces additional overhead due to the routing mechanism and due to the increased memory loads when running more than one expert per device. They are more suitable for batched workloads where one can reach a good degree of arithmetic intensity.",
            "Comparison with Llama 2 70B and GPT-3.5. In Table 3, we report the performance of Mixtral 8x7B compared to Llama 2 70B and GPT-3.5. We observe that Mixtral performs similarly or above the two other models. On MMLU, Mixtral obtains a better performance, despite its significantly smaller capacity (47B tokens compared to 70B). For MT Bench, we report the performance of the latest GPT-3.5-Turbo model available, gpt-3.5-turbo-1106.\n2Since Llama 2 34B was not open-sourced, we report results for Llama 1 34B.\n4",
            "Abstract\nWe introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine- tuned to follow instructions, Mixtral 8x7B â Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â chat model on human bench- marks. Both the base and instruct models are released under the Apache 2.0 license.\nCode: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/mixtral-of-experts/",
            "2Since Llama 2 34B was not open-sourced, we report results for Llama 1 34B.\n4\nLLaMA 2 70B GPT-3.5 MMLU (MCQ in 57 subjects) 69.9% 70.0% 70.6% HellaSwag (10-shot) 87.1% 85.5% 86.7% ARC Challenge (25-shot) 85.1% 85.2% 85.8% WinoGrande (5-shot) 83.2% 81.6% 81.2% MBPP (pass@1) 49.8% 52.2% 60.7% GSM-8K (5-shot) 53.6% 57.1% 58.4% MT Bench (for Instruct Models) 6.86 8.32 8.30\n# Mixtral 8x7B\nTable 3: Comparison of Mixtral with Llama 2 70B and GPT-3.5. Mixtral outperforms or matches Llama 2 70B and GPT-3.5 performance on most metrics.\nEvaluation Differences. On some benchmarks, there are some differences between our evaluation protocol and the one reported in the Llama 2 paper: 1) on MBPP, we use the hand-verified subset 2) on TriviaQA, we do not provide Wikipedia contexts.\n# 3.1 Multilingual benchmarks",
            "Llama 2 70B Mixtral 8x7B BBQ accuracy 51.5% 56.0% BOLD sentiment score (avg Â± std) gender profession religious_ideology political_ideology race 0.293 Â± 0.073 0.218 Â± 0.073 0.188 Â± 0.133 0.149 Â± 0.140 0.232 Â± 0.049 0.323 Â±0.045 0.243 Â± 0.087 0.144 Â± 0.089 0.186 Â± 0.146 0.232 Â± 0.052\nFigure 5: Bias Benchmarks. Compared Llama 2 70B, Mixtral presents less bias (higher accuracy on BBQ, lower std on BOLD) and displays more positive sentiment (higher avg on BOLD).\nWe benchmark Llama 2 and Mixtral on BBQ and BOLD with our evaluation framework and report the results in Table 5. Compared to Llama 2, Mixtral presents less bias on the BBQ benchmark (56.0% vs 51.5%). For each group in BOLD, a higher average sentiment score means more positive sentiments and a lower standard deviation indicates less bias within the group. Overall, Mixtral displays more positive sentiments than Llama 2, with similar variances within each group.\n# Instruction Fine-tuning",
            "# 3.1 Multilingual benchmarks\nCompared to Mistral 7B, we significantly upsample the proportion of multilingual data during pretraining. The extra capacity allows Mixtral to perform well on multilingual benchmarks while maintaining a high accuracy in English. In particular, Mixtral significantly outperforms Llama 2 70B in French, German, Spanish, and Italian, as shown in Table 4.\nActive Params French Arc-c HellaS MMLU German Arc-c HellaS MMLU Spanish Arc-c HellaS MMLU Italian Arc-c HellaS MMLU 33B 70B 13B 42.9% 65.4% 49.0% 39.3% 68.1% 49.9% 49.9% 72.5% 64.3% 49.4% 70.9% 65.1% 58.2% 77.4% 70.9% 54.3% 73.0% 71.5% 55.4% 77.6% 72.5% 52.8% 75.1% 70.9% 41.1% 63.3% 48.7% 47.3% 68.7% 64.2% 45.7% 69.8% 52.3% 50.5% 74.5% 66.0%",
            "Code: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/mixtral-of-experts/\n# Introduction\nIn this paper, we present Mixtral 8x7B, a sparse mixture of experts model (SMoE) with open weights, licensed under Apache 2.0. Mixtral outperforms Llama 2 70B and GPT-3.5 on most benchmarks. As it only uses a subset of its parameters for every token, Mixtral allows faster inference speed at low batch-sizes, and higher throughput at large batch-sizes.\nMixtral is a sparse mixture-of-experts network. It is a decoder-only model where the feedforward block picks from a set of 8 distinct groups of parameters. At every layer, for every token, a router network chooses two of these groups (the âexpertsâ) to process the token and combine their output additively. This technique increases the number of parameters of a model while controlling cost and latency, as the model only uses a fraction of the total set of parameters per token.\nMixtral is pretrained with multilingual data using a context size of 32k tokens. It either matches or exceeds the performance of Llama 2 70B and GPT-3.5, over several benchmarks. In particular,\nMixture of Experts Layer i gating inputs af outputs router expert",
            "# 6 Conclusion\nIn this paper, we introduced Mixtral 8x7B, the first mixture-of-experts network to reach a state-of-the- art performance among open-source models. Mixtral 8x7B Instruct outperforms Claude-2.1, Gem- ini Pro, and GPT-3.5 Turbo on human evaluation benchmarks. Because it only uses two experts at each time step, Mixtral only uses 13B active parameters per token while outperforming the previous best model using 70B parameters per token (Llama 2 70B). We are making our trained and fine-tuned mod- els publicly available under the Apache 2.0 license. By sharing our models, we aim to facilitate the de- velopment of new techniques and applications that can benefit a wide range of industries and domains.",
            "World Knowledge (5-shot): NaturalQuestions [20], TriviaQA [19] â¢ Reading Comprehension (0-shot): BoolQ [7], QuAC [5] â¢ Math: GSM8K [9] (8-shot) with maj@8 and MATH [17] (4-shot) with maj@4 â¢ Code: Humaneval [4] (0-shot) and MBPP [1] (3-shot) â¢ Popular aggregated results: MMLU [16] (5-shot), BBH [29] (3-shot), and AGI Eval [34]\n(3-5-shot, English multiple-choice questions only)\n80 SE Mistral 78 = LLaMA27B = Sl LLaMA134B, jam Mistral 78 = LlaMA27B Ss LLAMA 1348, cee Mixtral 8x78 Sm LLaMA213BÂ° mmm LLaMA2 70B je Mixtral 8x78 mm LlaMA2138 lm LLaMA2 708 70 50 60 50 20 40 10 BH Code MMU Knowledge Reasoning âComprehension AGI Eval Math âAccuracy (%)"
        ],
        "response": "Mixtral outperforms or matches Llama 2 70B on most benchmarks including code and math, while using 5x fewer active parameters during inference. [source: http://arxiv.org/pdf/2401.04088] In particular, Mixtral vastly outperforms Llama 2 70B on mathematics and code generation benchmarks. [source: http://arxiv.org/pdf/2401.04088] For example, on MBPP (pass@1), Mixtral achieves higher performance than Llama 2 70B. [source: http://arxiv.org/pdf/2401.04088] Similarly, on GSM-8K, Mixtral demonstrates superior performance in math. [source: http://arxiv.org/pdf/2401.04088] The model also matches or exceeds GPT-3.5 performance on several metrics. [source: http://arxiv.org/pdf/2401.04088] However, the context does not specify exact numerical comparisons for code and math benchmarks. [source: http://arxiv.org/pdf/2401.04088] Mixtral's efficiency is attributed to its sparse Mixture-of-Experts architecture, which uses only 13B active parameters per token. [source: http://arxiv.org/pdf/2401.04088] This makes it more cost-effective than Llama 2 70B. [source: http://arxiv.org/pdf/2401.04088] The model also shows better performance on multilingual tasks. [source: http://arxiv.org/pdf/2401.04088] ",
        "reference": "Mixtral's performance in code and math benchmarks is superior to Llama 2 70B."
    },
    {
        "user_input": "Which model outperforms Llama 2 70B on multilingual benchmarks?",
        "retrieved_contexts": [
            "# 3.1 Multilingual benchmarks\nCompared to Mistral 7B, we significantly upsample the proportion of multilingual data during pretraining. The extra capacity allows Mixtral to perform well on multilingual benchmarks while maintaining a high accuracy in English. In particular, Mixtral significantly outperforms Llama 2 70B in French, German, Spanish, and Italian, as shown in Table 4.\nActive Params French Arc-c HellaS MMLU German Arc-c HellaS MMLU Spanish Arc-c HellaS MMLU Italian Arc-c HellaS MMLU 33B 70B 13B 42.9% 65.4% 49.0% 39.3% 68.1% 49.9% 49.9% 72.5% 64.3% 49.4% 70.9% 65.1% 58.2% 77.4% 70.9% 54.3% 73.0% 71.5% 55.4% 77.6% 72.5% 52.8% 75.1% 70.9% 41.1% 63.3% 48.7% 47.3% 68.7% 64.2% 45.7% 69.8% 52.3% 50.5% 74.5% 66.0%",
            "Abstract\nWe introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine- tuned to follow instructions, Mixtral 8x7B â Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â chat model on human bench- marks. Both the base and instruct models are released under the Apache 2.0 license.\nCode: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/mixtral-of-experts/",
            "Comparison with Llama 2 70B and GPT-3.5. In Table 3, we report the performance of Mixtral 8x7B compared to Llama 2 70B and GPT-3.5. We observe that Mixtral performs similarly or above the two other models. On MMLU, Mixtral obtains a better performance, despite its significantly smaller capacity (47B tokens compared to 70B). For MT Bench, we report the performance of the latest GPT-3.5-Turbo model available, gpt-3.5-turbo-1106.\n2Since Llama 2 34B was not open-sourced, we report results for Llama 1 34B.\n4",
            "2Since Llama 2 34B was not open-sourced, we report results for Llama 1 34B.\n4\nLLaMA 2 70B GPT-3.5 MMLU (MCQ in 57 subjects) 69.9% 70.0% 70.6% HellaSwag (10-shot) 87.1% 85.5% 86.7% ARC Challenge (25-shot) 85.1% 85.2% 85.8% WinoGrande (5-shot) 83.2% 81.6% 81.2% MBPP (pass@1) 49.8% 52.2% 60.7% GSM-8K (5-shot) 53.6% 57.1% 58.4% MT Bench (for Instruct Models) 6.86 8.32 8.30\n# Mixtral 8x7B\nTable 3: Comparison of Mixtral with Llama 2 70B and GPT-3.5. Mixtral outperforms or matches Llama 2 70B and GPT-3.5 performance on most metrics.\nEvaluation Differences. On some benchmarks, there are some differences between our evaluation protocol and the one reported in the Llama 2 paper: 1) on MBPP, we use the hand-verified subset 2) on TriviaQA, we do not provide Wikipedia contexts.\n# 3.1 Multilingual benchmarks",
            "Table 2: Comparison of Mixtral with Llama. Mixtral outperforms or matches Llama 2 70B performance on almost all popular benchmarks while using 5x fewer active parameters during inference.\n70 Mixtral 8x7B. âMixtral 8x7B Mixtral 8x7B 355 =o = Es & E60! Mistral 78 % 2681 Mistral 78 3 3 s0 5 = A % 66 50 g 4 45 64 78 138 348708 78 138 348708 78 138 348 70B S66 Mixtral 8x7B 50 Mixtral 8x7B 5 = 564 340 g al Mistral 78 ee Mistral 78 3 5 Â§ 30 5 eo â= Mistral Â° 20 âe LlaMA2 78 (138 348 70B 7B (138 348 708 7B Â«13B 34B 708 Active Params Active Params Active Params\nFigure 3: Results on MMLU, commonsense reasoning, world knowledge and reading comprehension, math and code for Mistral (7B/8x7B) vs Llama 2 (7B/13B/70B). Mixtral largely outperforms Llama 2 70B on all benchmarks, except on reading comprehension benchmarks while using 5x lower active parameters. It is also vastly superior to Llama 2 70B on code and math.",
            "Detailed results for Mixtral, Mistral 7B and Llama 2 7B/13B/70B and Llama 1 34B2 are reported in Table 2. Figure 2 compares the performance of Mixtral with the Llama models in different categories. Mixtral surpasses Llama 2 70B across most metrics. In particular, Mixtral displays a superior performance in code and mathematics benchmarks.\nSize and Efficiency. We compare our performance to the Llama 2 family, aiming to understand Mixtral modelsâ efficiency in the cost-performance spectrum (see Figure 3). As a sparse Mixture- of-Experts model, Mixtral only uses 13B active parameters for each token. With 5x lower active parameters, Mixtral is able to outperform Llama 2 70B across most categories.\nNote that this analysis focuses on the active parameter count (see Section 2.1), which is directly proportional to the inference compute cost, but does not consider the memory costs and hardware utilization. The memory costs for serving Mixtral are proportional to its sparse parameter count, 47B, which is still smaller than Llama 2 70B. As for device utilization, we note that the SMoEs layer introduces additional overhead due to the routing mechanism and due to the increased memory loads when running more than one expert per device. They are more suitable for batched workloads where one can reach a good degree of arithmetic intensity.",
            "Table 4: Comparison of Mixtral with Llama on Multilingual Benchmarks. On ARC Challenge, Hellaswag, and MMLU, Mixtral outperforms Llama 2 70B on 4 languages: French, German, Spanish, and Italian.\n# 3.2 Long range performance\nTo assess the capabilities of Mixtral to tackle long context, we evaluate it on the passkey retrieval task introduced in [23], a synthetic task designed to measure the ability of the model to retrieve a passkey inserted randomly in a long prompt. Results in Figure 4 (Left) show that Mixtral achieves a 100% retrieval accuracy regardless of the context length or the position of passkey in the sequence. Figure 4 (Right) shows that the perplexity of Mixtral on a subset of the proof-pile dataset [2] decreases monotonically as the size of the context increases.\nPasskey Performance ry 0.8 0.6 04 0.2 0.0 OK 4K 8K 12K 16K 20K 24K 28K Seq Len Passkey Loc\n3.8 â Mixtral_8x7B 3.5 32 > $3.0 i] 228 fos a 2.0 0 5k 10k 15k 20k 25k 30k Context length",
            "Code: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/mixtral-of-experts/\n# Introduction\nIn this paper, we present Mixtral 8x7B, a sparse mixture of experts model (SMoE) with open weights, licensed under Apache 2.0. Mixtral outperforms Llama 2 70B and GPT-3.5 on most benchmarks. As it only uses a subset of its parameters for every token, Mixtral allows faster inference speed at low batch-sizes, and higher throughput at large batch-sizes.\nMixtral is a sparse mixture-of-experts network. It is a decoder-only model where the feedforward block picks from a set of 8 distinct groups of parameters. At every layer, for every token, a router network chooses two of these groups (the âexpertsâ) to process the token and combine their output additively. This technique increases the number of parameters of a model while controlling cost and latency, as the model only uses a fraction of the total set of parameters per token.\nMixtral is pretrained with multilingual data using a context size of 32k tokens. It either matches or exceeds the performance of Llama 2 70B and GPT-3.5, over several benchmarks. In particular,\nMixture of Experts Layer i gating inputs af outputs router expert",
            "Llama 2 70B Mixtral 8x7B BBQ accuracy 51.5% 56.0% BOLD sentiment score (avg Â± std) gender profession religious_ideology political_ideology race 0.293 Â± 0.073 0.218 Â± 0.073 0.188 Â± 0.133 0.149 Â± 0.140 0.232 Â± 0.049 0.323 Â±0.045 0.243 Â± 0.087 0.144 Â± 0.089 0.186 Â± 0.146 0.232 Â± 0.052\nFigure 5: Bias Benchmarks. Compared Llama 2 70B, Mixtral presents less bias (higher accuracy on BBQ, lower std on BOLD) and displays more positive sentiment (higher avg on BOLD).\nWe benchmark Llama 2 and Mixtral on BBQ and BOLD with our evaluation framework and report the results in Table 5. Compared to Llama 2, Mixtral presents less bias on the BBQ benchmark (56.0% vs 51.5%). For each group in BOLD, a higher average sentiment score means more positive sentiments and a lower standard deviation indicates less bias within the group. Overall, Mixtral displays more positive sentiments than Llama 2, with similar variances within each group.\n# Instruction Fine-tuning",
            "language model series characterized by distinct models with varying parameter counts [14], and the Qwen-14B open-source chat model is utilized in the current study. Aquila2 represents a language model series devised by BAAI, noted for surpassing comparable models in terms is of performance [22], and the Aquila2-34B chat model employed in this research. LLaMA2 constitutes a suite of pre-trained and fine-tuned LLMs, with scales ranging from 7 billion to 70 billion parameters [17]. Following additional training, LLaMA2-70B is incorporated into Xinyu2-70B."
        ],
        "response": "Mixtral 8x7B outperforms Llama 2 70B on multilingual benchmarks in French, German, Spanish, and Italian [http://arxiv.org/pdf/2401.04088]. Mixtral significantly outperforms Llama 2 70B in these languages as shown in Table 4 [http://arxiv.org/pdf/2401.04088]. It also matches or exceeds Llama 2 70B and GPT-3.5 across other benchmarks [http://arxiv.org/pdf/2401.04088]. Mixtral's multilingual performance is enhanced by upsampling proportion of multilingual data during pretraining [http://arxiv.org/pdf/2401.04088]. The model uses 13B active parameters during inference, significantly fewer than Llama 2 70B [http://arxiv.org/pdf/2401.04088]. Mixtral is a sparse Mixture of Experts (SMoE) language model with 8 experts per layer [http://arxiv.org/pdf/2401.04088]. Each token is processed by two experts selected by a router network [http://arxiv.org/pdf/2401.04088]. Mixtral achieves 100% retrieval accuracy on long context tasks [http://arxiv.org/pdf/2401.04088]. The model is released under the Apache 2.0 license [http://arxiv.org/pdf/2401.04088].",
        "reference": "Mixtral outperforms Llama 2 70B on multilingual benchmarks."
    },
    {
        "user_input": "How does the use of sparse Mixture-of-Experts (MoE) in language models contribute to faster token generation?",
        "retrieved_contexts": [
            "# Denis Mazur Moscow Institute of Physics and Technology Yandex Researchcore denismazur8@gmail.com\n# Abstract\nWith the widespread adoption of Large Language Models (LLMs), many deep learning practitioners are looking for strategies of running these models more efficiently. One such strategy is to use sparse Mixture-of-Experts (MoE) â a type of model architectures where only a fraction of model layers are active for any given input. This property allows MoE-based language models to generate tokens faster than their âdenseâ counterparts, but it also increases model size due to having multiple âexpertsâ. Unfortunately, this makes state-of-the-art MoE language models difficult to run without high-end GPUs. In this work, we study the problem of running large MoE language models on consumer hardware with limited accelerator memory. We build upon parameter offloading algorithms and propose a novel strategy that accelerates offloading by taking advantage of innate properties of MoE LLMs. Using this strategy, we build can run Mixtral-8x7B with mixed quantization on desktop hardware and free-tier Google Colab instances.\n# Introduction",
            "1When deployed in 16-bit precision, Falcon-180B needs approximately 360GB, while LLaMA-2 70B requires 140GB of combined accelerator memory.\nIn this work, we systematically develop techniques for running large MoE language models with limited GPU memory. Our main objective is inferencing (generating tokens) with Mixtral-8x7B- Instruct â a MoE-based chat assistant â on a desktop-grade hardware where only a fraction of experts fit into the accelerator memory. To that end:\nwe observe how MoE language model accesses its experts between tokens, and find several regularities: i) some experts are reused between adjacent tokens and ii) the model hidden states of early layers already âknowâ which experts are to be used at subsequent layers. â¢ we design a MoE-specific offloading strategy that takes advantage of these regularities: i) it uses LRU cache to significantly reduces GPU-RAM communication, leading to faster generation and ii) it guesses which experts are needed ahead of time to better overlap expert loading with computation.\nâ¢ we consider the specific scenario of running Mixtral-8x7B-Instruct on a T4, RTX 3060 and RTX 3080 Mobile and develop a practical combination of mixed quantization and the proposed offloading algorithm to run this model interactively at 2-3 tokens per second depending on the hardware. The source code with our implementation is available online2",
            "Shazeer et al. (2017) builds on this idea to train a sparsely gated Mixture-of-Experts to serve as a language model. The full model consists of a recurrent neural network backbone and a MoE module with up to 131072 experts. When processing a given token, a linear gating function select 4 most suitable experts based on the latest hidden state. The resulting model (including the gating function and experts) is trained end-to-end to minimize cross-entropy, with an additional regularizer to promote equal expert utilization. Shazeer et al. (2017) observed that the MoE model not only improves perplexity, but also learns interpretable expert specializations: some experts would âspecializeâ on prepositions, while others learn to express a particular concept (e.g. speed).",
            "Since then, several lines of work explore Mixture-of-Experts with Transformer-based language models for machine translation Lepikhin et al. (2020), masked language modeling Fedus et al. (2021), general-purpose LLMs Du et al. (2022) and others. Most of these models follow traditional (dense) Transformer architecture for embeddings and attention layers, and only use Mixture for the feedforward (MLP) blocks and use a linear token-level gating function. A common observation across most of these works is that MoE models are cheaper to train and inference Fedus et al. (2021); Lepikhin et al. (2020), but require more parameters than a dense model with equivalent perplexity. Pre-trained Mixture-of-Experts LLMs have been openly available for over a year3. However, these models seem to have gained less traction than equivalent dense models, arguable because their sheer model size (over a trillion parameters) makes them difficult to use. Most recently, Mistral AI released a family of sparse Mixture of Experts models called Mixtral-8x7B with near state-of-the-art performance Mixtral AI team (2023). This model has already inspired several follow-up works and practical applications, but it still requires a high-end GPU accelerator.\n# 2.2 Post-training Quantization of LLMs",
            "# 5 Conclusion and Future Work\nIn this work, we explore strategies for accelerating Mixture-of-Experts based language models on consumer hardware with limited GPU memory. We propose a MoE-centric approach to offloading\n7\nand explore how mixed quantization affects perplexity and performance on language understanding tasks. We evaluate the proposed strategies and show that they produce a significant increase in generation speed compared to naÂ¨ve approaches on consumer-grade hardware, including free-tier Google Colab.\nOur method provides a practical solution for inferencing large MoE language models on resource- constricted hardware, enabling broader access to these powerful models for research and development. As future work, we plan to explore further offloading strategies, based on speculative expert predic- tion.\n# Acknowledgements\nAuthors would like to acknowledge mobicham@ for helpful discussions on Mixtral quantization.\n# References\nAminabadi, R. Y., Rajbhandari, S., Awan, A. A., Li, C., Li, D., Zheng, E., Ruwase, O., Smith, S., Zhang, M., Rasley, J., and He, Y. Deepspeed-inference: Enabling efficient inference of transformer models at unprecedented scale. In Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis, SC â22. IEEE Press, 2022. ISBN 9784665454445.",
            "Several recent works modify transformer architecture by introducing sparse Mixture-of-Experts blocks (Jacobs et al., 1991; Shazeer et al., 2017). MoE blocks contain multiple âexpertsâ (layers), as well as a âgating functionâ that selects which experts are used on a given input. As a result, the MoE block uses a small portion of all âexpertsâ for any single forward pass, allowing for more compute-efficient training Fedus et al. (2021); Du et al. (2022). Notably, MoEs are among the largest Fedus et al. (2021) and among the best Mixtral AI team (2023) of available LLMs. While Mixture-of-Experts models can be more efficient than their dense counterparts, many techniques for efficient LLM inference were not designed with MoE in mind and perform suboptimally on modern large language models that use mixture-of-experts layers.\n1When deployed in 16-bit precision, Falcon-180B needs approximately 360GB, while LLaMA-2 70B requires 140GB of combined accelerator memory.",
            "Code: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/mixtral-of-experts/\n# Introduction\nIn this paper, we present Mixtral 8x7B, a sparse mixture of experts model (SMoE) with open weights, licensed under Apache 2.0. Mixtral outperforms Llama 2 70B and GPT-3.5 on most benchmarks. As it only uses a subset of its parameters for every token, Mixtral allows faster inference speed at low batch-sizes, and higher throughput at large batch-sizes.\nMixtral is a sparse mixture-of-experts network. It is a decoder-only model where the feedforward block picks from a set of 8 distinct groups of parameters. At every layer, for every token, a router network chooses two of these groups (the âexpertsâ) to process the token and combine their output additively. This technique increases the number of parameters of a model while controlling cost and latency, as the model only uses a fraction of the total set of parameters per token.\nMixtral is pretrained with multilingual data using a context size of 32k tokens. It either matches or exceeds the performance of Llama 2 70B and GPT-3.5, over several benchmarks. In particular,\nMixture of Experts Layer i gating inputs af outputs router expert",
            "The generative inference workload consists of two phases: 1) encoding the input prompt and 2) generating tokens conditioned on that prompt. The key difference between these two phases is that prompt tokens are encoded in parallel (layer-by-layer), whereas the generation runs sequentially (token-by-token and layer-by-layer). In general, phase 1 works relatively well with existing Mixture- of-Experts algorithms, since each layer can only be loaded once for the entire prompt. In turn, when generating tokens, one must load layer once per each token generated. In practice, this means that inference speed is limited by how fast one can fetch parameters from system memory.\nBelow, we look for patterns in how the MoE model loads its experts and propose ways to exploit these patterns to speed up inference time.\n4To learn more about these methods, please refer to surveys such as Gholami et al. (2021); Liang et al. (2021) 5As opposed to running a processing a large batch of texts over many seconds, as in Sheng et al. (2023)\n3\nSelected experts for Mixtral-8x7B-Instruct woe 0 (top) and 15 ae =n a oa ao a âme: a n: ee Layer 15 expert # Layer 0 expert # MAUR STARR O However about |= and 4 training data owerful language model based trained Trans former f architecture",
            "[10] Jwala Dhamala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada Pruksachatkun, Kai-Wei Chang, and Rahul Gupta. Bold: Dataset and metrics for measuring biases in open-ended language generation. In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency, pages 862â872, 2021.\n[11] Artyom Eliseev and Denis Mazur. Fast inference of mixture-of-experts language models with offloading. arXiv preprint arXiv:2312.17238, 2023.\n[12] William Fedus, Jeff Dean, and Barret Zoph. A review of sparse expert models in deep learning. arXiv preprint arXiv:2209.01667, 2022.\n[13] Trevor Gale, Deepak Narayanan, Cliff Young, and Matei Zaharia. Megablocks: Efficient sparse training with mixture-of-experts. arXiv preprint arXiv:2211.15841, 2022.",
            "We illustrate an example of how LRU cache saves experts in Figure 1 (see caption). LRU is a very simple strategy that does not consider factors like expert activation frequencies, varying cache size between MoE layers, or any sequential patterns in expert activation. However, we found that even this simple strategy can significantly speed up inference for modern Mixture-of-Experts models such as Mixtral-8x7B (see Section 4 for detailed evaluation).\n# 3.2 Speculative Expert Loading\nWhile LRU caching can reduce the average expert loading time, most of the inference time is still spent waiting for the next expert to be loaded. The reason behind this is that, unlike with dense models, MoE offloading cannot effectively overlap expert loading with computation. To understand this problem, let us zoom into the process of generating a single token, layer-by-layer. The full compute workload starts by embedding the previous token via look-up, then alternates between running self-attention and MLP for each transformer block in the model. Finally, the outputs from the last transformer block are used to predict next token logits with a linear projection."
        ],
        "response": "Sparse Mixture-of-Experts (MoE) language models generate tokens faster by activating only a fraction of experts per token, reducing computational load. The gating function selects the most suitable experts, enabling efficient token processing. LRU caching reduces GPU-RAM communication by reusing experts between adjacent tokens. Speculative expert loading overlaps expert fetching with computation, minimizing idle time. These optimizations allow MoE models like Mixtral-8x7B to achieve faster inference on consumer hardware.[http://arxiv.org/pdf/2312.17238]",
        "reference": "The use of sparse Mixture-of-Experts (MoE) in language models contributes to faster token generation because only a fraction of model layers are active for any given input. This means that MoE-based language models can generate tokens faster than their \"dense\" counterparts. The MoE blocks in the model contain multiple \"experts\" (layers) and a \"gating function\" that selects which experts are used for a given input. By using a small portion of all \"experts\" for each forward pass, the MoE block allows for more compute-efficient training, leading to faster token generation."
    },
    {
        "user_input": "What impact does the use of sparse Mixture-of-Experts (MoE) have on the size of language models?",
        "retrieved_contexts": [
            "# Denis Mazur Moscow Institute of Physics and Technology Yandex Researchcore denismazur8@gmail.com\n# Abstract\nWith the widespread adoption of Large Language Models (LLMs), many deep learning practitioners are looking for strategies of running these models more efficiently. One such strategy is to use sparse Mixture-of-Experts (MoE) â a type of model architectures where only a fraction of model layers are active for any given input. This property allows MoE-based language models to generate tokens faster than their âdenseâ counterparts, but it also increases model size due to having multiple âexpertsâ. Unfortunately, this makes state-of-the-art MoE language models difficult to run without high-end GPUs. In this work, we study the problem of running large MoE language models on consumer hardware with limited accelerator memory. We build upon parameter offloading algorithms and propose a novel strategy that accelerates offloading by taking advantage of innate properties of MoE LLMs. Using this strategy, we build can run Mixtral-8x7B with mixed quantization on desktop hardware and free-tier Google Colab instances.\n# Introduction",
            "Several recent works modify transformer architecture by introducing sparse Mixture-of-Experts blocks (Jacobs et al., 1991; Shazeer et al., 2017). MoE blocks contain multiple âexpertsâ (layers), as well as a âgating functionâ that selects which experts are used on a given input. As a result, the MoE block uses a small portion of all âexpertsâ for any single forward pass, allowing for more compute-efficient training Fedus et al. (2021); Du et al. (2022). Notably, MoEs are among the largest Fedus et al. (2021) and among the best Mixtral AI team (2023) of available LLMs. While Mixture-of-Experts models can be more efficient than their dense counterparts, many techniques for efficient LLM inference were not designed with MoE in mind and perform suboptimally on modern large language models that use mixture-of-experts layers.\n1When deployed in 16-bit precision, Falcon-180B needs approximately 360GB, while LLaMA-2 70B requires 140GB of combined accelerator memory.",
            "Since then, several lines of work explore Mixture-of-Experts with Transformer-based language models for machine translation Lepikhin et al. (2020), masked language modeling Fedus et al. (2021), general-purpose LLMs Du et al. (2022) and others. Most of these models follow traditional (dense) Transformer architecture for embeddings and attention layers, and only use Mixture for the feedforward (MLP) blocks and use a linear token-level gating function. A common observation across most of these works is that MoE models are cheaper to train and inference Fedus et al. (2021); Lepikhin et al. (2020), but require more parameters than a dense model with equivalent perplexity. Pre-trained Mixture-of-Experts LLMs have been openly available for over a year3. However, these models seem to have gained less traction than equivalent dense models, arguable because their sheer model size (over a trillion parameters) makes them difficult to use. Most recently, Mistral AI released a family of sparse Mixture of Experts models called Mixtral-8x7B with near state-of-the-art performance Mixtral AI team (2023). This model has already inspired several follow-up works and practical applications, but it still requires a high-end GPU accelerator.\n# 2.2 Post-training Quantization of LLMs",
            "Shazeer et al. (2017) builds on this idea to train a sparsely gated Mixture-of-Experts to serve as a language model. The full model consists of a recurrent neural network backbone and a MoE module with up to 131072 experts. When processing a given token, a linear gating function select 4 most suitable experts based on the latest hidden state. The resulting model (including the gating function and experts) is trained end-to-end to minimize cross-entropy, with an additional regularizer to promote equal expert utilization. Shazeer et al. (2017) observed that the MoE model not only improves perplexity, but also learns interpretable expert specializations: some experts would âspecializeâ on prepositions, while others learn to express a particular concept (e.g. speed).",
            "# 2 Background & Related Work\n# 2.1 Mixture-of-Experts\nThe recent surge in MoE language models builds on a relatively old idea (Jacobs et al., 1991; Jordan & Jacobs, 1994) of training ensembles of specialized models (âexpertsâ) and a gating function to select the right expert for the task. To achieve specialization, Mixture-of-Experts learn by simultaneously i) training the gating function to choose the best experts and ii) training the experts themselves on samples assigned to them by the gating function. Since then, many different MoE variants emerged, including mixture of SVM models (Collobert et al., 2002), Dirichlet processes (Shahbaba & Neal, 2009) and various neural networks.",
            "Abstract\nWe introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine- tuned to follow instructions, Mixtral 8x7B â Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â chat model on human bench- marks. Both the base and instruct models are released under the Apache 2.0 license.\nCode: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/mixtral-of-experts/",
            "[10] Jwala Dhamala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada Pruksachatkun, Kai-Wei Chang, and Rahul Gupta. Bold: Dataset and metrics for measuring biases in open-ended language generation. In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency, pages 862â872, 2021.\n[11] Artyom Eliseev and Denis Mazur. Fast inference of mixture-of-experts language models with offloading. arXiv preprint arXiv:2312.17238, 2023.\n[12] William Fedus, Jeff Dean, and Barret Zoph. A review of sparse expert models in deep learning. arXiv preprint arXiv:2209.01667, 2022.\n[13] Trevor Gale, Deepak Narayanan, Cliff Young, and Matei Zaharia. Megablocks: Efficient sparse training with mixture-of-experts. arXiv preprint arXiv:2211.15841, 2022.",
            "Code: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/mixtral-of-experts/\n# Introduction\nIn this paper, we present Mixtral 8x7B, a sparse mixture of experts model (SMoE) with open weights, licensed under Apache 2.0. Mixtral outperforms Llama 2 70B and GPT-3.5 on most benchmarks. As it only uses a subset of its parameters for every token, Mixtral allows faster inference speed at low batch-sizes, and higher throughput at large batch-sizes.\nMixtral is a sparse mixture-of-experts network. It is a decoder-only model where the feedforward block picks from a set of 8 distinct groups of parameters. At every layer, for every token, a router network chooses two of these groups (the âexpertsâ) to process the token and combine their output additively. This technique increases the number of parameters of a model while controlling cost and latency, as the model only uses a fraction of the total set of parameters per token.\nMixtral is pretrained with multilingual data using a context size of 32k tokens. It either matches or exceeds the performance of Llama 2 70B and GPT-3.5, over several benchmarks. In particular,\nMixture of Experts Layer i gating inputs af outputs router expert",
            "dim n_layers head_dim hidden_dim n_heads n_kv_heads context_len vocab_size num_experts top_k_experts\n# 2.1 Sparse Mixture of Experts\nWe present a brief overview of the Mixture of Experts layer (Figure 1). For a more in-depth overview, see [12]. The output of the MoE module for a given input x is determined by the weighted sum of the outputs of the expert networks, where the weights are given by the gating networkâs output. i.e. given n expert networks {E0, Ei, ..., Enâ1}, the output of the expert layer is given by: Table 1: Model architecture.\n# j nâ\nG(x)i Â· Ei(x). i=0\nHere, G(x)i denotes the n-dimensional output of the gating network for the i-th expert, and Ei(x) is the output of the i-th expert network. If the gating vector is sparse, we can avoid computing the outputs of experts whose gates are zero. There are multiple alternative ways of implementing G(x) [6, 15, 35], but a simple and performant one is implemented by taking the softmax over the Top-K logits of a linear layer [28]. We use\nG(x) := Softmax(TopK(x Â· Wg)),",
            "# 5 Conclusion and Future Work\nIn this work, we explore strategies for accelerating Mixture-of-Experts based language models on consumer hardware with limited GPU memory. We propose a MoE-centric approach to offloading\n7\nand explore how mixed quantization affects perplexity and performance on language understanding tasks. We evaluate the proposed strategies and show that they produce a significant increase in generation speed compared to naÂ¨ve approaches on consumer-grade hardware, including free-tier Google Colab.\nOur method provides a practical solution for inferencing large MoE language models on resource- constricted hardware, enabling broader access to these powerful models for research and development. As future work, we plan to explore further offloading strategies, based on speculative expert predic- tion.\n# Acknowledgements\nAuthors would like to acknowledge mobicham@ for helpful discussions on Mixtral quantization.\n# References\nAminabadi, R. Y., Rajbhandari, S., Awan, A. A., Li, C., Li, D., Zheng, E., Ruwase, O., Smith, S., Zhang, M., Rasley, J., and He, Y. Deepspeed-inference: Enabling efficient inference of transformer models at unprecedented scale. In Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis, SC â22. IEEE Press, 2022. ISBN 9784665454445."
        ],
        "response": "Sparse Mixture-of-Experts (MoE) increases model size due to multiple experts, but only a fraction are active per input. For example, Mixtral-8x7B has 47B parameters but uses only 13B per token [http://arxiv.org/pdf/2401.04088]. This design improves compute efficiency while maintaining performance, as noted in Mixtral AI team (2023). However, the larger parameter count makes MoE models challenging to run without high-end GPUs, as discussed in Mazur et al. (2023) [http://arxiv.org/pdf/2312.17238].",
        "reference": "The use of sparse Mixture-of-Experts (MoE) increases the size of language models due to having multiple \"experts.\""
    },
    {
        "user_input": "How does LRU caching improve the inference speed of Mixture-of-Experts language models?",
        "retrieved_contexts": [
            "We illustrate an example of how LRU cache saves experts in Figure 1 (see caption). LRU is a very simple strategy that does not consider factors like expert activation frequencies, varying cache size between MoE layers, or any sequential patterns in expert activation. However, we found that even this simple strategy can significantly speed up inference for modern Mixture-of-Experts models such as Mixtral-8x7B (see Section 4 for detailed evaluation).\n# 3.2 Speculative Expert Loading\nWhile LRU caching can reduce the average expert loading time, most of the inference time is still spent waiting for the next expert to be loaded. The reason behind this is that, unlike with dense models, MoE offloading cannot effectively overlap expert loading with computation. To understand this problem, let us zoom into the process of generating a single token, layer-by-layer. The full compute workload starts by embedding the previous token via look-up, then alternates between running self-attention and MLP for each transformer block in the model. Finally, the outputs from the last transformer block are used to predict next token logits with a linear projection.",
            "Figure 1: An example of expert loading pattern in Mixtral-8x7B-Instruct for select layers. Blue cells indicate that a certain expert was active when encoding a certain token; deeper blue indicates higher gating weight. Small gray squares show which experts are cached with an LRU cache for k=2.\n# 3.1 Expert Locality and LRU caching\nAs we discussed earlier in Section 2.1, Mixture-of-Experts language models were often observed to assign individual experts to distinct sub-tasks. However, this does not mean that the model uses the same expert over long stretches of tokens. Instead, some experts are active in short sequences of 2-4 tokens, while others are often used with âgapsâ, as shown in Figure 1.\nTo take advantage of this pattern, we can keep active experts in GPU memory as a âcacheâ for future tokens. If the same experts are activated again in future, they will be available instantaneously. Naturally, the number of experts that can be stored this way if very limited by the available GPU memory. For simplicity, we choose to always keep k least recently used experts as a type of LRU cache. If k is greater than the number of active experts, the cache will save experts from multiple previous tokens. For simplicity, we keep the same number of cached experts for each MoE layer.",
            "7Notably, Google Colab RAM cannot fit Mixtral-8x7B with a reasonable compression rate 8This corresponds to tensor.pin_memory() command in PyTorch.\n5\niy & cache_size =3 cache_size = 2 cache_size =4 0.84 | PIO â prefetch 1 experts ~ escent ae | PRS aa 0.2} ââ prefetch 2 experts ââ prefetch 3 experts 0.0 00 0 5 10 15 20 25 30 0 5 10 15 20 25 30 Layer # Layer # S Fd Ed Cache hit rate Bd ES Prediction recall = ES Ss &\nFigure 2: (left) LRU cache hit ratio for different cache size k; (right) speculative loading recall when pre-loading a different number of experts. Regular lines represent loading 1 layer ahead; dashed line stands for 2 layers ahead; dotted line is 10 layers ahead.\n# 4.1 Expert LRU Cache and Speculative Loading\nIn this section, we benchmark the effectiveness of the two expert offloading strategies: LRU caching and and speculative loading, as defined in Sections 3.1 and 3.2 respectively. For this evaluation, we measure âexpert recallâ â the fraction of times when an expert needed for inference was already available on GPU.",
            "Expert Offloading. As described earlier, we use LRU cache with an equal number k of cached experts per layer. For Mixtral-8x7B, we use k=2 for 12GB GPUs and k=4 for 16GB ones. We trigger speculative expert loading immediately after the system finished loading all experts for the current layer. The speculative expert loading fetches 1 â 2 most likely experts. The newly loaded experts do not replace the currently cached experts. If a speculatively loaded expert was later used during next layer inference, it will replace the least recently used expert from the next layerâs cache.\nMany consumer devices and free-tier cloud instances have limited host RAM that cannot fit the entire model7. In these cases, the experts must be split between host and device memory. To support this, our implementation of expert LRU cache splits experts between host and GPU devices. When loading and expert to the GPU cache, the system also offloads the least recently used on-device expert back to RAM so as to preserve memory parity.",
            "For this evaluation, we run Mixtral-8x7B-Instruct model on the OpenAssistant dataset (KÃ¶pf et al., 2023). We test LRU caching by running the model on recorded conversations and measuring the recall (aka âhit ratioâ from caching perspective) for different cache sizes k. Next, we test speculative loading in isolation by âguessingâ which experts should be loaded (by applying the next layerâs gating function on current layer activations), then measuring how often the actual next experts get loaded this way. A recall of 1.0 corresponds to a situation where both (2) Mixtral active experts were pre-fetched. We test speculative loading in three settings: 1, 2 and 10 layers ahead.\n# 4.2 Mixed MoE Quantization",
            "# 5 Conclusion and Future Work\nIn this work, we explore strategies for accelerating Mixture-of-Experts based language models on consumer hardware with limited GPU memory. We propose a MoE-centric approach to offloading\n7\nand explore how mixed quantization affects perplexity and performance on language understanding tasks. We evaluate the proposed strategies and show that they produce a significant increase in generation speed compared to naÂ¨ve approaches on consumer-grade hardware, including free-tier Google Colab.\nOur method provides a practical solution for inferencing large MoE language models on resource- constricted hardware, enabling broader access to these powerful models for research and development. As future work, we plan to explore further offloading strategies, based on speculative expert predic- tion.\n# Acknowledgements\nAuthors would like to acknowledge mobicham@ for helpful discussions on Mixtral quantization.\n# References\nAminabadi, R. Y., Rajbhandari, S., Awan, A. A., Li, C., Li, D., Zheng, E., Ruwase, O., Smith, S., Zhang, M., Rasley, J., and He, Y. Deepspeed-inference: Enabling efficient inference of transformer models at unprecedented scale. In Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis, SC â22. IEEE Press, 2022. ISBN 9784665454445.",
            "To speed up offloading in practice, we allocate all expert parameters in a contiguous memory buffer that can be moved as a single host-to-device copy. For host-side (RAM) experts, we pin8 this memory buffer for faster communication. Our implementation additionally allocates b=4 on-device buffers used to copy and prefetch experts asynchronously, without modifying existing experts. These buffers are shared between all MoE layers to reduce memory footprint. Overall, the system requires num_layers Ã num_experts expert memory buffers split between host and device memory and b=4 temporary buffers, the size of each buffer being equal to a single expert.\n# 4 Experiments\nIn this section, we verify our earlier hypotheses about MoE behavior and benchmark the inference latency in different conditions. We focus our evaluations on Mixtral-8x7B and Mixtral-8x7B-Instruct models since they represent the current state of the art among open-access MoE models. We organize this section as follows: Section 4.1 measures the effectiveness of expert caching and pre-loading in isolation, Section 4.2 compares different model compression algorithms and verifies our hypotheses from Section 3.3. Finally, Section 4.3 measures the inference latency in several hardware setups.",
            "The generative inference workload consists of two phases: 1) encoding the input prompt and 2) generating tokens conditioned on that prompt. The key difference between these two phases is that prompt tokens are encoded in parallel (layer-by-layer), whereas the generation runs sequentially (token-by-token and layer-by-layer). In general, phase 1 works relatively well with existing Mixture- of-Experts algorithms, since each layer can only be loaded once for the entire prompt. In turn, when generating tokens, one must load layer once per each token generated. In practice, this means that inference speed is limited by how fast one can fetch parameters from system memory.\nBelow, we look for patterns in how the MoE model loads its experts and propose ways to exploit these patterns to speed up inference time.\n4To learn more about these methods, please refer to surveys such as Gholami et al. (2021); Liang et al. (2021) 5As opposed to running a processing a large batch of texts over many seconds, as in Sheng et al. (2023)\n3\nSelected experts for Mixtral-8x7B-Instruct woe 0 (top) and 15 ae =n a oa ao a âme: a n: ee Layer 15 expert # Layer 0 expert # MAUR STARR O However about |= and 4 training data owerful language model based trained Trans former f architecture",
            "Shazeer et al. (2017) builds on this idea to train a sparsely gated Mixture-of-Experts to serve as a language model. The full model consists of a recurrent neural network backbone and a MoE module with up to 131072 experts. When processing a given token, a linear gating function select 4 most suitable experts based on the latest hidden state. The resulting model (including the gating function and experts) is trained end-to-end to minimize cross-entropy, with an additional regularizer to promote equal expert utilization. Shazeer et al. (2017) observed that the MoE model not only improves perplexity, but also learns interpretable expert specializations: some experts would âspecializeâ on prepositions, while others learn to express a particular concept (e.g. speed).",
            "# Denis Mazur Moscow Institute of Physics and Technology Yandex Researchcore denismazur8@gmail.com\n# Abstract\nWith the widespread adoption of Large Language Models (LLMs), many deep learning practitioners are looking for strategies of running these models more efficiently. One such strategy is to use sparse Mixture-of-Experts (MoE) â a type of model architectures where only a fraction of model layers are active for any given input. This property allows MoE-based language models to generate tokens faster than their âdenseâ counterparts, but it also increases model size due to having multiple âexpertsâ. Unfortunately, this makes state-of-the-art MoE language models difficult to run without high-end GPUs. In this work, we study the problem of running large MoE language models on consumer hardware with limited accelerator memory. We build upon parameter offloading algorithms and propose a novel strategy that accelerates offloading by taking advantage of innate properties of MoE LLMs. Using this strategy, we build can run Mixtral-8x7B with mixed quantization on desktop hardware and free-tier Google Colab instances.\n# Introduction"
        ],
        "response": "LRU caching improves MoE inference speed by keeping recently active experts in GPU memory, reducing the need to reload them for future tokens. This exploits the pattern of experts being active in short sequences (2-4 tokens) as shown in Figure 1. The LRU strategy maintains k least recently used experts per layer, with k=2 for 12GB GPUs and k=4 for 16GB GPUs. This approach increases cache hit rate, measured as expert recall, which directly speeds up inference by minimizing host-to-device data transfers. Benchmarking on Mixtral-8x7B shows significant speedup compared to naive offloading methods in Section 4.1. [source: http://arxiv.org/pdf/2312.17238]",
        "reference": "LRU caching improves the inference speed of Mixture-of-Experts language models by keeping active experts in GPU memory as a cache for future tokens. This allows for faster retrieval of experts when needed during inference."
    },
    {
        "user_input": "In terms of expert loading time, what impact does the use of LRU caching have on Mixture-of-Experts language models?",
        "retrieved_contexts": [
            "We illustrate an example of how LRU cache saves experts in Figure 1 (see caption). LRU is a very simple strategy that does not consider factors like expert activation frequencies, varying cache size between MoE layers, or any sequential patterns in expert activation. However, we found that even this simple strategy can significantly speed up inference for modern Mixture-of-Experts models such as Mixtral-8x7B (see Section 4 for detailed evaluation).\n# 3.2 Speculative Expert Loading\nWhile LRU caching can reduce the average expert loading time, most of the inference time is still spent waiting for the next expert to be loaded. The reason behind this is that, unlike with dense models, MoE offloading cannot effectively overlap expert loading with computation. To understand this problem, let us zoom into the process of generating a single token, layer-by-layer. The full compute workload starts by embedding the previous token via look-up, then alternates between running self-attention and MLP for each transformer block in the model. Finally, the outputs from the last transformer block are used to predict next token logits with a linear projection.",
            "Figure 1: An example of expert loading pattern in Mixtral-8x7B-Instruct for select layers. Blue cells indicate that a certain expert was active when encoding a certain token; deeper blue indicates higher gating weight. Small gray squares show which experts are cached with an LRU cache for k=2.\n# 3.1 Expert Locality and LRU caching\nAs we discussed earlier in Section 2.1, Mixture-of-Experts language models were often observed to assign individual experts to distinct sub-tasks. However, this does not mean that the model uses the same expert over long stretches of tokens. Instead, some experts are active in short sequences of 2-4 tokens, while others are often used with âgapsâ, as shown in Figure 1.\nTo take advantage of this pattern, we can keep active experts in GPU memory as a âcacheâ for future tokens. If the same experts are activated again in future, they will be available instantaneously. Naturally, the number of experts that can be stored this way if very limited by the available GPU memory. For simplicity, we choose to always keep k least recently used experts as a type of LRU cache. If k is greater than the number of active experts, the cache will save experts from multiple previous tokens. For simplicity, we keep the same number of cached experts for each MoE layer.",
            "Expert Offloading. As described earlier, we use LRU cache with an equal number k of cached experts per layer. For Mixtral-8x7B, we use k=2 for 12GB GPUs and k=4 for 16GB ones. We trigger speculative expert loading immediately after the system finished loading all experts for the current layer. The speculative expert loading fetches 1 â 2 most likely experts. The newly loaded experts do not replace the currently cached experts. If a speculatively loaded expert was later used during next layer inference, it will replace the least recently used expert from the next layerâs cache.\nMany consumer devices and free-tier cloud instances have limited host RAM that cannot fit the entire model7. In these cases, the experts must be split between host and device memory. To support this, our implementation of expert LRU cache splits experts between host and GPU devices. When loading and expert to the GPU cache, the system also offloads the least recently used on-device expert back to RAM so as to preserve memory parity.",
            "For this evaluation, we run Mixtral-8x7B-Instruct model on the OpenAssistant dataset (KÃ¶pf et al., 2023). We test LRU caching by running the model on recorded conversations and measuring the recall (aka âhit ratioâ from caching perspective) for different cache sizes k. Next, we test speculative loading in isolation by âguessingâ which experts should be loaded (by applying the next layerâs gating function on current layer activations), then measuring how often the actual next experts get loaded this way. A recall of 1.0 corresponds to a situation where both (2) Mixtral active experts were pre-fetched. We test speculative loading in three settings: 1, 2 and 10 layers ahead.\n# 4.2 Mixed MoE Quantization",
            "7Notably, Google Colab RAM cannot fit Mixtral-8x7B with a reasonable compression rate 8This corresponds to tensor.pin_memory() command in PyTorch.\n5\niy & cache_size =3 cache_size = 2 cache_size =4 0.84 | PIO â prefetch 1 experts ~ escent ae | PRS aa 0.2} ââ prefetch 2 experts ââ prefetch 3 experts 0.0 00 0 5 10 15 20 25 30 0 5 10 15 20 25 30 Layer # Layer # S Fd Ed Cache hit rate Bd ES Prediction recall = ES Ss &\nFigure 2: (left) LRU cache hit ratio for different cache size k; (right) speculative loading recall when pre-loading a different number of experts. Regular lines represent loading 1 layer ahead; dashed line stands for 2 layers ahead; dotted line is 10 layers ahead.\n# 4.1 Expert LRU Cache and Speculative Loading\nIn this section, we benchmark the effectiveness of the two expert offloading strategies: LRU caching and and speculative loading, as defined in Sections 3.1 and 3.2 respectively. For this evaluation, we measure âexpert recallâ â the fraction of times when an expert needed for inference was already available on GPU.",
            "For regular (dense) models, this architecture allows for efficient offloading schedule that pre-loads the next transformer layer ahead of time, while the previous layer is still running. Unfortunately, this schedule is no longer possible for Mixture-of-Experts models, where MoE MLP layers choose which experts to load just-in-time for computation. This is because the system cannot pre-fetch the next layer until it learns which experts should be loaded. Modern open-access MoE language models choose active experts using the final outputs of the previous layer, which means they cannot be pre-fetched them in parallel with previous layer. While it is not possible6 to pre-reliably prefetch the next set of experts ahead of time, the system could still try to guess the likely next experts and load them speculatively, while processing the previous layer. It the guess is correct, it will speed up the next layer inference; if not, it can load the actual next layerâs experts later. In other words, this type of speculative loading does not change the final model predictions, but may reduce latency if the guess is accurate enough.\n6More specifically, not possible without changing the model architecture, which would require re-training\n4",
            "1When deployed in 16-bit precision, Falcon-180B needs approximately 360GB, while LLaMA-2 70B requires 140GB of combined accelerator memory.\nIn this work, we systematically develop techniques for running large MoE language models with limited GPU memory. Our main objective is inferencing (generating tokens) with Mixtral-8x7B- Instruct â a MoE-based chat assistant â on a desktop-grade hardware where only a fraction of experts fit into the accelerator memory. To that end:\nwe observe how MoE language model accesses its experts between tokens, and find several regularities: i) some experts are reused between adjacent tokens and ii) the model hidden states of early layers already âknowâ which experts are to be used at subsequent layers. â¢ we design a MoE-specific offloading strategy that takes advantage of these regularities: i) it uses LRU cache to significantly reduces GPU-RAM communication, leading to faster generation and ii) it guesses which experts are needed ahead of time to better overlap expert loading with computation.\nâ¢ we consider the specific scenario of running Mixtral-8x7B-Instruct on a T4, RTX 3060 and RTX 3080 Mobile and develop a practical combination of mixed quantization and the proposed offloading algorithm to run this model interactively at 2-3 tokens per second depending on the hardware. The source code with our implementation is available online2",
            "To speed up offloading in practice, we allocate all expert parameters in a contiguous memory buffer that can be moved as a single host-to-device copy. For host-side (RAM) experts, we pin8 this memory buffer for faster communication. Our implementation additionally allocates b=4 on-device buffers used to copy and prefetch experts asynchronously, without modifying existing experts. These buffers are shared between all MoE layers to reduce memory footprint. Overall, the system requires num_layers Ã num_experts expert memory buffers split between host and device memory and b=4 temporary buffers, the size of each buffer being equal to a single expert.\n# 4 Experiments\nIn this section, we verify our earlier hypotheses about MoE behavior and benchmark the inference latency in different conditions. We focus our evaluations on Mixtral-8x7B and Mixtral-8x7B-Instruct models since they represent the current state of the art among open-access MoE models. We organize this section as follows: Section 4.1 measures the effectiveness of expert caching and pre-loading in isolation, Section 4.2 compares different model compression algorithms and verifies our hypotheses from Section 3.3. Finally, Section 4.3 measures the inference latency in several hardware setups.",
            "Table 1: Perplexity and model size evaluation of Mixtral-8x7B with different quantization for shared attention (Attn quant) and experts (Experts quant) layers. For comprarison, a Mistral-7B 4-bit quantized model has Wiki2 perplexity 5.03, C4 perplexity 7.56 and MMLU score 61.3%. See Section 4.2 for details. Green values correspond to the configurations we chose for full system evaluation.\n6\nAlgorithm 2-bit Experts 3-bit Experts A100 3080 Mobile 3060 T4 (Colab) A100 3080 Mobile 3060 T4 (Cloud) 3.061 Full algorithm 2.918 W/o expert pre-loading 2.265 W/o LRU cache & pre-loading Naive offloading (accelerate) 1.392 2.655 2.227 1.758 1.059 2.278 2.051 1.547 0.919 2.092 1.567 1.168 0.661 2.845 2.683 2.055 1.246 2.475 2.024 1.595 0.914 2.038 1.857 1.346 1.791 1.603 1.365 1.061 0.580",
            "# 5 Conclusion and Future Work\nIn this work, we explore strategies for accelerating Mixture-of-Experts based language models on consumer hardware with limited GPU memory. We propose a MoE-centric approach to offloading\n7\nand explore how mixed quantization affects perplexity and performance on language understanding tasks. We evaluate the proposed strategies and show that they produce a significant increase in generation speed compared to naÂ¨ve approaches on consumer-grade hardware, including free-tier Google Colab.\nOur method provides a practical solution for inferencing large MoE language models on resource- constricted hardware, enabling broader access to these powerful models for research and development. As future work, we plan to explore further offloading strategies, based on speculative expert predic- tion.\n# Acknowledgements\nAuthors would like to acknowledge mobicham@ for helpful discussions on Mixtral quantization.\n# References\nAminabadi, R. Y., Rajbhandari, S., Awan, A. A., Li, C., Li, D., Zheng, E., Ruwase, O., Smith, S., Zhang, M., Rasley, J., and He, Y. Deepspeed-inference: Enabling efficient inference of transformer models at unprecedented scale. In Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis, SC â22. IEEE Press, 2022. ISBN 9784665454445."
        ],
        "response": "LRU caching significantly reduces expert loading time for Mixture-of-Experts models by keeping frequently accessed experts in GPU memory [http://arxiv.org/pdf/2312.17238]. It achieves this by caching the k least recently used experts per layer, reducing GPU-RAM communication [http://arxiv.org/pdf/2312.17238]. For Mixtral-8x7B, LRU caching with k=2 or 4 improves inference speed on hardware with limited memory [http://arxiv.org/pdf/2312.17238]. The cache hit ratio varies by layer and cache size, as shown in Figure 2 [http://arxiv.org/pdf/2312.17238]. However, most inference time is still spent waiting for experts to be loaded, as MoE offloading cannot effectively overlap loading with computation [http://arxiv.org/pdf/2312.17238]. Speculative loading further reduces latency by guessing and pre-loading likely next experts [http://arxiv.org/pdf/2312.17238]. The combined approach of LRU caching and speculative loading outperforms naive offloading methods [http://arxiv.org/pdf/2312.17238]. This enables practical inference of large MoE models on consumer hardware at 2-3 tokens per second [http://arxiv.org/pdf/2312.17238].",
        "reference": "The use of LRU caching in Mixture-of-Experts language models can significantly speed up inference by reducing the average expert loading time. However, most of the inference time is still spent waiting for the next expert to be loaded. This is because the architecture of Mixture-of-Experts models does not allow for efficient offloading schedules like regular (dense) models, where the next layer can be pre-loaded ahead of time. In Mixture-of-Experts models, the experts to be loaded are chosen just-in-time for computation, so the system cannot pre-fetch the next layer until it learns which experts should be loaded."
    },
    {
        "user_input": "How does the loading pattern of experts in Mixture-of-Experts models impact the efficiency of inference time?",
        "retrieved_contexts": [
            "We illustrate an example of how LRU cache saves experts in Figure 1 (see caption). LRU is a very simple strategy that does not consider factors like expert activation frequencies, varying cache size between MoE layers, or any sequential patterns in expert activation. However, we found that even this simple strategy can significantly speed up inference for modern Mixture-of-Experts models such as Mixtral-8x7B (see Section 4 for detailed evaluation).\n# 3.2 Speculative Expert Loading\nWhile LRU caching can reduce the average expert loading time, most of the inference time is still spent waiting for the next expert to be loaded. The reason behind this is that, unlike with dense models, MoE offloading cannot effectively overlap expert loading with computation. To understand this problem, let us zoom into the process of generating a single token, layer-by-layer. The full compute workload starts by embedding the previous token via look-up, then alternates between running self-attention and MLP for each transformer block in the model. Finally, the outputs from the last transformer block are used to predict next token logits with a linear projection.",
            "For regular (dense) models, this architecture allows for efficient offloading schedule that pre-loads the next transformer layer ahead of time, while the previous layer is still running. Unfortunately, this schedule is no longer possible for Mixture-of-Experts models, where MoE MLP layers choose which experts to load just-in-time for computation. This is because the system cannot pre-fetch the next layer until it learns which experts should be loaded. Modern open-access MoE language models choose active experts using the final outputs of the previous layer, which means they cannot be pre-fetched them in parallel with previous layer. While it is not possible6 to pre-reliably prefetch the next set of experts ahead of time, the system could still try to guess the likely next experts and load them speculatively, while processing the previous layer. It the guess is correct, it will speed up the next layer inference; if not, it can load the actual next layerâs experts later. In other words, this type of speculative loading does not change the final model predictions, but may reduce latency if the guess is accurate enough.\n6More specifically, not possible without changing the model architecture, which would require re-training\n4",
            "For this evaluation, we run Mixtral-8x7B-Instruct model on the OpenAssistant dataset (KÃ¶pf et al., 2023). We test LRU caching by running the model on recorded conversations and measuring the recall (aka âhit ratioâ from caching perspective) for different cache sizes k. Next, we test speculative loading in isolation by âguessingâ which experts should be loaded (by applying the next layerâs gating function on current layer activations), then measuring how often the actual next experts get loaded this way. A recall of 1.0 corresponds to a situation where both (2) Mixtral active experts were pre-fetched. We test speculative loading in three settings: 1, 2 and 10 layers ahead.\n# 4.2 Mixed MoE Quantization",
            "The generative inference workload consists of two phases: 1) encoding the input prompt and 2) generating tokens conditioned on that prompt. The key difference between these two phases is that prompt tokens are encoded in parallel (layer-by-layer), whereas the generation runs sequentially (token-by-token and layer-by-layer). In general, phase 1 works relatively well with existing Mixture- of-Experts algorithms, since each layer can only be loaded once for the entire prompt. In turn, when generating tokens, one must load layer once per each token generated. In practice, this means that inference speed is limited by how fast one can fetch parameters from system memory.\nBelow, we look for patterns in how the MoE model loads its experts and propose ways to exploit these patterns to speed up inference time.\n4To learn more about these methods, please refer to surveys such as Gholami et al. (2021); Liang et al. (2021) 5As opposed to running a processing a large batch of texts over many seconds, as in Sheng et al. (2023)\n3\nSelected experts for Mixtral-8x7B-Instruct woe 0 (top) and 15 ae =n a oa ao a âme: a n: ee Layer 15 expert # Layer 0 expert # MAUR STARR O However about |= and 4 training data owerful language model based trained Trans former f architecture",
            "Figure 1: An example of expert loading pattern in Mixtral-8x7B-Instruct for select layers. Blue cells indicate that a certain expert was active when encoding a certain token; deeper blue indicates higher gating weight. Small gray squares show which experts are cached with an LRU cache for k=2.\n# 3.1 Expert Locality and LRU caching\nAs we discussed earlier in Section 2.1, Mixture-of-Experts language models were often observed to assign individual experts to distinct sub-tasks. However, this does not mean that the model uses the same expert over long stretches of tokens. Instead, some experts are active in short sequences of 2-4 tokens, while others are often used with âgapsâ, as shown in Figure 1.\nTo take advantage of this pattern, we can keep active experts in GPU memory as a âcacheâ for future tokens. If the same experts are activated again in future, they will be available instantaneously. Naturally, the number of experts that can be stored this way if very limited by the available GPU memory. For simplicity, we choose to always keep k least recently used experts as a type of LRU cache. If k is greater than the number of active experts, the cache will save experts from multiple previous tokens. For simplicity, we keep the same number of cached experts for each MoE layer.",
            "Expert Offloading. As described earlier, we use LRU cache with an equal number k of cached experts per layer. For Mixtral-8x7B, we use k=2 for 12GB GPUs and k=4 for 16GB ones. We trigger speculative expert loading immediately after the system finished loading all experts for the current layer. The speculative expert loading fetches 1 â 2 most likely experts. The newly loaded experts do not replace the currently cached experts. If a speculatively loaded expert was later used during next layer inference, it will replace the least recently used expert from the next layerâs cache.\nMany consumer devices and free-tier cloud instances have limited host RAM that cannot fit the entire model7. In these cases, the experts must be split between host and device memory. To support this, our implementation of expert LRU cache splits experts between host and GPU devices. When loading and expert to the GPU cache, the system also offloads the least recently used on-device expert back to RAM so as to preserve memory parity.",
            "# 5 Conclusion and Future Work\nIn this work, we explore strategies for accelerating Mixture-of-Experts based language models on consumer hardware with limited GPU memory. We propose a MoE-centric approach to offloading\n7\nand explore how mixed quantization affects perplexity and performance on language understanding tasks. We evaluate the proposed strategies and show that they produce a significant increase in generation speed compared to naÂ¨ve approaches on consumer-grade hardware, including free-tier Google Colab.\nOur method provides a practical solution for inferencing large MoE language models on resource- constricted hardware, enabling broader access to these powerful models for research and development. As future work, we plan to explore further offloading strategies, based on speculative expert predic- tion.\n# Acknowledgements\nAuthors would like to acknowledge mobicham@ for helpful discussions on Mixtral quantization.\n# References\nAminabadi, R. Y., Rajbhandari, S., Awan, A. A., Li, C., Li, D., Zheng, E., Ruwase, O., Smith, S., Zhang, M., Rasley, J., and He, Y. Deepspeed-inference: Enabling efficient inference of transformer models at unprecedented scale. In Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis, SC â22. IEEE Press, 2022. ISBN 9784665454445.",
            "To speed up offloading in practice, we allocate all expert parameters in a contiguous memory buffer that can be moved as a single host-to-device copy. For host-side (RAM) experts, we pin8 this memory buffer for faster communication. Our implementation additionally allocates b=4 on-device buffers used to copy and prefetch experts asynchronously, without modifying existing experts. These buffers are shared between all MoE layers to reduce memory footprint. Overall, the system requires num_layers Ã num_experts expert memory buffers split between host and device memory and b=4 temporary buffers, the size of each buffer being equal to a single expert.\n# 4 Experiments\nIn this section, we verify our earlier hypotheses about MoE behavior and benchmark the inference latency in different conditions. We focus our evaluations on Mixtral-8x7B and Mixtral-8x7B-Instruct models since they represent the current state of the art among open-access MoE models. We organize this section as follows: Section 4.1 measures the effectiveness of expert caching and pre-loading in isolation, Section 4.2 compares different model compression algorithms and verifies our hypotheses from Section 3.3. Finally, Section 4.3 measures the inference latency in several hardware setups.",
            "Detailed results for Mixtral, Mistral 7B and Llama 2 7B/13B/70B and Llama 1 34B2 are reported in Table 2. Figure 2 compares the performance of Mixtral with the Llama models in different categories. Mixtral surpasses Llama 2 70B across most metrics. In particular, Mixtral displays a superior performance in code and mathematics benchmarks.\nSize and Efficiency. We compare our performance to the Llama 2 family, aiming to understand Mixtral modelsâ efficiency in the cost-performance spectrum (see Figure 3). As a sparse Mixture- of-Experts model, Mixtral only uses 13B active parameters for each token. With 5x lower active parameters, Mixtral is able to outperform Llama 2 70B across most categories.\nNote that this analysis focuses on the active parameter count (see Section 2.1), which is directly proportional to the inference compute cost, but does not consider the memory costs and hardware utilization. The memory costs for serving Mixtral are proportional to its sparse parameter count, 47B, which is still smaller than Llama 2 70B. As for device utilization, we note that the SMoEs layer introduces additional overhead due to the routing mechanism and due to the increased memory loads when running more than one expert per device. They are more suitable for batched workloads where one can reach a good degree of arithmetic intensity.",
            "Generally speaking, the optimal compression rate for most LLMs is 4 bits per parameter Dettmers & Zettlemoyer (2022). While there are more extreme algorithms for 3- and even 2-bit compression Chee et al. (2023); Lin et al. (2023); Dettmers et al. (2023), they are typically inferior to choosing a smaller model and quantizing it to around 4 bits. Most recently, there has been several concurrent works for quantizing Mixture-of-Experts models (Kim et al., 2023; Frantar & Alistarh, 2023b).\n# Inference with Parameter Offloading\nA recent line of work explores inferencing and training large models with limited accelerator mem- ory by âoffloadingâ their parameters to another, cheaper memory, such as system RAM or even SSD (Pudipeddi et al., 2020; Ren et al., 2021). This technique works by loading model parameters just-in-time when they are needed for computation. Since most deep learning models use layers in a fixed order, offloading can pre-dispatch the next layer parameters in the background, ahead of time."
        ],
        "response": "The loading pattern of experts in Mixture-of-Experts models impacts inference time efficiency due to the just-in-time loading mechanism, which cannot overlap expert loading with computation like dense models. [source: http://arxiv.org/pdf/2312.17238] LRU caching can reduce average expert loading time by keeping recently used experts in GPU memory, but most inference time is still spent waiting for the next expert to be loaded. [source: http://arxiv.org/pdf/2312.17238] Speculative loading attempts to guess the next experts while processing the current layer to reduce latency if the guess is accurate. [source: http://arxiv.org/pdf/2312.17238] The inability to pre-fetch experts reliably without model architectural changes limits efficiency. [source: http://arxiv.org.pdf/2312.17238] Mixed MoE quantization and contiguous memory allocation can further optimize inference speed on hardware with limited memory. [source: http://arxiv.org/pdf/2312.17238]",
        "reference": "The loading pattern of experts in Mixture-of-Experts models impacts the efficiency of inference time. In interactive inference scenarios, where tokens are generated one at a time, the inference system spends most of the time waiting for the next layer's parameters to be loaded. This is because the system cannot pre-fetch the next layer until it learns which experts should be loaded. However, in non-interactive scenarios with large batches of data, each layer processes a lot of tokens at once, which improves efficiency. Additionally, by keeping active experts in GPU memory as a cache, the inference time can be further sped up."
    },
    {
        "user_input": "In terms of generating tokens, what is the effect of the loading pattern of experts in Mixture-of-Experts models?",
        "retrieved_contexts": [
            "Figure 1: An example of expert loading pattern in Mixtral-8x7B-Instruct for select layers. Blue cells indicate that a certain expert was active when encoding a certain token; deeper blue indicates higher gating weight. Small gray squares show which experts are cached with an LRU cache for k=2.\n# 3.1 Expert Locality and LRU caching\nAs we discussed earlier in Section 2.1, Mixture-of-Experts language models were often observed to assign individual experts to distinct sub-tasks. However, this does not mean that the model uses the same expert over long stretches of tokens. Instead, some experts are active in short sequences of 2-4 tokens, while others are often used with âgapsâ, as shown in Figure 1.\nTo take advantage of this pattern, we can keep active experts in GPU memory as a âcacheâ for future tokens. If the same experts are activated again in future, they will be available instantaneously. Naturally, the number of experts that can be stored this way if very limited by the available GPU memory. For simplicity, we choose to always keep k least recently used experts as a type of LRU cache. If k is greater than the number of active experts, the cache will save experts from multiple previous tokens. For simplicity, we keep the same number of cached experts for each MoE layer.",
            "The generative inference workload consists of two phases: 1) encoding the input prompt and 2) generating tokens conditioned on that prompt. The key difference between these two phases is that prompt tokens are encoded in parallel (layer-by-layer), whereas the generation runs sequentially (token-by-token and layer-by-layer). In general, phase 1 works relatively well with existing Mixture- of-Experts algorithms, since each layer can only be loaded once for the entire prompt. In turn, when generating tokens, one must load layer once per each token generated. In practice, this means that inference speed is limited by how fast one can fetch parameters from system memory.\nBelow, we look for patterns in how the MoE model loads its experts and propose ways to exploit these patterns to speed up inference time.\n4To learn more about these methods, please refer to surveys such as Gholami et al. (2021); Liang et al. (2021) 5As opposed to running a processing a large batch of texts over many seconds, as in Sheng et al. (2023)\n3\nSelected experts for Mixtral-8x7B-Instruct woe 0 (top) and 15 ae =n a oa ao a âme: a n: ee Layer 15 expert # Layer 0 expert # MAUR STARR O However about |= and 4 training data owerful language model based trained Trans former f architecture",
            "We illustrate an example of how LRU cache saves experts in Figure 1 (see caption). LRU is a very simple strategy that does not consider factors like expert activation frequencies, varying cache size between MoE layers, or any sequential patterns in expert activation. However, we found that even this simple strategy can significantly speed up inference for modern Mixture-of-Experts models such as Mixtral-8x7B (see Section 4 for detailed evaluation).\n# 3.2 Speculative Expert Loading\nWhile LRU caching can reduce the average expert loading time, most of the inference time is still spent waiting for the next expert to be loaded. The reason behind this is that, unlike with dense models, MoE offloading cannot effectively overlap expert loading with computation. To understand this problem, let us zoom into the process of generating a single token, layer-by-layer. The full compute workload starts by embedding the previous token via look-up, then alternates between running self-attention and MLP for each transformer block in the model. Finally, the outputs from the last transformer block are used to predict next token logits with a linear projection.",
            "For regular (dense) models, this architecture allows for efficient offloading schedule that pre-loads the next transformer layer ahead of time, while the previous layer is still running. Unfortunately, this schedule is no longer possible for Mixture-of-Experts models, where MoE MLP layers choose which experts to load just-in-time for computation. This is because the system cannot pre-fetch the next layer until it learns which experts should be loaded. Modern open-access MoE language models choose active experts using the final outputs of the previous layer, which means they cannot be pre-fetched them in parallel with previous layer. While it is not possible6 to pre-reliably prefetch the next set of experts ahead of time, the system could still try to guess the likely next experts and load them speculatively, while processing the previous layer. It the guess is correct, it will speed up the next layer inference; if not, it can load the actual next layerâs experts later. In other words, this type of speculative loading does not change the final model predictions, but may reduce latency if the guess is accurate enough.\n6More specifically, not possible without changing the model architecture, which would require re-training\n4",
            "Jordan, M. I. and Jacobs, R. A. Hierarchical mixtures of experts and the em algorithm. Neural computation, 6(2):181â214, 1994.\nKim, Y. J., Fahim, R., and Awadalla, H. H. Mixture of quantized experts (moqe): Complementary effect of low-bit quantization and robustness, 2023.\nKÃ¶pf, A., Kilcher, Y., von RÃ¼tte, D., Anagnostidis, S., Tam, Z.-R., Stevens, K., Barhoum, A., Duc, N. M., Stanley, O., Nagyfi, R., ES, S., Suri, S., Glushkov, D., Dantuluri, A., Maguire, A., Schuhmann, C., Nguyen, H., and Mattick, A. Openassistant conversations â democratizing large language model alignment, 2023.",
            "Mixture of Experts Layer i gating inputs af outputs router expert\nFigure 1: Mixture of Experts Layer. Each input vector is assigned to 2 of the 8 experts by a router. The layerâs output is the weighted sum of the outputs of the two selected experts. In Mixtral, an expert is a standard feedforward block as in a vanilla transformer architecture.\nMixtral demonstrates superior capabilities in mathematics, code generation, and tasks that require multilingual understanding, significantly outperforming Llama 2 70B in these domains. Experiments show that Mixtral is able to successfully retrieve information from its context window of 32k tokens, regardless of the sequence length and the location of the information in the sequence.",
            "1When deployed in 16-bit precision, Falcon-180B needs approximately 360GB, while LLaMA-2 70B requires 140GB of combined accelerator memory.\nIn this work, we systematically develop techniques for running large MoE language models with limited GPU memory. Our main objective is inferencing (generating tokens) with Mixtral-8x7B- Instruct â a MoE-based chat assistant â on a desktop-grade hardware where only a fraction of experts fit into the accelerator memory. To that end:\nwe observe how MoE language model accesses its experts between tokens, and find several regularities: i) some experts are reused between adjacent tokens and ii) the model hidden states of early layers already âknowâ which experts are to be used at subsequent layers. â¢ we design a MoE-specific offloading strategy that takes advantage of these regularities: i) it uses LRU cache to significantly reduces GPU-RAM communication, leading to faster generation and ii) it guesses which experts are needed ahead of time to better overlap expert loading with computation.\nâ¢ we consider the specific scenario of running Mixtral-8x7B-Instruct on a T4, RTX 3060 and RTX 3080 Mobile and develop a practical combination of mixed quantization and the proposed offloading algorithm to run this model interactively at 2-3 tokens per second depending on the hardware. The source code with our implementation is available online2",
            "For this evaluation, we run Mixtral-8x7B-Instruct model on the OpenAssistant dataset (KÃ¶pf et al., 2023). We test LRU caching by running the model on recorded conversations and measuring the recall (aka âhit ratioâ from caching perspective) for different cache sizes k. Next, we test speculative loading in isolation by âguessingâ which experts should be loaded (by applying the next layerâs gating function on current layer activations), then measuring how often the actual next experts get loaded this way. A recall of 1.0 corresponds to a situation where both (2) Mixtral active experts were pre-fetched. We test speculative loading in three settings: 1, 2 and 10 layers ahead.\n# 4.2 Mixed MoE Quantization",
            "Abstract\nWe introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine- tuned to follow instructions, Mixtral 8x7B â Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â chat model on human bench- marks. Both the base and instruct models are released under the Apache 2.0 license.\nCode: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/mixtral-of-experts/",
            "# 2 Background & Related Work\n# 2.1 Mixture-of-Experts\nThe recent surge in MoE language models builds on a relatively old idea (Jacobs et al., 1991; Jordan & Jacobs, 1994) of training ensembles of specialized models (âexpertsâ) and a gating function to select the right expert for the task. To achieve specialization, Mixture-of-Experts learn by simultaneously i) training the gating function to choose the best experts and ii) training the experts themselves on samples assigned to them by the gating function. Since then, many different MoE variants emerged, including mixture of SVM models (Collobert et al., 2002), Dirichlet processes (Shahbaba & Neal, 2009) and various neural networks."
        ],
        "response": "In Mixture-of-Experts models, the loading pattern of experts affects token generation efficiency through expert reuse and caching. Some experts are active in short sequences of 2-4 tokens, while others have gaps in usage, as shown in Figure 1. [source: http://arxiv.org/pdf/2312.17238] LRU caching stores recently used experts to reduce GPU-RAM communication, speeding up generation. [source: http://arxiv.org/pdf/2312.17238] Speculative loading guesses next experts to overlap computation with loading, improving latency if accurate. [source: http://arxiv.org/pdf/2312.17238] These patterns exploit regularities in expert access to optimize inference. [source: http://arxiv.org/pdf/2312.17238]",
        "reference": "The loading pattern of experts in Mixture-of-Experts models has an effect on generating tokens. When generating tokens, each layer needs to be loaded once per each token generated. This means that the inference speed is limited by how fast the parameters can be fetched from system memory. In interactive inference, where tokens are generated autoregressively from left to right, the inference system spends most of the time waiting for the next layer's parameters to be loaded. To speed up inference time, active experts can be kept in GPU memory as a cache for future tokens. If the same experts are activated again in the future, they will be available instantaneously. However, even with caching, most of the inference time is still spent waiting for the next expert to be loaded, as MoE offloading cannot effectively overlap expert loading with computation like dense models."
    },
    {
        "user_input": "What are the architectural differences in terms of feedforward blocks and active parameters used during inference between Mixtral 8x7B and Mistral 7B?",
        "retrieved_contexts": [
            "Abstract\nWe introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine- tuned to follow instructions, Mixtral 8x7B â Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â chat model on human bench- marks. Both the base and instruct models are released under the Apache 2.0 license.\nCode: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/mixtral-of-experts/",
            "Table 2: Comparison of Mixtral with Llama. Mixtral outperforms or matches Llama 2 70B performance on almost all popular benchmarks while using 5x fewer active parameters during inference.\n70 Mixtral 8x7B. âMixtral 8x7B Mixtral 8x7B 355 =o = Es & E60! Mistral 78 % 2681 Mistral 78 3 3 s0 5 = A % 66 50 g 4 45 64 78 138 348708 78 138 348708 78 138 348 70B S66 Mixtral 8x7B 50 Mixtral 8x7B 5 = 564 340 g al Mistral 78 ee Mistral 78 3 5 Â§ 30 5 eo â= Mistral Â° 20 âe LlaMA2 78 (138 348 70B 7B (138 348 708 7B Â«13B 34B 708 Active Params Active Params Active Params\nFigure 3: Results on MMLU, commonsense reasoning, world knowledge and reading comprehension, math and code for Mistral (7B/8x7B) vs Llama 2 (7B/13B/70B). Mixtral largely outperforms Llama 2 70B on all benchmarks, except on reading comprehension benchmarks while using 5x lower active parameters. It is also vastly superior to Llama 2 70B on code and math.",
            "We also present Mixtral 8x7B â Instruct, a chat model fine-tuned to follow instructions using supervised fine-tuning and Direct Preference Optimization [25]. Its performance notably surpasses that of GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â chat model on human evaluation benchmarks. Mixtral â Instruct also demonstrates reduced biases, and a more balanced sentiment profile in benchmarks such as BBQ, and BOLD. We release both Mixtral 8x7B and Mixtral 8x7B â Instruct under the Apache 2.0 license1, free for academic and commercial usage, ensuring broad accessibility and potential for diverse applications. To enable the community to run Mixtral with a fully open-source stack, we submitted changes to the vLLM project, which integrates Megablocks CUDA kernels for efficient inference. Skypilot also allows the deployment of vLLM endpoints on any instance in the cloud.\n# 2 Architectural details\nMixtral is based on a transformer architecture [31] and uses the same modifications as described in [18], with the notable exceptions that Mix- tral supports a fully dense context length of 32k tokens, and the feed- forward blocks are replaced by Mixture-of-Expert layers (Section 2.1). The model architecture parameters are summarized in Table 1. Parameter Value",
            "Detailed results for Mixtral, Mistral 7B and Llama 2 7B/13B/70B and Llama 1 34B2 are reported in Table 2. Figure 2 compares the performance of Mixtral with the Llama models in different categories. Mixtral surpasses Llama 2 70B across most metrics. In particular, Mixtral displays a superior performance in code and mathematics benchmarks.\nSize and Efficiency. We compare our performance to the Llama 2 family, aiming to understand Mixtral modelsâ efficiency in the cost-performance spectrum (see Figure 3). As a sparse Mixture- of-Experts model, Mixtral only uses 13B active parameters for each token. With 5x lower active parameters, Mixtral is able to outperform Llama 2 70B across most categories.\nNote that this analysis focuses on the active parameter count (see Section 2.1), which is directly proportional to the inference compute cost, but does not consider the memory costs and hardware utilization. The memory costs for serving Mixtral are proportional to its sparse parameter count, 47B, which is still smaller than Llama 2 70B. As for device utilization, we note that the SMoEs layer introduces additional overhead due to the routing mechanism and due to the increased memory loads when running more than one expert per device. They are more suitable for batched workloads where one can reach a good degree of arithmetic intensity.",
            "Code: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/mixtral-of-experts/\n# Introduction\nIn this paper, we present Mixtral 8x7B, a sparse mixture of experts model (SMoE) with open weights, licensed under Apache 2.0. Mixtral outperforms Llama 2 70B and GPT-3.5 on most benchmarks. As it only uses a subset of its parameters for every token, Mixtral allows faster inference speed at low batch-sizes, and higher throughput at large batch-sizes.\nMixtral is a sparse mixture-of-experts network. It is a decoder-only model where the feedforward block picks from a set of 8 distinct groups of parameters. At every layer, for every token, a router network chooses two of these groups (the âexpertsâ) to process the token and combine their output additively. This technique increases the number of parameters of a model while controlling cost and latency, as the model only uses a fraction of the total set of parameters per token.\nMixtral is pretrained with multilingual data using a context size of 32k tokens. It either matches or exceeds the performance of Llama 2 70B and GPT-3.5, over several benchmarks. In particular,\nMixture of Experts Layer i gating inputs af outputs router expert",
            "# 6 Conclusion\nIn this paper, we introduced Mixtral 8x7B, the first mixture-of-experts network to reach a state-of-the- art performance among open-source models. Mixtral 8x7B Instruct outperforms Claude-2.1, Gem- ini Pro, and GPT-3.5 Turbo on human evaluation benchmarks. Because it only uses two experts at each time step, Mixtral only uses 13B active parameters per token while outperforming the previous best model using 70B parameters per token (Llama 2 70B). We are making our trained and fine-tuned mod- els publicly available under the Apache 2.0 license. By sharing our models, we aim to facilitate the de- velopment of new techniques and applications that can benefit a wide range of industries and domains.",
            "Since then, several lines of work explore Mixture-of-Experts with Transformer-based language models for machine translation Lepikhin et al. (2020), masked language modeling Fedus et al. (2021), general-purpose LLMs Du et al. (2022) and others. Most of these models follow traditional (dense) Transformer architecture for embeddings and attention layers, and only use Mixture for the feedforward (MLP) blocks and use a linear token-level gating function. A common observation across most of these works is that MoE models are cheaper to train and inference Fedus et al. (2021); Lepikhin et al. (2020), but require more parameters than a dense model with equivalent perplexity. Pre-trained Mixture-of-Experts LLMs have been openly available for over a year3. However, these models seem to have gained less traction than equivalent dense models, arguable because their sheer model size (over a trillion parameters) makes them difficult to use. Most recently, Mistral AI released a family of sparse Mixture of Experts models called Mixtral-8x7B with near state-of-the-art performance Mixtral AI team (2023). This model has already inspired several follow-up works and practical applications, but it still requires a high-end GPU accelerator.\n# 2.2 Post-training Quantization of LLMs",
            "# 3.1 Multilingual benchmarks\nCompared to Mistral 7B, we significantly upsample the proportion of multilingual data during pretraining. The extra capacity allows Mixtral to perform well on multilingual benchmarks while maintaining a high accuracy in English. In particular, Mixtral significantly outperforms Llama 2 70B in French, German, Spanish, and Italian, as shown in Table 4.\nActive Params French Arc-c HellaS MMLU German Arc-c HellaS MMLU Spanish Arc-c HellaS MMLU Italian Arc-c HellaS MMLU 33B 70B 13B 42.9% 65.4% 49.0% 39.3% 68.1% 49.9% 49.9% 72.5% 64.3% 49.4% 70.9% 65.1% 58.2% 77.4% 70.9% 54.3% 73.0% 71.5% 55.4% 77.6% 72.5% 52.8% 75.1% 70.9% 41.1% 63.3% 48.7% 47.3% 68.7% 64.2% 45.7% 69.8% 52.3% 50.5% 74.5% 66.0%",
            "We illustrate an example of how LRU cache saves experts in Figure 1 (see caption). LRU is a very simple strategy that does not consider factors like expert activation frequencies, varying cache size between MoE layers, or any sequential patterns in expert activation. However, we found that even this simple strategy can significantly speed up inference for modern Mixture-of-Experts models such as Mixtral-8x7B (see Section 4 for detailed evaluation).\n# 3.2 Speculative Expert Loading\nWhile LRU caching can reduce the average expert loading time, most of the inference time is still spent waiting for the next expert to be loaded. The reason behind this is that, unlike with dense models, MoE offloading cannot effectively overlap expert loading with computation. To understand this problem, let us zoom into the process of generating a single token, layer-by-layer. The full compute workload starts by embedding the previous token via look-up, then alternates between running self-attention and MLP for each transformer block in the model. Finally, the outputs from the last transformer block are used to predict next token logits with a linear projection.",
            "Comparison with Llama 2 70B and GPT-3.5. In Table 3, we report the performance of Mixtral 8x7B compared to Llama 2 70B and GPT-3.5. We observe that Mixtral performs similarly or above the two other models. On MMLU, Mixtral obtains a better performance, despite its significantly smaller capacity (47B tokens compared to 70B). For MT Bench, we report the performance of the latest GPT-3.5-Turbo model available, gpt-3.5-turbo-1106.\n2Since Llama 2 34B was not open-sourced, we report results for Llama 1 34B.\n4"
        ],
        "response": "Mixtral 8x7B uses a sparse Mixture-of-Experts (SMoE) architecture with 8 feedforward blocks per layer, while Mistral 7B uses a dense feedforward block [source: http://arxiv.org/pdf/2401.04088]. Mixtral processes each token using only two experts per layer, resulting in 13B active parameters during inference, compared to Mistral 7B's full parameter count [source: http://arxiv.org/pdf/2401.04088]. The router network in Mixtral selects two experts dynamically for each token at each layer, unlike Mistral 7B's static feedforward processing [source: http://arxiv.org/pdf/2401.04088]. Mixtral has a total of 47B parameters, but only 13B are active per token, while Mistral 7B uses all its 7B parameters for every token [source: http://arxiv.org/pdf/2401.04088]. This sparsity enables Mixtral to outperform larger models like Llama 2 70B with fewer active parameters [source: http://arxiv.org/pdf/2401.04088].",
        "reference": "The architectural difference between Mixtral 8x7B and Mistral 7B lies in the number of feedforward blocks (experts) in each layer. Mixtral 8x7B has 8 feedforward blocks per layer, while Mistral 7B has a different number of feedforward blocks. Additionally, during inference, each token in both models has access to 47B parameters, but only 13B active parameters are used."
    },
    {
        "user_input": "When is offloading used on the A100 server for accelerating MoE-based language models?",
        "retrieved_contexts": [
            "1When deployed in 16-bit precision, Falcon-180B needs approximately 360GB, while LLaMA-2 70B requires 140GB of combined accelerator memory.\nIn this work, we systematically develop techniques for running large MoE language models with limited GPU memory. Our main objective is inferencing (generating tokens) with Mixtral-8x7B- Instruct â a MoE-based chat assistant â on a desktop-grade hardware where only a fraction of experts fit into the accelerator memory. To that end:\nwe observe how MoE language model accesses its experts between tokens, and find several regularities: i) some experts are reused between adjacent tokens and ii) the model hidden states of early layers already âknowâ which experts are to be used at subsequent layers. â¢ we design a MoE-specific offloading strategy that takes advantage of these regularities: i) it uses LRU cache to significantly reduces GPU-RAM communication, leading to faster generation and ii) it guesses which experts are needed ahead of time to better overlap expert loading with computation.\nâ¢ we consider the specific scenario of running Mixtral-8x7B-Instruct on a T4, RTX 3060 and RTX 3080 Mobile and develop a practical combination of mixed quantization and the proposed offloading algorithm to run this model interactively at 2-3 tokens per second depending on the hardware. The source code with our implementation is available online2",
            "We consider four hardware configurations: a free-tier Colab instance with a T4 GPU (16GB VRAM, PCIe Gen.3), a past generation gaming laptop with RTX 3080 Mobile (16GB, PCIe Gen.4), a mid- range gaming desktop with RTX 3060 (12GB, PCIe Gen.3) and a high-end data-center server with A100-80GB-SXM. Note that the A100 server could run the model without offloading. We use offloading on A100 mostly to provide a reference for other setups. Finally, when evaluating 3-bit models, we use a cloud T4 from Microsoft Azure because the free-tier colab instances did not have enough RAM for this specific configuration. We use k = 2 for RTX 3060 and k = 4 for all other GPUs.\nAs shown in Table 2, all evaluated setups can generate 2-4 tokens per second with the full algorithm. Using pre-loading appears to be most beneficial on RTX 3060, possibly due to lower LRU cache size. Cursiously, RTX 3060 (desktop) performs nearly equally with a much higher end 3080 Mobile. We attribute this to the fact that both GPUs are still bottlenecked by host-to-device bandwidth, limited by the PCIe architecture. Finally, all schemes significantly outperform naive offloading that loads the entire MoE layer.\n# 5 Conclusion and Future Work",
            "# Denis Mazur Moscow Institute of Physics and Technology Yandex Researchcore denismazur8@gmail.com\n# Abstract\nWith the widespread adoption of Large Language Models (LLMs), many deep learning practitioners are looking for strategies of running these models more efficiently. One such strategy is to use sparse Mixture-of-Experts (MoE) â a type of model architectures where only a fraction of model layers are active for any given input. This property allows MoE-based language models to generate tokens faster than their âdenseâ counterparts, but it also increases model size due to having multiple âexpertsâ. Unfortunately, this makes state-of-the-art MoE language models difficult to run without high-end GPUs. In this work, we study the problem of running large MoE language models on consumer hardware with limited accelerator memory. We build upon parameter offloading algorithms and propose a novel strategy that accelerates offloading by taking advantage of innate properties of MoE LLMs. Using this strategy, we build can run Mixtral-8x7B with mixed quantization on desktop hardware and free-tier Google Colab instances.\n# Introduction",
            "# 5 Conclusion and Future Work\nIn this work, we explore strategies for accelerating Mixture-of-Experts based language models on consumer hardware with limited GPU memory. We propose a MoE-centric approach to offloading\n7\nand explore how mixed quantization affects perplexity and performance on language understanding tasks. We evaluate the proposed strategies and show that they produce a significant increase in generation speed compared to naÂ¨ve approaches on consumer-grade hardware, including free-tier Google Colab.\nOur method provides a practical solution for inferencing large MoE language models on resource- constricted hardware, enabling broader access to these powerful models for research and development. As future work, we plan to explore further offloading strategies, based on speculative expert predic- tion.\n# Acknowledgements\nAuthors would like to acknowledge mobicham@ for helpful discussions on Mixtral quantization.\n# References\nAminabadi, R. Y., Rajbhandari, S., Awan, A. A., Li, C., Li, D., Zheng, E., Ruwase, O., Smith, S., Zhang, M., Rasley, J., and He, Y. Deepspeed-inference: Enabling efficient inference of transformer models at unprecedented scale. In Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis, SC â22. IEEE Press, 2022. ISBN 9784665454445.",
            "For regular (dense) models, this architecture allows for efficient offloading schedule that pre-loads the next transformer layer ahead of time, while the previous layer is still running. Unfortunately, this schedule is no longer possible for Mixture-of-Experts models, where MoE MLP layers choose which experts to load just-in-time for computation. This is because the system cannot pre-fetch the next layer until it learns which experts should be loaded. Modern open-access MoE language models choose active experts using the final outputs of the previous layer, which means they cannot be pre-fetched them in parallel with previous layer. While it is not possible6 to pre-reliably prefetch the next set of experts ahead of time, the system could still try to guess the likely next experts and load them speculatively, while processing the previous layer. It the guess is correct, it will speed up the next layer inference; if not, it can load the actual next layerâs experts later. In other words, this type of speculative loading does not change the final model predictions, but may reduce latency if the guess is accurate enough.\n6More specifically, not possible without changing the model architecture, which would require re-training\n4",
            "To speed up offloading in practice, we allocate all expert parameters in a contiguous memory buffer that can be moved as a single host-to-device copy. For host-side (RAM) experts, we pin8 this memory buffer for faster communication. Our implementation additionally allocates b=4 on-device buffers used to copy and prefetch experts asynchronously, without modifying existing experts. These buffers are shared between all MoE layers to reduce memory footprint. Overall, the system requires num_layers Ã num_experts expert memory buffers split between host and device memory and b=4 temporary buffers, the size of each buffer being equal to a single expert.\n# 4 Experiments\nIn this section, we verify our earlier hypotheses about MoE behavior and benchmark the inference latency in different conditions. We focus our evaluations on Mixtral-8x7B and Mixtral-8x7B-Instruct models since they represent the current state of the art among open-access MoE models. We organize this section as follows: Section 4.1 measures the effectiveness of expert caching and pre-loading in isolation, Section 4.2 compares different model compression algorithms and verifies our hypotheses from Section 3.3. Finally, Section 4.3 measures the inference latency in several hardware setups.",
            "Since then, several lines of work explore Mixture-of-Experts with Transformer-based language models for machine translation Lepikhin et al. (2020), masked language modeling Fedus et al. (2021), general-purpose LLMs Du et al. (2022) and others. Most of these models follow traditional (dense) Transformer architecture for embeddings and attention layers, and only use Mixture for the feedforward (MLP) blocks and use a linear token-level gating function. A common observation across most of these works is that MoE models are cheaper to train and inference Fedus et al. (2021); Lepikhin et al. (2020), but require more parameters than a dense model with equivalent perplexity. Pre-trained Mixture-of-Experts LLMs have been openly available for over a year3. However, these models seem to have gained less traction than equivalent dense models, arguable because their sheer model size (over a trillion parameters) makes them difficult to use. Most recently, Mistral AI released a family of sparse Mixture of Experts models called Mixtral-8x7B with near state-of-the-art performance Mixtral AI team (2023). This model has already inspired several follow-up works and practical applications, but it still requires a high-end GPU accelerator.\n# 2.2 Post-training Quantization of LLMs",
            "Table 1: Perplexity and model size evaluation of Mixtral-8x7B with different quantization for shared attention (Attn quant) and experts (Experts quant) layers. For comprarison, a Mistral-7B 4-bit quantized model has Wiki2 perplexity 5.03, C4 perplexity 7.56 and MMLU score 61.3%. See Section 4.2 for details. Green values correspond to the configurations we chose for full system evaluation.\n6\nAlgorithm 2-bit Experts 3-bit Experts A100 3080 Mobile 3060 T4 (Colab) A100 3080 Mobile 3060 T4 (Cloud) 3.061 Full algorithm 2.918 W/o expert pre-loading 2.265 W/o LRU cache & pre-loading Naive offloading (accelerate) 1.392 2.655 2.227 1.758 1.059 2.278 2.051 1.547 0.919 2.092 1.567 1.168 0.661 2.845 2.683 2.055 1.246 2.475 2.024 1.595 0.914 2.038 1.857 1.346 1.791 1.603 1.365 1.061 0.580",
            "# 4.3 Practical offloading performance\nFinally we evaluate the performance of the Mixtral8x7B-Instruct model using the offloading tech- niquesproposed throughout this report. Based on the perplexity evaluations from the previous section, we chose 4-bit HQQ quantization for the shared attention layers and 2- or 3-bit quantization for experts. We evaluate this system by generating tokens via sampling on OpenAssistant (KÃ¶pf et al., 2023) conversations and measuring the average number of tokens generated per second with batch size 1. For this evaluation, we always sample proportionally to the predicted probabilities, i.e. without temperature or nucleus sampling.",
            "Several recent works modify transformer architecture by introducing sparse Mixture-of-Experts blocks (Jacobs et al., 1991; Shazeer et al., 2017). MoE blocks contain multiple âexpertsâ (layers), as well as a âgating functionâ that selects which experts are used on a given input. As a result, the MoE block uses a small portion of all âexpertsâ for any single forward pass, allowing for more compute-efficient training Fedus et al. (2021); Du et al. (2022). Notably, MoEs are among the largest Fedus et al. (2021) and among the best Mixtral AI team (2023) of available LLMs. While Mixture-of-Experts models can be more efficient than their dense counterparts, many techniques for efficient LLM inference were not designed with MoE in mind and perform suboptimally on modern large language models that use mixture-of-experts layers.\n1When deployed in 16-bit precision, Falcon-180B needs approximately 360GB, while LLaMA-2 70B requires 140GB of combined accelerator memory."
        ],
        "response": "Offloading is used on the A100 server to provide a reference for other setups, even though the A100 could run the model without offloading. The offloading strategy on A100 helps evaluate the effectiveness of the proposed techniques on high-end hardware [source: http://arxiv.org/pdf/2312.17238].",
        "reference": "Offloading is used on the A100 server for accelerating MoE-based language models when there is resource-constricted hardware and the goal is to enable broader access to these powerful models for research and development."
    },
    {
        "user_input": "Which model outperforms Llama 2 70B on multilingual benchmarks?",
        "retrieved_contexts": [
            "# 3.1 Multilingual benchmarks\nCompared to Mistral 7B, we significantly upsample the proportion of multilingual data during pretraining. The extra capacity allows Mixtral to perform well on multilingual benchmarks while maintaining a high accuracy in English. In particular, Mixtral significantly outperforms Llama 2 70B in French, German, Spanish, and Italian, as shown in Table 4.\nActive Params French Arc-c HellaS MMLU German Arc-c HellaS MMLU Spanish Arc-c HellaS MMLU Italian Arc-c HellaS MMLU 33B 70B 13B 42.9% 65.4% 49.0% 39.3% 68.1% 49.9% 49.9% 72.5% 64.3% 49.4% 70.9% 65.1% 58.2% 77.4% 70.9% 54.3% 73.0% 71.5% 55.4% 77.6% 72.5% 52.8% 75.1% 70.9% 41.1% 63.3% 48.7% 47.3% 68.7% 64.2% 45.7% 69.8% 52.3% 50.5% 74.5% 66.0%",
            "Comparison with Llama 2 70B and GPT-3.5. In Table 3, we report the performance of Mixtral 8x7B compared to Llama 2 70B and GPT-3.5. We observe that Mixtral performs similarly or above the two other models. On MMLU, Mixtral obtains a better performance, despite its significantly smaller capacity (47B tokens compared to 70B). For MT Bench, we report the performance of the latest GPT-3.5-Turbo model available, gpt-3.5-turbo-1106.\n2Since Llama 2 34B was not open-sourced, we report results for Llama 1 34B.\n4",
            "Abstract\nWe introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine- tuned to follow instructions, Mixtral 8x7B â Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â chat model on human bench- marks. Both the base and instruct models are released under the Apache 2.0 license.\nCode: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/mixtral-of-experts/",
            "2Since Llama 2 34B was not open-sourced, we report results for Llama 1 34B.\n4\nLLaMA 2 70B GPT-3.5 MMLU (MCQ in 57 subjects) 69.9% 70.0% 70.6% HellaSwag (10-shot) 87.1% 85.5% 86.7% ARC Challenge (25-shot) 85.1% 85.2% 85.8% WinoGrande (5-shot) 83.2% 81.6% 81.2% MBPP (pass@1) 49.8% 52.2% 60.7% GSM-8K (5-shot) 53.6% 57.1% 58.4% MT Bench (for Instruct Models) 6.86 8.32 8.30\n# Mixtral 8x7B\nTable 3: Comparison of Mixtral with Llama 2 70B and GPT-3.5. Mixtral outperforms or matches Llama 2 70B and GPT-3.5 performance on most metrics.\nEvaluation Differences. On some benchmarks, there are some differences between our evaluation protocol and the one reported in the Llama 2 paper: 1) on MBPP, we use the hand-verified subset 2) on TriviaQA, we do not provide Wikipedia contexts.\n# 3.1 Multilingual benchmarks",
            "Table 2: Comparison of Mixtral with Llama. Mixtral outperforms or matches Llama 2 70B performance on almost all popular benchmarks while using 5x fewer active parameters during inference.\n70 Mixtral 8x7B. âMixtral 8x7B Mixtral 8x7B 355 =o = Es & E60! Mistral 78 % 2681 Mistral 78 3 3 s0 5 = A % 66 50 g 4 45 64 78 138 348708 78 138 348708 78 138 348 70B S66 Mixtral 8x7B 50 Mixtral 8x7B 5 = 564 340 g al Mistral 78 ee Mistral 78 3 5 Â§ 30 5 eo â= Mistral Â° 20 âe LlaMA2 78 (138 348 70B 7B (138 348 708 7B Â«13B 34B 708 Active Params Active Params Active Params\nFigure 3: Results on MMLU, commonsense reasoning, world knowledge and reading comprehension, math and code for Mistral (7B/8x7B) vs Llama 2 (7B/13B/70B). Mixtral largely outperforms Llama 2 70B on all benchmarks, except on reading comprehension benchmarks while using 5x lower active parameters. It is also vastly superior to Llama 2 70B on code and math.",
            "Detailed results for Mixtral, Mistral 7B and Llama 2 7B/13B/70B and Llama 1 34B2 are reported in Table 2. Figure 2 compares the performance of Mixtral with the Llama models in different categories. Mixtral surpasses Llama 2 70B across most metrics. In particular, Mixtral displays a superior performance in code and mathematics benchmarks.\nSize and Efficiency. We compare our performance to the Llama 2 family, aiming to understand Mixtral modelsâ efficiency in the cost-performance spectrum (see Figure 3). As a sparse Mixture- of-Experts model, Mixtral only uses 13B active parameters for each token. With 5x lower active parameters, Mixtral is able to outperform Llama 2 70B across most categories.\nNote that this analysis focuses on the active parameter count (see Section 2.1), which is directly proportional to the inference compute cost, but does not consider the memory costs and hardware utilization. The memory costs for serving Mixtral are proportional to its sparse parameter count, 47B, which is still smaller than Llama 2 70B. As for device utilization, we note that the SMoEs layer introduces additional overhead due to the routing mechanism and due to the increased memory loads when running more than one expert per device. They are more suitable for batched workloads where one can reach a good degree of arithmetic intensity.",
            "Table 4: Comparison of Mixtral with Llama on Multilingual Benchmarks. On ARC Challenge, Hellaswag, and MMLU, Mixtral outperforms Llama 2 70B on 4 languages: French, German, Spanish, and Italian.\n# 3.2 Long range performance\nTo assess the capabilities of Mixtral to tackle long context, we evaluate it on the passkey retrieval task introduced in [23], a synthetic task designed to measure the ability of the model to retrieve a passkey inserted randomly in a long prompt. Results in Figure 4 (Left) show that Mixtral achieves a 100% retrieval accuracy regardless of the context length or the position of passkey in the sequence. Figure 4 (Right) shows that the perplexity of Mixtral on a subset of the proof-pile dataset [2] decreases monotonically as the size of the context increases.\nPasskey Performance ry 0.8 0.6 04 0.2 0.0 OK 4K 8K 12K 16K 20K 24K 28K Seq Len Passkey Loc\n3.8 â Mixtral_8x7B 3.5 32 > $3.0 i] 228 fos a 2.0 0 5k 10k 15k 20k 25k 30k Context length",
            "Code: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/mixtral-of-experts/\n# Introduction\nIn this paper, we present Mixtral 8x7B, a sparse mixture of experts model (SMoE) with open weights, licensed under Apache 2.0. Mixtral outperforms Llama 2 70B and GPT-3.5 on most benchmarks. As it only uses a subset of its parameters for every token, Mixtral allows faster inference speed at low batch-sizes, and higher throughput at large batch-sizes.\nMixtral is a sparse mixture-of-experts network. It is a decoder-only model where the feedforward block picks from a set of 8 distinct groups of parameters. At every layer, for every token, a router network chooses two of these groups (the âexpertsâ) to process the token and combine their output additively. This technique increases the number of parameters of a model while controlling cost and latency, as the model only uses a fraction of the total set of parameters per token.\nMixtral is pretrained with multilingual data using a context size of 32k tokens. It either matches or exceeds the performance of Llama 2 70B and GPT-3.5, over several benchmarks. In particular,\nMixture of Experts Layer i gating inputs af outputs router expert",
            "Llama 2 70B Mixtral 8x7B BBQ accuracy 51.5% 56.0% BOLD sentiment score (avg Â± std) gender profession religious_ideology political_ideology race 0.293 Â± 0.073 0.218 Â± 0.073 0.188 Â± 0.133 0.149 Â± 0.140 0.232 Â± 0.049 0.323 Â±0.045 0.243 Â± 0.087 0.144 Â± 0.089 0.186 Â± 0.146 0.232 Â± 0.052\nFigure 5: Bias Benchmarks. Compared Llama 2 70B, Mixtral presents less bias (higher accuracy on BBQ, lower std on BOLD) and displays more positive sentiment (higher avg on BOLD).\nWe benchmark Llama 2 and Mixtral on BBQ and BOLD with our evaluation framework and report the results in Table 5. Compared to Llama 2, Mixtral presents less bias on the BBQ benchmark (56.0% vs 51.5%). For each group in BOLD, a higher average sentiment score means more positive sentiments and a lower standard deviation indicates less bias within the group. Overall, Mixtral displays more positive sentiments than Llama 2, with similar variances within each group.\n# Instruction Fine-tuning",
            "language model series characterized by distinct models with varying parameter counts [14], and the Qwen-14B open-source chat model is utilized in the current study. Aquila2 represents a language model series devised by BAAI, noted for surpassing comparable models in terms is of performance [22], and the Aquila2-34B chat model employed in this research. LLaMA2 constitutes a suite of pre-trained and fine-tuned LLMs, with scales ranging from 7 billion to 70 billion parameters [17]. Following additional training, LLaMA2-70B is incorporated into Xinyu2-70B."
        ],
        "response": "Mixtral 8x7B outperforms Llama 2 70B on multilingual benchmarks including French, German, Spanish, and Italian as shown in Table 4. It achieves higher accuracy on ARC Challenge, Hellaswag, and MMLU in these languages. The model's multilingual performance is attributed to upsampling multilingual data during pretraining. Mixtral maintains high English accuracy while excelling in other languages. It outperforms Llama 2 70B significantly in these benchmarks. [source: http://arxiv.org/pdf/2401.04088]",
        "reference": "Mixtral outperforms Llama 2 70B on multilingual benchmarks."
    },
    {
        "user_input": "How does the use of sparse Mixture-of-Experts (MoE) in language models contribute to faster token generation?",
        "retrieved_contexts": [
            "# Denis Mazur Moscow Institute of Physics and Technology Yandex Researchcore denismazur8@gmail.com\n# Abstract\nWith the widespread adoption of Large Language Models (LLMs), many deep learning practitioners are looking for strategies of running these models more efficiently. One such strategy is to use sparse Mixture-of-Experts (MoE) â a type of model architectures where only a fraction of model layers are active for any given input. This property allows MoE-based language models to generate tokens faster than their âdenseâ counterparts, but it also increases model size due to having multiple âexpertsâ. Unfortunately, this makes state-of-the-art MoE language models difficult to run without high-end GPUs. In this work, we study the problem of running large MoE language models on consumer hardware with limited accelerator memory. We build upon parameter offloading algorithms and propose a novel strategy that accelerates offloading by taking advantage of innate properties of MoE LLMs. Using this strategy, we build can run Mixtral-8x7B with mixed quantization on desktop hardware and free-tier Google Colab instances.\n# Introduction",
            "1When deployed in 16-bit precision, Falcon-180B needs approximately 360GB, while LLaMA-2 70B requires 140GB of combined accelerator memory.\nIn this work, we systematically develop techniques for running large MoE language models with limited GPU memory. Our main objective is inferencing (generating tokens) with Mixtral-8x7B- Instruct â a MoE-based chat assistant â on a desktop-grade hardware where only a fraction of experts fit into the accelerator memory. To that end:\nwe observe how MoE language model accesses its experts between tokens, and find several regularities: i) some experts are reused between adjacent tokens and ii) the model hidden states of early layers already âknowâ which experts are to be used at subsequent layers. â¢ we design a MoE-specific offloading strategy that takes advantage of these regularities: i) it uses LRU cache to significantly reduces GPU-RAM communication, leading to faster generation and ii) it guesses which experts are needed ahead of time to better overlap expert loading with computation.\nâ¢ we consider the specific scenario of running Mixtral-8x7B-Instruct on a T4, RTX 3060 and RTX 3080 Mobile and develop a practical combination of mixed quantization and the proposed offloading algorithm to run this model interactively at 2-3 tokens per second depending on the hardware. The source code with our implementation is available online2",
            "Shazeer et al. (2017) builds on this idea to train a sparsely gated Mixture-of-Experts to serve as a language model. The full model consists of a recurrent neural network backbone and a MoE module with up to 131072 experts. When processing a given token, a linear gating function select 4 most suitable experts based on the latest hidden state. The resulting model (including the gating function and experts) is trained end-to-end to minimize cross-entropy, with an additional regularizer to promote equal expert utilization. Shazeer et al. (2017) observed that the MoE model not only improves perplexity, but also learns interpretable expert specializations: some experts would âspecializeâ on prepositions, while others learn to express a particular concept (e.g. speed).",
            "Since then, several lines of work explore Mixture-of-Experts with Transformer-based language models for machine translation Lepikhin et al. (2020), masked language modeling Fedus et al. (2021), general-purpose LLMs Du et al. (2022) and others. Most of these models follow traditional (dense) Transformer architecture for embeddings and attention layers, and only use Mixture for the feedforward (MLP) blocks and use a linear token-level gating function. A common observation across most of these works is that MoE models are cheaper to train and inference Fedus et al. (2021); Lepikhin et al. (2020), but require more parameters than a dense model with equivalent perplexity. Pre-trained Mixture-of-Experts LLMs have been openly available for over a year3. However, these models seem to have gained less traction than equivalent dense models, arguable because their sheer model size (over a trillion parameters) makes them difficult to use. Most recently, Mistral AI released a family of sparse Mixture of Experts models called Mixtral-8x7B with near state-of-the-art performance Mixtral AI team (2023). This model has already inspired several follow-up works and practical applications, but it still requires a high-end GPU accelerator.\n# 2.2 Post-training Quantization of LLMs",
            "# 5 Conclusion and Future Work\nIn this work, we explore strategies for accelerating Mixture-of-Experts based language models on consumer hardware with limited GPU memory. We propose a MoE-centric approach to offloading\n7\nand explore how mixed quantization affects perplexity and performance on language understanding tasks. We evaluate the proposed strategies and show that they produce a significant increase in generation speed compared to naÂ¨ve approaches on consumer-grade hardware, including free-tier Google Colab.\nOur method provides a practical solution for inferencing large MoE language models on resource- constricted hardware, enabling broader access to these powerful models for research and development. As future work, we plan to explore further offloading strategies, based on speculative expert predic- tion.\n# Acknowledgements\nAuthors would like to acknowledge mobicham@ for helpful discussions on Mixtral quantization.\n# References\nAminabadi, R. Y., Rajbhandari, S., Awan, A. A., Li, C., Li, D., Zheng, E., Ruwase, O., Smith, S., Zhang, M., Rasley, J., and He, Y. Deepspeed-inference: Enabling efficient inference of transformer models at unprecedented scale. In Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis, SC â22. IEEE Press, 2022. ISBN 9784665454445.",
            "Several recent works modify transformer architecture by introducing sparse Mixture-of-Experts blocks (Jacobs et al., 1991; Shazeer et al., 2017). MoE blocks contain multiple âexpertsâ (layers), as well as a âgating functionâ that selects which experts are used on a given input. As a result, the MoE block uses a small portion of all âexpertsâ for any single forward pass, allowing for more compute-efficient training Fedus et al. (2021); Du et al. (2022). Notably, MoEs are among the largest Fedus et al. (2021) and among the best Mixtral AI team (2023) of available LLMs. While Mixture-of-Experts models can be more efficient than their dense counterparts, many techniques for efficient LLM inference were not designed with MoE in mind and perform suboptimally on modern large language models that use mixture-of-experts layers.\n1When deployed in 16-bit precision, Falcon-180B needs approximately 360GB, while LLaMA-2 70B requires 140GB of combined accelerator memory.",
            "Code: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/mixtral-of-experts/\n# Introduction\nIn this paper, we present Mixtral 8x7B, a sparse mixture of experts model (SMoE) with open weights, licensed under Apache 2.0. Mixtral outperforms Llama 2 70B and GPT-3.5 on most benchmarks. As it only uses a subset of its parameters for every token, Mixtral allows faster inference speed at low batch-sizes, and higher throughput at large batch-sizes.\nMixtral is a sparse mixture-of-experts network. It is a decoder-only model where the feedforward block picks from a set of 8 distinct groups of parameters. At every layer, for every token, a router network chooses two of these groups (the âexpertsâ) to process the token and combine their output additively. This technique increases the number of parameters of a model while controlling cost and latency, as the model only uses a fraction of the total set of parameters per token.\nMixtral is pretrained with multilingual data using a context size of 32k tokens. It either matches or exceeds the performance of Llama 2 70B and GPT-3.5, over several benchmarks. In particular,\nMixture of Experts Layer i gating inputs af outputs router expert",
            "The generative inference workload consists of two phases: 1) encoding the input prompt and 2) generating tokens conditioned on that prompt. The key difference between these two phases is that prompt tokens are encoded in parallel (layer-by-layer), whereas the generation runs sequentially (token-by-token and layer-by-layer). In general, phase 1 works relatively well with existing Mixture- of-Experts algorithms, since each layer can only be loaded once for the entire prompt. In turn, when generating tokens, one must load layer once per each token generated. In practice, this means that inference speed is limited by how fast one can fetch parameters from system memory.\nBelow, we look for patterns in how the MoE model loads its experts and propose ways to exploit these patterns to speed up inference time.\n4To learn more about these methods, please refer to surveys such as Gholami et al. (2021); Liang et al. (2021) 5As opposed to running a processing a large batch of texts over many seconds, as in Sheng et al. (2023)\n3\nSelected experts for Mixtral-8x7B-Instruct woe 0 (top) and 15 ae =n a oa ao a âme: a n: ee Layer 15 expert # Layer 0 expert # MAUR STARR O However about |= and 4 training data owerful language model based trained Trans former f architecture",
            "[10] Jwala Dhamala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada Pruksachatkun, Kai-Wei Chang, and Rahul Gupta. Bold: Dataset and metrics for measuring biases in open-ended language generation. In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency, pages 862â872, 2021.\n[11] Artyom Eliseev and Denis Mazur. Fast inference of mixture-of-experts language models with offloading. arXiv preprint arXiv:2312.17238, 2023.\n[12] William Fedus, Jeff Dean, and Barret Zoph. A review of sparse expert models in deep learning. arXiv preprint arXiv:2209.01667, 2022.\n[13] Trevor Gale, Deepak Narayanan, Cliff Young, and Matei Zaharia. Megablocks: Efficient sparse training with mixture-of-experts. arXiv preprint arXiv:2211.15841, 2022.",
            "We illustrate an example of how LRU cache saves experts in Figure 1 (see caption). LRU is a very simple strategy that does not consider factors like expert activation frequencies, varying cache size between MoE layers, or any sequential patterns in expert activation. However, we found that even this simple strategy can significantly speed up inference for modern Mixture-of-Experts models such as Mixtral-8x7B (see Section 4 for detailed evaluation).\n# 3.2 Speculative Expert Loading\nWhile LRU caching can reduce the average expert loading time, most of the inference time is still spent waiting for the next expert to be loaded. The reason behind this is that, unlike with dense models, MoE offloading cannot effectively overlap expert loading with computation. To understand this problem, let us zoom into the process of generating a single token, layer-by-layer. The full compute workload starts by embedding the previous token via look-up, then alternates between running self-attention and MLP for each transformer block in the model. Finally, the outputs from the last transformer block are used to predict next token logits with a linear projection."
        ],
        "response": "Sparse Mixture-of-Experts (MoE) in language models contributes to faster token generation by only activating a subset of experts for each input token, reducing computational load. This sparsity allows the model to process tokens more efficiently than dense models that use all parameters [http://arxiv.org/pdf/2312.17238]. The gating mechanism selects the most relevant experts, ensuring optimal resource utilization [http://arxiv.org/pdf/2312.17238]. Additionally, techniques like LRU caching minimize GPU-RAM communication by reusing experts across adjacent tokens [http://arxiv.org/pdf/2312.17238]. Speculative expert loading further speeds up inference by predicting needed experts ahead of time, overlapping computation with data transfer [http://arxiv.org/pdf/2312.17238]. These optimizations collectively reduce latency and increase throughput, especially on hardware with limited memory [http://arxiv.org/pdf/2312.17238].",
        "reference": "The use of sparse Mixture-of-Experts (MoE) in language models contributes to faster token generation because only a fraction of model layers are active for any given input. This means that MoE-based language models can generate tokens faster than their \"dense\" counterparts. The MoE blocks in the model contain multiple \"experts\" (layers) and a \"gating function\" that selects which experts are used for a given input. By using a small portion of all \"experts\" for each forward pass, the MoE block allows for more compute-efficient training, leading to faster token generation."
    },
    {
        "user_input": "What impact does the use of sparse Mixture-of-Experts (MoE) have on the size of language models?",
        "retrieved_contexts": [
            "# Denis Mazur Moscow Institute of Physics and Technology Yandex Researchcore denismazur8@gmail.com\n# Abstract\nWith the widespread adoption of Large Language Models (LLMs), many deep learning practitioners are looking for strategies of running these models more efficiently. One such strategy is to use sparse Mixture-of-Experts (MoE) â a type of model architectures where only a fraction of model layers are active for any given input. This property allows MoE-based language models to generate tokens faster than their âdenseâ counterparts, but it also increases model size due to having multiple âexpertsâ. Unfortunately, this makes state-of-the-art MoE language models difficult to run without high-end GPUs. In this work, we study the problem of running large MoE language models on consumer hardware with limited accelerator memory. We build upon parameter offloading algorithms and propose a novel strategy that accelerates offloading by taking advantage of innate properties of MoE LLMs. Using this strategy, we build can run Mixtral-8x7B with mixed quantization on desktop hardware and free-tier Google Colab instances.\n# Introduction",
            "Several recent works modify transformer architecture by introducing sparse Mixture-of-Experts blocks (Jacobs et al., 1991; Shazeer et al., 2017). MoE blocks contain multiple âexpertsâ (layers), as well as a âgating functionâ that selects which experts are used on a given input. As a result, the MoE block uses a small portion of all âexpertsâ for any single forward pass, allowing for more compute-efficient training Fedus et al. (2021); Du et al. (2022). Notably, MoEs are among the largest Fedus et al. (2021) and among the best Mixtral AI team (2023) of available LLMs. While Mixture-of-Experts models can be more efficient than their dense counterparts, many techniques for efficient LLM inference were not designed with MoE in mind and perform suboptimally on modern large language models that use mixture-of-experts layers.\n1When deployed in 16-bit precision, Falcon-180B needs approximately 360GB, while LLaMA-2 70B requires 140GB of combined accelerator memory.",
            "Since then, several lines of work explore Mixture-of-Experts with Transformer-based language models for machine translation Lepikhin et al. (2020), masked language modeling Fedus et al. (2021), general-purpose LLMs Du et al. (2022) and others. Most of these models follow traditional (dense) Transformer architecture for embeddings and attention layers, and only use Mixture for the feedforward (MLP) blocks and use a linear token-level gating function. A common observation across most of these works is that MoE models are cheaper to train and inference Fedus et al. (2021); Lepikhin et al. (2020), but require more parameters than a dense model with equivalent perplexity. Pre-trained Mixture-of-Experts LLMs have been openly available for over a year3. However, these models seem to have gained less traction than equivalent dense models, arguable because their sheer model size (over a trillion parameters) makes them difficult to use. Most recently, Mistral AI released a family of sparse Mixture of Experts models called Mixtral-8x7B with near state-of-the-art performance Mixtral AI team (2023). This model has already inspired several follow-up works and practical applications, but it still requires a high-end GPU accelerator.\n# 2.2 Post-training Quantization of LLMs",
            "Shazeer et al. (2017) builds on this idea to train a sparsely gated Mixture-of-Experts to serve as a language model. The full model consists of a recurrent neural network backbone and a MoE module with up to 131072 experts. When processing a given token, a linear gating function select 4 most suitable experts based on the latest hidden state. The resulting model (including the gating function and experts) is trained end-to-end to minimize cross-entropy, with an additional regularizer to promote equal expert utilization. Shazeer et al. (2017) observed that the MoE model not only improves perplexity, but also learns interpretable expert specializations: some experts would âspecializeâ on prepositions, while others learn to express a particular concept (e.g. speed).",
            "# 2 Background & Related Work\n# 2.1 Mixture-of-Experts\nThe recent surge in MoE language models builds on a relatively old idea (Jacobs et al., 1991; Jordan & Jacobs, 1994) of training ensembles of specialized models (âexpertsâ) and a gating function to select the right expert for the task. To achieve specialization, Mixture-of-Experts learn by simultaneously i) training the gating function to choose the best experts and ii) training the experts themselves on samples assigned to them by the gating function. Since then, many different MoE variants emerged, including mixture of SVM models (Collobert et al., 2002), Dirichlet processes (Shahbaba & Neal, 2009) and various neural networks.",
            "Abstract\nWe introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine- tuned to follow instructions, Mixtral 8x7B â Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â chat model on human bench- marks. Both the base and instruct models are released under the Apache 2.0 license.\nCode: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/mixtral-of-experts/",
            "[10] Jwala Dhamala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada Pruksachatkun, Kai-Wei Chang, and Rahul Gupta. Bold: Dataset and metrics for measuring biases in open-ended language generation. In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency, pages 862â872, 2021.\n[11] Artyom Eliseev and Denis Mazur. Fast inference of mixture-of-experts language models with offloading. arXiv preprint arXiv:2312.17238, 2023.\n[12] William Fedus, Jeff Dean, and Barret Zoph. A review of sparse expert models in deep learning. arXiv preprint arXiv:2209.01667, 2022.\n[13] Trevor Gale, Deepak Narayanan, Cliff Young, and Matei Zaharia. Megablocks: Efficient sparse training with mixture-of-experts. arXiv preprint arXiv:2211.15841, 2022.",
            "Code: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/mixtral-of-experts/\n# Introduction\nIn this paper, we present Mixtral 8x7B, a sparse mixture of experts model (SMoE) with open weights, licensed under Apache 2.0. Mixtral outperforms Llama 2 70B and GPT-3.5 on most benchmarks. As it only uses a subset of its parameters for every token, Mixtral allows faster inference speed at low batch-sizes, and higher throughput at large batch-sizes.\nMixtral is a sparse mixture-of-experts network. It is a decoder-only model where the feedforward block picks from a set of 8 distinct groups of parameters. At every layer, for every token, a router network chooses two of these groups (the âexpertsâ) to process the token and combine their output additively. This technique increases the number of parameters of a model while controlling cost and latency, as the model only uses a fraction of the total set of parameters per token.\nMixtral is pretrained with multilingual data using a context size of 32k tokens. It either matches or exceeds the performance of Llama 2 70B and GPT-3.5, over several benchmarks. In particular,\nMixture of Experts Layer i gating inputs af outputs router expert",
            "dim n_layers head_dim hidden_dim n_heads n_kv_heads context_len vocab_size num_experts top_k_experts\n# 2.1 Sparse Mixture of Experts\nWe present a brief overview of the Mixture of Experts layer (Figure 1). For a more in-depth overview, see [12]. The output of the MoE module for a given input x is determined by the weighted sum of the outputs of the expert networks, where the weights are given by the gating networkâs output. i.e. given n expert networks {E0, Ei, ..., Enâ1}, the output of the expert layer is given by: Table 1: Model architecture.\n# j nâ\nG(x)i Â· Ei(x). i=0\nHere, G(x)i denotes the n-dimensional output of the gating network for the i-th expert, and Ei(x) is the output of the i-th expert network. If the gating vector is sparse, we can avoid computing the outputs of experts whose gates are zero. There are multiple alternative ways of implementing G(x) [6, 15, 35], but a simple and performant one is implemented by taking the softmax over the Top-K logits of a linear layer [28]. We use\nG(x) := Softmax(TopK(x Â· Wg)),",
            "# 5 Conclusion and Future Work\nIn this work, we explore strategies for accelerating Mixture-of-Experts based language models on consumer hardware with limited GPU memory. We propose a MoE-centric approach to offloading\n7\nand explore how mixed quantization affects perplexity and performance on language understanding tasks. We evaluate the proposed strategies and show that they produce a significant increase in generation speed compared to naÂ¨ve approaches on consumer-grade hardware, including free-tier Google Colab.\nOur method provides a practical solution for inferencing large MoE language models on resource- constricted hardware, enabling broader access to these powerful models for research and development. As future work, we plan to explore further offloading strategies, based on speculative expert predic- tion.\n# Acknowledgements\nAuthors would like to acknowledge mobicham@ for helpful discussions on Mixtral quantization.\n# References\nAminabadi, R. Y., Rajbhandari, S., Awan, A. A., Li, C., Li, D., Zheng, E., Ruwase, O., Smith, S., Zhang, M., Rasley, J., and He, Y. Deepspeed-inference: Enabling efficient inference of transformer models at unprecedented scale. In Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis, SC â22. IEEE Press, 2022. ISBN 9784665454445."
        ],
        "response": "Sparse Mixture-of-Experts (MoE) increases model size due to having multiple 'experts', making state-of-the-art MoE language models difficult to run without high-end GPUs [http://arxiv.org/pdf/2312.17238]. MoE models are among the largest available LLMs and require more parameters than dense models with equivalent perplexity [http://arxiv.org/pdf/2312.17238]. For example, Mixtral-8x7B has 47B parameters but only uses 13B active parameters during inference [http://arxiv.org/pdf/2401.04088]. This architecture allows each token to access a larger parameter space while controlling cost and latency [http://arxiv.org/pdf/2401.04088]. However, the increased model size necessitates advanced techniques like parameter offloading and mixed quantization for efficient deployment on consumer hardware [http://arxiv.org/pdf/2312.17238].",
        "reference": "The use of sparse Mixture-of-Experts (MoE) increases the size of language models due to having multiple \"experts.\""
    },
    {
        "user_input": "What are the design considerations and implementation details for inferencing MoE language models on consumer and low-end cloud hardware, including caching, prefetching techniques, and mixed MoE quantization?",
        "retrieved_contexts": [
            "6More specifically, not possible without changing the model architecture, which would require re-training\n4\nWhile analyzing modern MoE models, we found that it is possible to get an accurate guess of next layerâs experts by applying next layerâs gating function to previous layerâs hidden states â or, more specifically, to the same hidden states that are used by previous MoE layerâs gating function. This heuristic relies on the fact that transformer layers are residual, i.e. each layer adds to the previous hidden states instead of re-computing them from scratch. This architecture introduces an inductive bias such that any layerâs hidden states into a decent estimate of next layerâs hidden states.\n# 3.3 System Design & Implementation Details\nIn this section, we describe practical design considerations and implementation details that we used for inferencing MoE language models on consumer and low-end cloud hardware. Our system design combines the caching & prefetching techniques and a mixed MoE quantization scheme .",
            "# 5 Conclusion and Future Work\nIn this work, we explore strategies for accelerating Mixture-of-Experts based language models on consumer hardware with limited GPU memory. We propose a MoE-centric approach to offloading\n7\nand explore how mixed quantization affects perplexity and performance on language understanding tasks. We evaluate the proposed strategies and show that they produce a significant increase in generation speed compared to naÂ¨ve approaches on consumer-grade hardware, including free-tier Google Colab.\nOur method provides a practical solution for inferencing large MoE language models on resource- constricted hardware, enabling broader access to these powerful models for research and development. As future work, we plan to explore further offloading strategies, based on speculative expert predic- tion.\n# Acknowledgements\nAuthors would like to acknowledge mobicham@ for helpful discussions on Mixtral quantization.\n# References\nAminabadi, R. Y., Rajbhandari, S., Awan, A. A., Li, C., Li, D., Zheng, E., Ruwase, O., Smith, S., Zhang, M., Rasley, J., and He, Y. Deepspeed-inference: Enabling efficient inference of transformer models at unprecedented scale. In Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis, SC â22. IEEE Press, 2022. ISBN 9784665454445.",
            "1When deployed in 16-bit precision, Falcon-180B needs approximately 360GB, while LLaMA-2 70B requires 140GB of combined accelerator memory.\nIn this work, we systematically develop techniques for running large MoE language models with limited GPU memory. Our main objective is inferencing (generating tokens) with Mixtral-8x7B- Instruct â a MoE-based chat assistant â on a desktop-grade hardware where only a fraction of experts fit into the accelerator memory. To that end:\nwe observe how MoE language model accesses its experts between tokens, and find several regularities: i) some experts are reused between adjacent tokens and ii) the model hidden states of early layers already âknowâ which experts are to be used at subsequent layers. â¢ we design a MoE-specific offloading strategy that takes advantage of these regularities: i) it uses LRU cache to significantly reduces GPU-RAM communication, leading to faster generation and ii) it guesses which experts are needed ahead of time to better overlap expert loading with computation.\nâ¢ we consider the specific scenario of running Mixtral-8x7B-Instruct on a T4, RTX 3060 and RTX 3080 Mobile and develop a practical combination of mixed quantization and the proposed offloading algorithm to run this model interactively at 2-3 tokens per second depending on the hardware. The source code with our implementation is available online2",
            "# 2.4 Hardware Setup\nWhile our analysis is not specific to any hardware setup, we target the hardware specifications of cheap / free-tier cloud instances Google (2023) and the upper half of gaming computers Steam (2023): i) enough system memory to hold model parameters, ii) a GPU with 11-16GB VRAM and iii) host-to-device communication at 8-16GB/s (PCIe Gen.3). If we examine popular open-access MoE models (Mixtral-8x7B and switch-c-2048), we find that all non-experts can fit a fraction of available GPU memory. In turn, the experts that constitute vast majority of model parameters do not fit even with quantization. Finally, even if we could fit the model parameters in memory, running generative inference requires additional memory for layer activations and past attention keys & values.\n# 3 Method\nIn this work, we aim to systematically find the optimal way to inference modern Mixture-of-Experts LLMs on desktop or low-end cloud instances. More specifically, we focus on the task of generating tokens interactively, i.e. generate multiple tokens per second at batch size 15.",
            "# Denis Mazur Moscow Institute of Physics and Technology Yandex Researchcore denismazur8@gmail.com\n# Abstract\nWith the widespread adoption of Large Language Models (LLMs), many deep learning practitioners are looking for strategies of running these models more efficiently. One such strategy is to use sparse Mixture-of-Experts (MoE) â a type of model architectures where only a fraction of model layers are active for any given input. This property allows MoE-based language models to generate tokens faster than their âdenseâ counterparts, but it also increases model size due to having multiple âexpertsâ. Unfortunately, this makes state-of-the-art MoE language models difficult to run without high-end GPUs. In this work, we study the problem of running large MoE language models on consumer hardware with limited accelerator memory. We build upon parameter offloading algorithms and propose a novel strategy that accelerates offloading by taking advantage of innate properties of MoE LLMs. Using this strategy, we build can run Mixtral-8x7B with mixed quantization on desktop hardware and free-tier Google Colab instances.\n# Introduction",
            "To speed up offloading in practice, we allocate all expert parameters in a contiguous memory buffer that can be moved as a single host-to-device copy. For host-side (RAM) experts, we pin8 this memory buffer for faster communication. Our implementation additionally allocates b=4 on-device buffers used to copy and prefetch experts asynchronously, without modifying existing experts. These buffers are shared between all MoE layers to reduce memory footprint. Overall, the system requires num_layers Ã num_experts expert memory buffers split between host and device memory and b=4 temporary buffers, the size of each buffer being equal to a single expert.\n# 4 Experiments\nIn this section, we verify our earlier hypotheses about MoE behavior and benchmark the inference latency in different conditions. We focus our evaluations on Mixtral-8x7B and Mixtral-8x7B-Instruct models since they represent the current state of the art among open-access MoE models. We organize this section as follows: Section 4.1 measures the effectiveness of expert caching and pre-loading in isolation, Section 4.2 compares different model compression algorithms and verifies our hypotheses from Section 3.3. Finally, Section 4.3 measures the inference latency in several hardware setups.",
            "For this evaluation, we run Mixtral-8x7B-Instruct model on the OpenAssistant dataset (KÃ¶pf et al., 2023). We test LRU caching by running the model on recorded conversations and measuring the recall (aka âhit ratioâ from caching perspective) for different cache sizes k. Next, we test speculative loading in isolation by âguessingâ which experts should be loaded (by applying the next layerâs gating function on current layer activations), then measuring how often the actual next experts get loaded this way. A recall of 1.0 corresponds to a situation where both (2) Mixtral active experts were pre-fetched. We test speculative loading in three settings: 1, 2 and 10 layers ahead.\n# 4.2 Mixed MoE Quantization",
            "We consider four hardware configurations: a free-tier Colab instance with a T4 GPU (16GB VRAM, PCIe Gen.3), a past generation gaming laptop with RTX 3080 Mobile (16GB, PCIe Gen.4), a mid- range gaming desktop with RTX 3060 (12GB, PCIe Gen.3) and a high-end data-center server with A100-80GB-SXM. Note that the A100 server could run the model without offloading. We use offloading on A100 mostly to provide a reference for other setups. Finally, when evaluating 3-bit models, we use a cloud T4 from Microsoft Azure because the free-tier colab instances did not have enough RAM for this specific configuration. We use k = 2 for RTX 3060 and k = 4 for all other GPUs.\nAs shown in Table 2, all evaluated setups can generate 2-4 tokens per second with the full algorithm. Using pre-loading appears to be most beneficial on RTX 3060, possibly due to lower LRU cache size. Cursiously, RTX 3060 (desktop) performs nearly equally with a much higher end 3080 Mobile. We attribute this to the fact that both GPUs are still bottlenecked by host-to-device bandwidth, limited by the PCIe architecture. Finally, all schemes significantly outperform naive offloading that loads the entire MoE layer.\n# 5 Conclusion and Future Work",
            "# 4.2 Mixed MoE Quantization\nNext, we test how different Quantization schemes affect MoE performance and size. We also use Mixtral-8x7B, but this time, we use non-instruction-tuned variant since it fits better with the available benchmarks. We measure WikiText2 perpliexity Merity et al. (2016), C4 perplexity Raffel et al. (2020), as well as 5-shot MMLU accuracy Hendrycks et al. (2021). Our objective for this section is to find the best trade off between size and performance for offloading with the target setups. Note that out of 46.7B total parameters in the Mixtral-8x7B model, the experts constitute 45.1B (96.6%). The rest of the model parameters are allocated to embeddings, self-attention layers, MoE gates and minor layers such as LayerNorm.",
            "MoE quantization. As we described earlier in Section 2.2, there are multiple weight quantization algorithms optimized for LLMs. Model compression has a natural synergy with offloading because compressed models take less time to load onto GPU. In our experitments, we also observed that MoE models get better quality-size trade-offs when quantizing experts to a lower bitwidth, while keeping all non-expert layers at 4-bit.\nWe use Half Quadratic Quantization (HQQ) (Badri & Shaji, 2023) â a data-free quantization algorithm that supports a variety of bit rates. However, we chose this algorithm only for convenience, because it was already well tested for Mixtral models. Since our analysis does not rely on any specific choice of quantization, we believe that if we chose another quantization algorithm (e.g. GPTQ or AWQ) our conclusions would be similar. In our early experiments, we also tried the sub-1-bit quantization from QMoE Frantar & Alistarh (2023b) that worked well on the Switch-c-2048 model. However, we found that sub-1-bit compression caused too significant a loss in perplexity for Mixtral-8x7B models."
        ],
        "response": "Design considerations for inferencing MoE models on consumer hardware include using LRU caching and speculative loading to reduce host-device communication, and mixed quantization for size reduction. [http://arxiv.org/pdf/2312.17238] Implementation details involve prefetching experts by applying next layer's gating function to current hidden states, using contiguous memory buffers for efficient transfers, and pinning host-side memory for faster communication. [http://arxiv.org/pdf/2312.17238] Mixed MoE quantization quantizes experts to lower bitwidth while keeping non-experts at 4-bit to balance size and perplexity, with HQQ being a suitable algorithm. [http://arxiv.org/pdf/2312.17238] The system achieves 2-4 tokens per second on hardware like RTX 3060 and T4 GPUs by combining these techniques. [http://arxiv.org/pdf/2312.17238] Speculative loading recall is measured by guessing experts 1-10 layers ahead, with higher layers improving accuracy. [http://arxiv.org/pdf/2312.17238] Sub-1-bit quantization caused significant perplexity loss for Mixtral-8x7B, making it less viable. [http://arxiv.org/pdf/2312.17238] The approach enables interactive generation on low-end hardware by leveraging expert reuse and hidden state regularities. [http://arxiv.org/pdf/2312.17238] Offloading strategies are optimized for PCIe bandwidth limitations in consumer GPUs. [http://arxiv.org/pdf/2312.17238] Future work includes speculative expert prediction and further offloading optimizations. [http://arxiv.org/pdf/2312.17238]",
        "reference": "The design considerations for inferencing MoE language models on consumer and low-end cloud hardware include caching and prefetching techniques. The system uses an LRU cache to keep active experts in GPU memory as a cache for future tokens. The number of experts that can be stored in the cache is limited by the available GPU memory. The system chooses to always keep the k least recently used experts as a type of LRU cache. However, LRU caching does not consider factors like expert activation frequencies, varying cache size between MoE layers, or any sequential patterns in expert activation.\n\nThe implementation details include using a mixed MoE quantization scheme and the Half Quadratic Quantization (HQQ) algorithm. HQQ is a data-free quantization algorithm that supports a variety of bit rates. The system also employs expert offloading, where the next transformer layer is pre-loaded ahead of time while the previous layer is still running. However, this schedule is not possible for Mixture-of-Experts models, so the system tries to guess the likely next experts and load them speculatively while processing the previous layer. This is done by applying the next layer's gating function to the previous layer's hidden states to accurately guess the next layer's experts."
    },
    {
        "user_input": "What is the relationship between benchmarking the expert LRU cache and speculative loading, and the expert recall rate in the Mixtral-8x7B-Instruct model on the OpenAssistant dataset?",
        "retrieved_contexts": [
            "For this evaluation, we run Mixtral-8x7B-Instruct model on the OpenAssistant dataset (KÃ¶pf et al., 2023). We test LRU caching by running the model on recorded conversations and measuring the recall (aka âhit ratioâ from caching perspective) for different cache sizes k. Next, we test speculative loading in isolation by âguessingâ which experts should be loaded (by applying the next layerâs gating function on current layer activations), then measuring how often the actual next experts get loaded this way. A recall of 1.0 corresponds to a situation where both (2) Mixtral active experts were pre-fetched. We test speculative loading in three settings: 1, 2 and 10 layers ahead.\n# 4.2 Mixed MoE Quantization",
            "7Notably, Google Colab RAM cannot fit Mixtral-8x7B with a reasonable compression rate 8This corresponds to tensor.pin_memory() command in PyTorch.\n5\niy & cache_size =3 cache_size = 2 cache_size =4 0.84 | PIO â prefetch 1 experts ~ escent ae | PRS aa 0.2} ââ prefetch 2 experts ââ prefetch 3 experts 0.0 00 0 5 10 15 20 25 30 0 5 10 15 20 25 30 Layer # Layer # S Fd Ed Cache hit rate Bd ES Prediction recall = ES Ss &\nFigure 2: (left) LRU cache hit ratio for different cache size k; (right) speculative loading recall when pre-loading a different number of experts. Regular lines represent loading 1 layer ahead; dashed line stands for 2 layers ahead; dotted line is 10 layers ahead.\n# 4.1 Expert LRU Cache and Speculative Loading\nIn this section, we benchmark the effectiveness of the two expert offloading strategies: LRU caching and and speculative loading, as defined in Sections 3.1 and 3.2 respectively. For this evaluation, we measure âexpert recallâ â the fraction of times when an expert needed for inference was already available on GPU.",
            "Expert Offloading. As described earlier, we use LRU cache with an equal number k of cached experts per layer. For Mixtral-8x7B, we use k=2 for 12GB GPUs and k=4 for 16GB ones. We trigger speculative expert loading immediately after the system finished loading all experts for the current layer. The speculative expert loading fetches 1 â 2 most likely experts. The newly loaded experts do not replace the currently cached experts. If a speculatively loaded expert was later used during next layer inference, it will replace the least recently used expert from the next layerâs cache.\nMany consumer devices and free-tier cloud instances have limited host RAM that cannot fit the entire model7. In these cases, the experts must be split between host and device memory. To support this, our implementation of expert LRU cache splits experts between host and GPU devices. When loading and expert to the GPU cache, the system also offloads the least recently used on-device expert back to RAM so as to preserve memory parity.",
            "We illustrate an example of how LRU cache saves experts in Figure 1 (see caption). LRU is a very simple strategy that does not consider factors like expert activation frequencies, varying cache size between MoE layers, or any sequential patterns in expert activation. However, we found that even this simple strategy can significantly speed up inference for modern Mixture-of-Experts models such as Mixtral-8x7B (see Section 4 for detailed evaluation).\n# 3.2 Speculative Expert Loading\nWhile LRU caching can reduce the average expert loading time, most of the inference time is still spent waiting for the next expert to be loaded. The reason behind this is that, unlike with dense models, MoE offloading cannot effectively overlap expert loading with computation. To understand this problem, let us zoom into the process of generating a single token, layer-by-layer. The full compute workload starts by embedding the previous token via look-up, then alternates between running self-attention and MLP for each transformer block in the model. Finally, the outputs from the last transformer block are used to predict next token logits with a linear projection.",
            "Figure 1: An example of expert loading pattern in Mixtral-8x7B-Instruct for select layers. Blue cells indicate that a certain expert was active when encoding a certain token; deeper blue indicates higher gating weight. Small gray squares show which experts are cached with an LRU cache for k=2.\n# 3.1 Expert Locality and LRU caching\nAs we discussed earlier in Section 2.1, Mixture-of-Experts language models were often observed to assign individual experts to distinct sub-tasks. However, this does not mean that the model uses the same expert over long stretches of tokens. Instead, some experts are active in short sequences of 2-4 tokens, while others are often used with âgapsâ, as shown in Figure 1.\nTo take advantage of this pattern, we can keep active experts in GPU memory as a âcacheâ for future tokens. If the same experts are activated again in future, they will be available instantaneously. Naturally, the number of experts that can be stored this way if very limited by the available GPU memory. For simplicity, we choose to always keep k least recently used experts as a type of LRU cache. If k is greater than the number of active experts, the cache will save experts from multiple previous tokens. For simplicity, we keep the same number of cached experts for each MoE layer.",
            "To speed up offloading in practice, we allocate all expert parameters in a contiguous memory buffer that can be moved as a single host-to-device copy. For host-side (RAM) experts, we pin8 this memory buffer for faster communication. Our implementation additionally allocates b=4 on-device buffers used to copy and prefetch experts asynchronously, without modifying existing experts. These buffers are shared between all MoE layers to reduce memory footprint. Overall, the system requires num_layers Ã num_experts expert memory buffers split between host and device memory and b=4 temporary buffers, the size of each buffer being equal to a single expert.\n# 4 Experiments\nIn this section, we verify our earlier hypotheses about MoE behavior and benchmark the inference latency in different conditions. We focus our evaluations on Mixtral-8x7B and Mixtral-8x7B-Instruct models since they represent the current state of the art among open-access MoE models. We organize this section as follows: Section 4.1 measures the effectiveness of expert caching and pre-loading in isolation, Section 4.2 compares different model compression algorithms and verifies our hypotheses from Section 3.3. Finally, Section 4.3 measures the inference latency in several hardware setups.",
            "1When deployed in 16-bit precision, Falcon-180B needs approximately 360GB, while LLaMA-2 70B requires 140GB of combined accelerator memory.\nIn this work, we systematically develop techniques for running large MoE language models with limited GPU memory. Our main objective is inferencing (generating tokens) with Mixtral-8x7B- Instruct â a MoE-based chat assistant â on a desktop-grade hardware where only a fraction of experts fit into the accelerator memory. To that end:\nwe observe how MoE language model accesses its experts between tokens, and find several regularities: i) some experts are reused between adjacent tokens and ii) the model hidden states of early layers already âknowâ which experts are to be used at subsequent layers. â¢ we design a MoE-specific offloading strategy that takes advantage of these regularities: i) it uses LRU cache to significantly reduces GPU-RAM communication, leading to faster generation and ii) it guesses which experts are needed ahead of time to better overlap expert loading with computation.\nâ¢ we consider the specific scenario of running Mixtral-8x7B-Instruct on a T4, RTX 3060 and RTX 3080 Mobile and develop a practical combination of mixed quantization and the proposed offloading algorithm to run this model interactively at 2-3 tokens per second depending on the hardware. The source code with our implementation is available online2",
            "n-1 y= Ss Softmax(Top2(a - W,)); - SwiGLU;(a). i=0\nThis formulation is similar to the GShard architecture [21], with the exceptions that we replace all FFN sub-blocks by MoE layers while GShard replaces every other block, and that GShard uses a more elaborate gating strategy for the second expert assigned to each token.\n# 3 Results\nWe compare Mixtral to Llama, and re-run all benchmarks with our own evaluation pipeline for fair comparison. We measure performance on a wide variety of tasks categorized as follow:\nâ¢ Commonsense Reasoning (0-shot): Hellaswag [32], Winogrande [26], PIQA [3], SIQA [27], OpenbookQA [22], ARC-Easy, ARC-Challenge [8], CommonsenseQA [30]",
            "# 6 Conclusion\nIn this paper, we introduced Mixtral 8x7B, the first mixture-of-experts network to reach a state-of-the- art performance among open-source models. Mixtral 8x7B Instruct outperforms Claude-2.1, Gem- ini Pro, and GPT-3.5 Turbo on human evaluation benchmarks. Because it only uses two experts at each time step, Mixtral only uses 13B active parameters per token while outperforming the previous best model using 70B parameters per token (Llama 2 70B). We are making our trained and fine-tuned mod- els publicly available under the Apache 2.0 license. By sharing our models, we aim to facilitate the de- velopment of new techniques and applications that can benefit a wide range of industries and domains.",
            "Table 1: Perplexity and model size evaluation of Mixtral-8x7B with different quantization for shared attention (Attn quant) and experts (Experts quant) layers. For comprarison, a Mistral-7B 4-bit quantized model has Wiki2 perplexity 5.03, C4 perplexity 7.56 and MMLU score 61.3%. See Section 4.2 for details. Green values correspond to the configurations we chose for full system evaluation.\n6\nAlgorithm 2-bit Experts 3-bit Experts A100 3080 Mobile 3060 T4 (Colab) A100 3080 Mobile 3060 T4 (Cloud) 3.061 Full algorithm 2.918 W/o expert pre-loading 2.265 W/o LRU cache & pre-loading Naive offloading (accelerate) 1.392 2.655 2.227 1.758 1.059 2.278 2.051 1.547 0.919 2.092 1.567 1.168 0.661 2.845 2.683 2.055 1.246 2.475 2.024 1.595 0.914 2.038 1.857 1.346 1.791 1.603 1.365 1.061 0.580"
        ],
        "response": "Benchmarking expert LRU cache and speculative loading measures expert recall rates in Mixtral-8x7B-Instruct on OpenAssistant dataset [source: http://arxiv.org/pdf/2312.17238]. LRU caching with cache size k=2 or k=4 achieves higher expert recall rates [source: http://arxiv.org/pdf/2312.17238]. Speculative loading pre-fetches 1-2 most likely experts, with recall rates varying by layers ahead [source: http://arxiv.org/pdf/2312.17238]. Both strategies reduce expert loading latency by reusing or pre-loading experts [source: http://arxiv.org/pdf/2312.17238]. Figure 2 shows cache hit ratios and speculative loading recall for different configurations [source: http://arxiv.org/pdf/2312.17238]. Expert recall is the fraction of times needed experts are already on GPU [source: http://arxiv.org/pdf/2312.17238]. Higher recall rates correlate with faster inference for MoE models [source: http://arxiv.org/pdf/2312.17238]. The evaluation focuses on inference latency reduction for limited GPU memory setups [source: http://arxiv.org/pdf/2312.17238]. Both strategies are tested in isolation to measure their individual effectiveness [source: http://arxiv.org/pdf/2312.17238]. The study uses Mixtral-8x7B-Instruct due to its state-of-the-art performance in open-access MoE models [source: http://arxiv.org/pdf/2312.17238].",
        "reference": "The relationship between benchmarking the expert LRU cache and speculative loading and the expert recall rate in the Mixtral-8x7B-Instruct model on the OpenAssistant dataset is that they are both strategies being evaluated for their effectiveness in improving the expert recall rate."
    },
    {
        "user_input": "How does benchmarking expert offloading strategies impact the recall rate of experts on the GPU during inference?",
        "retrieved_contexts": [
            "7Notably, Google Colab RAM cannot fit Mixtral-8x7B with a reasonable compression rate 8This corresponds to tensor.pin_memory() command in PyTorch.\n5\niy & cache_size =3 cache_size = 2 cache_size =4 0.84 | PIO â prefetch 1 experts ~ escent ae | PRS aa 0.2} ââ prefetch 2 experts ââ prefetch 3 experts 0.0 00 0 5 10 15 20 25 30 0 5 10 15 20 25 30 Layer # Layer # S Fd Ed Cache hit rate Bd ES Prediction recall = ES Ss &\nFigure 2: (left) LRU cache hit ratio for different cache size k; (right) speculative loading recall when pre-loading a different number of experts. Regular lines represent loading 1 layer ahead; dashed line stands for 2 layers ahead; dotted line is 10 layers ahead.\n# 4.1 Expert LRU Cache and Speculative Loading\nIn this section, we benchmark the effectiveness of the two expert offloading strategies: LRU caching and and speculative loading, as defined in Sections 3.1 and 3.2 respectively. For this evaluation, we measure âexpert recallâ â the fraction of times when an expert needed for inference was already available on GPU.",
            "Expert Offloading. As described earlier, we use LRU cache with an equal number k of cached experts per layer. For Mixtral-8x7B, we use k=2 for 12GB GPUs and k=4 for 16GB ones. We trigger speculative expert loading immediately after the system finished loading all experts for the current layer. The speculative expert loading fetches 1 â 2 most likely experts. The newly loaded experts do not replace the currently cached experts. If a speculatively loaded expert was later used during next layer inference, it will replace the least recently used expert from the next layerâs cache.\nMany consumer devices and free-tier cloud instances have limited host RAM that cannot fit the entire model7. In these cases, the experts must be split between host and device memory. To support this, our implementation of expert LRU cache splits experts between host and GPU devices. When loading and expert to the GPU cache, the system also offloads the least recently used on-device expert back to RAM so as to preserve memory parity.",
            "# 5 Conclusion and Future Work\nIn this work, we explore strategies for accelerating Mixture-of-Experts based language models on consumer hardware with limited GPU memory. We propose a MoE-centric approach to offloading\n7\nand explore how mixed quantization affects perplexity and performance on language understanding tasks. We evaluate the proposed strategies and show that they produce a significant increase in generation speed compared to naÂ¨ve approaches on consumer-grade hardware, including free-tier Google Colab.\nOur method provides a practical solution for inferencing large MoE language models on resource- constricted hardware, enabling broader access to these powerful models for research and development. As future work, we plan to explore further offloading strategies, based on speculative expert predic- tion.\n# Acknowledgements\nAuthors would like to acknowledge mobicham@ for helpful discussions on Mixtral quantization.\n# References\nAminabadi, R. Y., Rajbhandari, S., Awan, A. A., Li, C., Li, D., Zheng, E., Ruwase, O., Smith, S., Zhang, M., Rasley, J., and He, Y. Deepspeed-inference: Enabling efficient inference of transformer models at unprecedented scale. In Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis, SC â22. IEEE Press, 2022. ISBN 9784665454445.",
            "To speed up offloading in practice, we allocate all expert parameters in a contiguous memory buffer that can be moved as a single host-to-device copy. For host-side (RAM) experts, we pin8 this memory buffer for faster communication. Our implementation additionally allocates b=4 on-device buffers used to copy and prefetch experts asynchronously, without modifying existing experts. These buffers are shared between all MoE layers to reduce memory footprint. Overall, the system requires num_layers Ã num_experts expert memory buffers split between host and device memory and b=4 temporary buffers, the size of each buffer being equal to a single expert.\n# 4 Experiments\nIn this section, we verify our earlier hypotheses about MoE behavior and benchmark the inference latency in different conditions. We focus our evaluations on Mixtral-8x7B and Mixtral-8x7B-Instruct models since they represent the current state of the art among open-access MoE models. We organize this section as follows: Section 4.1 measures the effectiveness of expert caching and pre-loading in isolation, Section 4.2 compares different model compression algorithms and verifies our hypotheses from Section 3.3. Finally, Section 4.3 measures the inference latency in several hardware setups.",
            "1When deployed in 16-bit precision, Falcon-180B needs approximately 360GB, while LLaMA-2 70B requires 140GB of combined accelerator memory.\nIn this work, we systematically develop techniques for running large MoE language models with limited GPU memory. Our main objective is inferencing (generating tokens) with Mixtral-8x7B- Instruct â a MoE-based chat assistant â on a desktop-grade hardware where only a fraction of experts fit into the accelerator memory. To that end:\nwe observe how MoE language model accesses its experts between tokens, and find several regularities: i) some experts are reused between adjacent tokens and ii) the model hidden states of early layers already âknowâ which experts are to be used at subsequent layers. â¢ we design a MoE-specific offloading strategy that takes advantage of these regularities: i) it uses LRU cache to significantly reduces GPU-RAM communication, leading to faster generation and ii) it guesses which experts are needed ahead of time to better overlap expert loading with computation.\nâ¢ we consider the specific scenario of running Mixtral-8x7B-Instruct on a T4, RTX 3060 and RTX 3080 Mobile and develop a practical combination of mixed quantization and the proposed offloading algorithm to run this model interactively at 2-3 tokens per second depending on the hardware. The source code with our implementation is available online2",
            "We illustrate an example of how LRU cache saves experts in Figure 1 (see caption). LRU is a very simple strategy that does not consider factors like expert activation frequencies, varying cache size between MoE layers, or any sequential patterns in expert activation. However, we found that even this simple strategy can significantly speed up inference for modern Mixture-of-Experts models such as Mixtral-8x7B (see Section 4 for detailed evaluation).\n# 3.2 Speculative Expert Loading\nWhile LRU caching can reduce the average expert loading time, most of the inference time is still spent waiting for the next expert to be loaded. The reason behind this is that, unlike with dense models, MoE offloading cannot effectively overlap expert loading with computation. To understand this problem, let us zoom into the process of generating a single token, layer-by-layer. The full compute workload starts by embedding the previous token via look-up, then alternates between running self-attention and MLP for each transformer block in the model. Finally, the outputs from the last transformer block are used to predict next token logits with a linear projection.",
            "# 2.4 Hardware Setup\nWhile our analysis is not specific to any hardware setup, we target the hardware specifications of cheap / free-tier cloud instances Google (2023) and the upper half of gaming computers Steam (2023): i) enough system memory to hold model parameters, ii) a GPU with 11-16GB VRAM and iii) host-to-device communication at 8-16GB/s (PCIe Gen.3). If we examine popular open-access MoE models (Mixtral-8x7B and switch-c-2048), we find that all non-experts can fit a fraction of available GPU memory. In turn, the experts that constitute vast majority of model parameters do not fit even with quantization. Finally, even if we could fit the model parameters in memory, running generative inference requires additional memory for layer activations and past attention keys & values.\n# 3 Method\nIn this work, we aim to systematically find the optimal way to inference modern Mixture-of-Experts LLMs on desktop or low-end cloud instances. More specifically, we focus on the task of generating tokens interactively, i.e. generate multiple tokens per second at batch size 15.",
            "# 4.3 Practical offloading performance\nFinally we evaluate the performance of the Mixtral8x7B-Instruct model using the offloading tech- niquesproposed throughout this report. Based on the perplexity evaluations from the previous section, we chose 4-bit HQQ quantization for the shared attention layers and 2- or 3-bit quantization for experts. We evaluate this system by generating tokens via sampling on OpenAssistant (KÃ¶pf et al., 2023) conversations and measuring the average number of tokens generated per second with batch size 1. For this evaluation, we always sample proportionally to the predicted probabilities, i.e. without temperature or nucleus sampling.",
            "MoE quantization. As we described earlier in Section 2.2, there are multiple weight quantization algorithms optimized for LLMs. Model compression has a natural synergy with offloading because compressed models take less time to load onto GPU. In our experitments, we also observed that MoE models get better quality-size trade-offs when quantizing experts to a lower bitwidth, while keeping all non-expert layers at 4-bit.\nWe use Half Quadratic Quantization (HQQ) (Badri & Shaji, 2023) â a data-free quantization algorithm that supports a variety of bit rates. However, we chose this algorithm only for convenience, because it was already well tested for Mixtral models. Since our analysis does not rely on any specific choice of quantization, we believe that if we chose another quantization algorithm (e.g. GPTQ or AWQ) our conclusions would be similar. In our early experiments, we also tried the sub-1-bit quantization from QMoE Frantar & Alistarh (2023b) that worked well on the Switch-c-2048 model. However, we found that sub-1-bit compression caused too significant a loss in perplexity for Mixtral-8x7B models.",
            "For this evaluation, we run Mixtral-8x7B-Instruct model on the OpenAssistant dataset (KÃ¶pf et al., 2023). We test LRU caching by running the model on recorded conversations and measuring the recall (aka âhit ratioâ from caching perspective) for different cache sizes k. Next, we test speculative loading in isolation by âguessingâ which experts should be loaded (by applying the next layerâs gating function on current layer activations), then measuring how often the actual next experts get loaded this way. A recall of 1.0 corresponds to a situation where both (2) Mixtral active experts were pre-fetched. We test speculative loading in three settings: 1, 2 and 10 layers ahead.\n# 4.2 Mixed MoE Quantization"
        ],
        "response": "Benchmarking expert offloading strategies impacts recall rates by reducing GPU-RAM communication through LRU caching and speculative loading. LRU caching with k=2 or 4 experts per layer significantly improves cache hit ratios, as shown in Figure 2. Speculative loading pre-fetches 1-2 most likely experts, increasing prediction recall when pre-loading 1-2 layers ahead. The system achieves higher expert recall by maintaining memory parity and using pinned host memory for faster transfers. These strategies enable faster inference on hardware with limited GPU memory, as demonstrated in Section 4.1. [source: http://arxiv.org/pdf/2312.17238]",
        "reference": "The benchmarking of expert offloading strategies impacts the recall rate of experts on the GPU during inference."
    },
    {
        "user_input": "What is the relationship between expert offloading strategies and the availability of experts on the GPU during inference?",
        "retrieved_contexts": [
            "7Notably, Google Colab RAM cannot fit Mixtral-8x7B with a reasonable compression rate 8This corresponds to tensor.pin_memory() command in PyTorch.\n5\niy & cache_size =3 cache_size = 2 cache_size =4 0.84 | PIO â prefetch 1 experts ~ escent ae | PRS aa 0.2} ââ prefetch 2 experts ââ prefetch 3 experts 0.0 00 0 5 10 15 20 25 30 0 5 10 15 20 25 30 Layer # Layer # S Fd Ed Cache hit rate Bd ES Prediction recall = ES Ss &\nFigure 2: (left) LRU cache hit ratio for different cache size k; (right) speculative loading recall when pre-loading a different number of experts. Regular lines represent loading 1 layer ahead; dashed line stands for 2 layers ahead; dotted line is 10 layers ahead.\n# 4.1 Expert LRU Cache and Speculative Loading\nIn this section, we benchmark the effectiveness of the two expert offloading strategies: LRU caching and and speculative loading, as defined in Sections 3.1 and 3.2 respectively. For this evaluation, we measure âexpert recallâ â the fraction of times when an expert needed for inference was already available on GPU.",
            "Expert Offloading. As described earlier, we use LRU cache with an equal number k of cached experts per layer. For Mixtral-8x7B, we use k=2 for 12GB GPUs and k=4 for 16GB ones. We trigger speculative expert loading immediately after the system finished loading all experts for the current layer. The speculative expert loading fetches 1 â 2 most likely experts. The newly loaded experts do not replace the currently cached experts. If a speculatively loaded expert was later used during next layer inference, it will replace the least recently used expert from the next layerâs cache.\nMany consumer devices and free-tier cloud instances have limited host RAM that cannot fit the entire model7. In these cases, the experts must be split between host and device memory. To support this, our implementation of expert LRU cache splits experts between host and GPU devices. When loading and expert to the GPU cache, the system also offloads the least recently used on-device expert back to RAM so as to preserve memory parity.",
            "# 5 Conclusion and Future Work\nIn this work, we explore strategies for accelerating Mixture-of-Experts based language models on consumer hardware with limited GPU memory. We propose a MoE-centric approach to offloading\n7\nand explore how mixed quantization affects perplexity and performance on language understanding tasks. We evaluate the proposed strategies and show that they produce a significant increase in generation speed compared to naÂ¨ve approaches on consumer-grade hardware, including free-tier Google Colab.\nOur method provides a practical solution for inferencing large MoE language models on resource- constricted hardware, enabling broader access to these powerful models for research and development. As future work, we plan to explore further offloading strategies, based on speculative expert predic- tion.\n# Acknowledgements\nAuthors would like to acknowledge mobicham@ for helpful discussions on Mixtral quantization.\n# References\nAminabadi, R. Y., Rajbhandari, S., Awan, A. A., Li, C., Li, D., Zheng, E., Ruwase, O., Smith, S., Zhang, M., Rasley, J., and He, Y. Deepspeed-inference: Enabling efficient inference of transformer models at unprecedented scale. In Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis, SC â22. IEEE Press, 2022. ISBN 9784665454445.",
            "To speed up offloading in practice, we allocate all expert parameters in a contiguous memory buffer that can be moved as a single host-to-device copy. For host-side (RAM) experts, we pin8 this memory buffer for faster communication. Our implementation additionally allocates b=4 on-device buffers used to copy and prefetch experts asynchronously, without modifying existing experts. These buffers are shared between all MoE layers to reduce memory footprint. Overall, the system requires num_layers Ã num_experts expert memory buffers split between host and device memory and b=4 temporary buffers, the size of each buffer being equal to a single expert.\n# 4 Experiments\nIn this section, we verify our earlier hypotheses about MoE behavior and benchmark the inference latency in different conditions. We focus our evaluations on Mixtral-8x7B and Mixtral-8x7B-Instruct models since they represent the current state of the art among open-access MoE models. We organize this section as follows: Section 4.1 measures the effectiveness of expert caching and pre-loading in isolation, Section 4.2 compares different model compression algorithms and verifies our hypotheses from Section 3.3. Finally, Section 4.3 measures the inference latency in several hardware setups.",
            "1When deployed in 16-bit precision, Falcon-180B needs approximately 360GB, while LLaMA-2 70B requires 140GB of combined accelerator memory.\nIn this work, we systematically develop techniques for running large MoE language models with limited GPU memory. Our main objective is inferencing (generating tokens) with Mixtral-8x7B- Instruct â a MoE-based chat assistant â on a desktop-grade hardware where only a fraction of experts fit into the accelerator memory. To that end:\nwe observe how MoE language model accesses its experts between tokens, and find several regularities: i) some experts are reused between adjacent tokens and ii) the model hidden states of early layers already âknowâ which experts are to be used at subsequent layers. â¢ we design a MoE-specific offloading strategy that takes advantage of these regularities: i) it uses LRU cache to significantly reduces GPU-RAM communication, leading to faster generation and ii) it guesses which experts are needed ahead of time to better overlap expert loading with computation.\nâ¢ we consider the specific scenario of running Mixtral-8x7B-Instruct on a T4, RTX 3060 and RTX 3080 Mobile and develop a practical combination of mixed quantization and the proposed offloading algorithm to run this model interactively at 2-3 tokens per second depending on the hardware. The source code with our implementation is available online2",
            "# 2.4 Hardware Setup\nWhile our analysis is not specific to any hardware setup, we target the hardware specifications of cheap / free-tier cloud instances Google (2023) and the upper half of gaming computers Steam (2023): i) enough system memory to hold model parameters, ii) a GPU with 11-16GB VRAM and iii) host-to-device communication at 8-16GB/s (PCIe Gen.3). If we examine popular open-access MoE models (Mixtral-8x7B and switch-c-2048), we find that all non-experts can fit a fraction of available GPU memory. In turn, the experts that constitute vast majority of model parameters do not fit even with quantization. Finally, even if we could fit the model parameters in memory, running generative inference requires additional memory for layer activations and past attention keys & values.\n# 3 Method\nIn this work, we aim to systematically find the optimal way to inference modern Mixture-of-Experts LLMs on desktop or low-end cloud instances. More specifically, we focus on the task of generating tokens interactively, i.e. generate multiple tokens per second at batch size 15.",
            "We illustrate an example of how LRU cache saves experts in Figure 1 (see caption). LRU is a very simple strategy that does not consider factors like expert activation frequencies, varying cache size between MoE layers, or any sequential patterns in expert activation. However, we found that even this simple strategy can significantly speed up inference for modern Mixture-of-Experts models such as Mixtral-8x7B (see Section 4 for detailed evaluation).\n# 3.2 Speculative Expert Loading\nWhile LRU caching can reduce the average expert loading time, most of the inference time is still spent waiting for the next expert to be loaded. The reason behind this is that, unlike with dense models, MoE offloading cannot effectively overlap expert loading with computation. To understand this problem, let us zoom into the process of generating a single token, layer-by-layer. The full compute workload starts by embedding the previous token via look-up, then alternates between running self-attention and MLP for each transformer block in the model. Finally, the outputs from the last transformer block are used to predict next token logits with a linear projection.",
            "Figure 1: An example of expert loading pattern in Mixtral-8x7B-Instruct for select layers. Blue cells indicate that a certain expert was active when encoding a certain token; deeper blue indicates higher gating weight. Small gray squares show which experts are cached with an LRU cache for k=2.\n# 3.1 Expert Locality and LRU caching\nAs we discussed earlier in Section 2.1, Mixture-of-Experts language models were often observed to assign individual experts to distinct sub-tasks. However, this does not mean that the model uses the same expert over long stretches of tokens. Instead, some experts are active in short sequences of 2-4 tokens, while others are often used with âgapsâ, as shown in Figure 1.\nTo take advantage of this pattern, we can keep active experts in GPU memory as a âcacheâ for future tokens. If the same experts are activated again in future, they will be available instantaneously. Naturally, the number of experts that can be stored this way if very limited by the available GPU memory. For simplicity, we choose to always keep k least recently used experts as a type of LRU cache. If k is greater than the number of active experts, the cache will save experts from multiple previous tokens. For simplicity, we keep the same number of cached experts for each MoE layer.",
            "This technique works particularly well when processing large batches of data, during train- ing Pudipeddi et al. (2020); Ren et al. (2021) or large-batch non-interactive inference Aminabadi et al. (2022); Sheng et al. (2023), where each layer processes a lot of tokens each time the layer is loaded from RAM. In turn, when doing interactive inference (e.g. as a chat assistants), offloading works significantly slower than on-device inference. This is because interactive inference generates tokens autoregressively, from left to right. This way, the inference system processes one or few tokens at a time, and therefore spends most of the time waiting for next layerâs parameters to be loaded.\n# 2.4 Hardware Setup",
            "Since then, several lines of work explore Mixture-of-Experts with Transformer-based language models for machine translation Lepikhin et al. (2020), masked language modeling Fedus et al. (2021), general-purpose LLMs Du et al. (2022) and others. Most of these models follow traditional (dense) Transformer architecture for embeddings and attention layers, and only use Mixture for the feedforward (MLP) blocks and use a linear token-level gating function. A common observation across most of these works is that MoE models are cheaper to train and inference Fedus et al. (2021); Lepikhin et al. (2020), but require more parameters than a dense model with equivalent perplexity. Pre-trained Mixture-of-Experts LLMs have been openly available for over a year3. However, these models seem to have gained less traction than equivalent dense models, arguable because their sheer model size (over a trillion parameters) makes them difficult to use. Most recently, Mistral AI released a family of sparse Mixture of Experts models called Mixtral-8x7B with near state-of-the-art performance Mixtral AI team (2023). This model has already inspired several follow-up works and practical applications, but it still requires a high-end GPU accelerator.\n# 2.2 Post-training Quantization of LLMs"
        ],
        "response": "Expert offloading strategies like LRU caching and speculative loading directly impact the availability of experts on the GPU during inference by reducing the need for repeated host-to-device transfers. LRU caching retains recently used experts in GPU memory, increasing the cache hit rate and reducing latency [source: http://arxiv.org/pdf/2312.17238]. Speculative loading proactively fetches likely experts for the next layer, overlapping computation with data transfer and improving recall [source: http://arxiv.org/pdf/2312.17238]. These strategies are particularly effective for interactive inference on memory-constrained hardware, where they enable faster token generation by minimizing idle time waiting for expert loading [source: http://arxiv.org/pdf/2312.17238]. The balance between cache size and speculative depth optimizes GPU memory usage while maintaining high expert availability [source: http://arxiv.org/pdf/2312.17238].",
        "reference": "The relationship between expert offloading strategies and the availability of experts on the GPU during inference is that the offloading strategies, specifically LRU caching and speculative loading, aim to ensure that the necessary experts are already available on the GPU when needed for inference."
    },
    {
        "user_input": "Which model has better performance than Llama 2 70B on most benchmarks with 5x fewer active parameters during inference?",
        "retrieved_contexts": [
            "Table 2: Comparison of Mixtral with Llama. Mixtral outperforms or matches Llama 2 70B performance on almost all popular benchmarks while using 5x fewer active parameters during inference.\n70 Mixtral 8x7B. âMixtral 8x7B Mixtral 8x7B 355 =o = Es & E60! Mistral 78 % 2681 Mistral 78 3 3 s0 5 = A % 66 50 g 4 45 64 78 138 348708 78 138 348708 78 138 348 70B S66 Mixtral 8x7B 50 Mixtral 8x7B 5 = 564 340 g al Mistral 78 ee Mistral 78 3 5 Â§ 30 5 eo â= Mistral Â° 20 âe LlaMA2 78 (138 348 70B 7B (138 348 708 7B Â«13B 34B 708 Active Params Active Params Active Params\nFigure 3: Results on MMLU, commonsense reasoning, world knowledge and reading comprehension, math and code for Mistral (7B/8x7B) vs Llama 2 (7B/13B/70B). Mixtral largely outperforms Llama 2 70B on all benchmarks, except on reading comprehension benchmarks while using 5x lower active parameters. It is also vastly superior to Llama 2 70B on code and math.",
            "Detailed results for Mixtral, Mistral 7B and Llama 2 7B/13B/70B and Llama 1 34B2 are reported in Table 2. Figure 2 compares the performance of Mixtral with the Llama models in different categories. Mixtral surpasses Llama 2 70B across most metrics. In particular, Mixtral displays a superior performance in code and mathematics benchmarks.\nSize and Efficiency. We compare our performance to the Llama 2 family, aiming to understand Mixtral modelsâ efficiency in the cost-performance spectrum (see Figure 3). As a sparse Mixture- of-Experts model, Mixtral only uses 13B active parameters for each token. With 5x lower active parameters, Mixtral is able to outperform Llama 2 70B across most categories.\nNote that this analysis focuses on the active parameter count (see Section 2.1), which is directly proportional to the inference compute cost, but does not consider the memory costs and hardware utilization. The memory costs for serving Mixtral are proportional to its sparse parameter count, 47B, which is still smaller than Llama 2 70B. As for device utilization, we note that the SMoEs layer introduces additional overhead due to the routing mechanism and due to the increased memory loads when running more than one expert per device. They are more suitable for batched workloads where one can reach a good degree of arithmetic intensity.",
            "Comparison with Llama 2 70B and GPT-3.5. In Table 3, we report the performance of Mixtral 8x7B compared to Llama 2 70B and GPT-3.5. We observe that Mixtral performs similarly or above the two other models. On MMLU, Mixtral obtains a better performance, despite its significantly smaller capacity (47B tokens compared to 70B). For MT Bench, we report the performance of the latest GPT-3.5-Turbo model available, gpt-3.5-turbo-1106.\n2Since Llama 2 34B was not open-sourced, we report results for Llama 1 34B.\n4",
            "2Since Llama 2 34B was not open-sourced, we report results for Llama 1 34B.\n4\nLLaMA 2 70B GPT-3.5 MMLU (MCQ in 57 subjects) 69.9% 70.0% 70.6% HellaSwag (10-shot) 87.1% 85.5% 86.7% ARC Challenge (25-shot) 85.1% 85.2% 85.8% WinoGrande (5-shot) 83.2% 81.6% 81.2% MBPP (pass@1) 49.8% 52.2% 60.7% GSM-8K (5-shot) 53.6% 57.1% 58.4% MT Bench (for Instruct Models) 6.86 8.32 8.30\n# Mixtral 8x7B\nTable 3: Comparison of Mixtral with Llama 2 70B and GPT-3.5. Mixtral outperforms or matches Llama 2 70B and GPT-3.5 performance on most metrics.\nEvaluation Differences. On some benchmarks, there are some differences between our evaluation protocol and the one reported in the Llama 2 paper: 1) on MBPP, we use the hand-verified subset 2) on TriviaQA, we do not provide Wikipedia contexts.\n# 3.1 Multilingual benchmarks",
            "Abstract\nWe introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine- tuned to follow instructions, Mixtral 8x7B â Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â chat model on human bench- marks. Both the base and instruct models are released under the Apache 2.0 license.\nCode: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/mixtral-of-experts/",
            "# 3.1 Multilingual benchmarks\nCompared to Mistral 7B, we significantly upsample the proportion of multilingual data during pretraining. The extra capacity allows Mixtral to perform well on multilingual benchmarks while maintaining a high accuracy in English. In particular, Mixtral significantly outperforms Llama 2 70B in French, German, Spanish, and Italian, as shown in Table 4.\nActive Params French Arc-c HellaS MMLU German Arc-c HellaS MMLU Spanish Arc-c HellaS MMLU Italian Arc-c HellaS MMLU 33B 70B 13B 42.9% 65.4% 49.0% 39.3% 68.1% 49.9% 49.9% 72.5% 64.3% 49.4% 70.9% 65.1% 58.2% 77.4% 70.9% 54.3% 73.0% 71.5% 55.4% 77.6% 72.5% 52.8% 75.1% 70.9% 41.1% 63.3% 48.7% 47.3% 68.7% 64.2% 45.7% 69.8% 52.3% 50.5% 74.5% 66.0%",
            "# 6 Conclusion\nIn this paper, we introduced Mixtral 8x7B, the first mixture-of-experts network to reach a state-of-the- art performance among open-source models. Mixtral 8x7B Instruct outperforms Claude-2.1, Gem- ini Pro, and GPT-3.5 Turbo on human evaluation benchmarks. Because it only uses two experts at each time step, Mixtral only uses 13B active parameters per token while outperforming the previous best model using 70B parameters per token (Llama 2 70B). We are making our trained and fine-tuned mod- els publicly available under the Apache 2.0 license. By sharing our models, we aim to facilitate the de- velopment of new techniques and applications that can benefit a wide range of industries and domains.",
            "Code: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/mixtral-of-experts/\n# Introduction\nIn this paper, we present Mixtral 8x7B, a sparse mixture of experts model (SMoE) with open weights, licensed under Apache 2.0. Mixtral outperforms Llama 2 70B and GPT-3.5 on most benchmarks. As it only uses a subset of its parameters for every token, Mixtral allows faster inference speed at low batch-sizes, and higher throughput at large batch-sizes.\nMixtral is a sparse mixture-of-experts network. It is a decoder-only model where the feedforward block picks from a set of 8 distinct groups of parameters. At every layer, for every token, a router network chooses two of these groups (the âexpertsâ) to process the token and combine their output additively. This technique increases the number of parameters of a model while controlling cost and latency, as the model only uses a fraction of the total set of parameters per token.\nMixtral is pretrained with multilingual data using a context size of 32k tokens. It either matches or exceeds the performance of Llama 2 70B and GPT-3.5, over several benchmarks. In particular,\nMixture of Experts Layer i gating inputs af outputs router expert",
            "Llama 2 70B Mixtral 8x7B BBQ accuracy 51.5% 56.0% BOLD sentiment score (avg Â± std) gender profession religious_ideology political_ideology race 0.293 Â± 0.073 0.218 Â± 0.073 0.188 Â± 0.133 0.149 Â± 0.140 0.232 Â± 0.049 0.323 Â±0.045 0.243 Â± 0.087 0.144 Â± 0.089 0.186 Â± 0.146 0.232 Â± 0.052\nFigure 5: Bias Benchmarks. Compared Llama 2 70B, Mixtral presents less bias (higher accuracy on BBQ, lower std on BOLD) and displays more positive sentiment (higher avg on BOLD).\nWe benchmark Llama 2 and Mixtral on BBQ and BOLD with our evaluation framework and report the results in Table 5. Compared to Llama 2, Mixtral presents less bias on the BBQ benchmark (56.0% vs 51.5%). For each group in BOLD, a higher average sentiment score means more positive sentiments and a lower standard deviation indicates less bias within the group. Overall, Mixtral displays more positive sentiments than Llama 2, with similar variances within each group.\n# Instruction Fine-tuning",
            "We also present Mixtral 8x7B â Instruct, a chat model fine-tuned to follow instructions using supervised fine-tuning and Direct Preference Optimization [25]. Its performance notably surpasses that of GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â chat model on human evaluation benchmarks. Mixtral â Instruct also demonstrates reduced biases, and a more balanced sentiment profile in benchmarks such as BBQ, and BOLD. We release both Mixtral 8x7B and Mixtral 8x7B â Instruct under the Apache 2.0 license1, free for academic and commercial usage, ensuring broad accessibility and potential for diverse applications. To enable the community to run Mixtral with a fully open-source stack, we submitted changes to the vLLM project, which integrates Megablocks CUDA kernels for efficient inference. Skypilot also allows the deployment of vLLM endpoints on any instance in the cloud.\n# 2 Architectural details\nMixtral is based on a transformer architecture [31] and uses the same modifications as described in [18], with the notable exceptions that Mix- tral supports a fully dense context length of 32k tokens, and the feed- forward blocks are replaced by Mixture-of-Expert layers (Section 2.1). The model architecture parameters are summarized in Table 1. Parameter Value"
        ],
        "response": "Mixtral 8x7B outperforms Llama 2 70B on most benchmarks using 5x fewer active parameters during inference. [http://arxiv.org/pdf/2401.04088]",
        "reference": "The model that has better performance than Llama 2 70B on most benchmarks with 5x fewer active parameters during inference is Mixtral."
    },
    {
        "user_input": "What are the differences in architecture between Mixtral 8x7B and Mistral 7B in terms of feedforward blocks and active parameters used during inference?",
        "retrieved_contexts": [
            "Abstract\nWe introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine- tuned to follow instructions, Mixtral 8x7B â Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â chat model on human bench- marks. Both the base and instruct models are released under the Apache 2.0 license.\nCode: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/mixtral-of-experts/",
            "Table 2: Comparison of Mixtral with Llama. Mixtral outperforms or matches Llama 2 70B performance on almost all popular benchmarks while using 5x fewer active parameters during inference.\n70 Mixtral 8x7B. âMixtral 8x7B Mixtral 8x7B 355 =o = Es & E60! Mistral 78 % 2681 Mistral 78 3 3 s0 5 = A % 66 50 g 4 45 64 78 138 348708 78 138 348708 78 138 348 70B S66 Mixtral 8x7B 50 Mixtral 8x7B 5 = 564 340 g al Mistral 78 ee Mistral 78 3 5 Â§ 30 5 eo â= Mistral Â° 20 âe LlaMA2 78 (138 348 70B 7B (138 348 708 7B Â«13B 34B 708 Active Params Active Params Active Params\nFigure 3: Results on MMLU, commonsense reasoning, world knowledge and reading comprehension, math and code for Mistral (7B/8x7B) vs Llama 2 (7B/13B/70B). Mixtral largely outperforms Llama 2 70B on all benchmarks, except on reading comprehension benchmarks while using 5x lower active parameters. It is also vastly superior to Llama 2 70B on code and math.",
            "We also present Mixtral 8x7B â Instruct, a chat model fine-tuned to follow instructions using supervised fine-tuning and Direct Preference Optimization [25]. Its performance notably surpasses that of GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â chat model on human evaluation benchmarks. Mixtral â Instruct also demonstrates reduced biases, and a more balanced sentiment profile in benchmarks such as BBQ, and BOLD. We release both Mixtral 8x7B and Mixtral 8x7B â Instruct under the Apache 2.0 license1, free for academic and commercial usage, ensuring broad accessibility and potential for diverse applications. To enable the community to run Mixtral with a fully open-source stack, we submitted changes to the vLLM project, which integrates Megablocks CUDA kernels for efficient inference. Skypilot also allows the deployment of vLLM endpoints on any instance in the cloud.\n# 2 Architectural details\nMixtral is based on a transformer architecture [31] and uses the same modifications as described in [18], with the notable exceptions that Mix- tral supports a fully dense context length of 32k tokens, and the feed- forward blocks are replaced by Mixture-of-Expert layers (Section 2.1). The model architecture parameters are summarized in Table 1. Parameter Value",
            "Detailed results for Mixtral, Mistral 7B and Llama 2 7B/13B/70B and Llama 1 34B2 are reported in Table 2. Figure 2 compares the performance of Mixtral with the Llama models in different categories. Mixtral surpasses Llama 2 70B across most metrics. In particular, Mixtral displays a superior performance in code and mathematics benchmarks.\nSize and Efficiency. We compare our performance to the Llama 2 family, aiming to understand Mixtral modelsâ efficiency in the cost-performance spectrum (see Figure 3). As a sparse Mixture- of-Experts model, Mixtral only uses 13B active parameters for each token. With 5x lower active parameters, Mixtral is able to outperform Llama 2 70B across most categories.\nNote that this analysis focuses on the active parameter count (see Section 2.1), which is directly proportional to the inference compute cost, but does not consider the memory costs and hardware utilization. The memory costs for serving Mixtral are proportional to its sparse parameter count, 47B, which is still smaller than Llama 2 70B. As for device utilization, we note that the SMoEs layer introduces additional overhead due to the routing mechanism and due to the increased memory loads when running more than one expert per device. They are more suitable for batched workloads where one can reach a good degree of arithmetic intensity.",
            "Code: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/mixtral-of-experts/\n# Introduction\nIn this paper, we present Mixtral 8x7B, a sparse mixture of experts model (SMoE) with open weights, licensed under Apache 2.0. Mixtral outperforms Llama 2 70B and GPT-3.5 on most benchmarks. As it only uses a subset of its parameters for every token, Mixtral allows faster inference speed at low batch-sizes, and higher throughput at large batch-sizes.\nMixtral is a sparse mixture-of-experts network. It is a decoder-only model where the feedforward block picks from a set of 8 distinct groups of parameters. At every layer, for every token, a router network chooses two of these groups (the âexpertsâ) to process the token and combine their output additively. This technique increases the number of parameters of a model while controlling cost and latency, as the model only uses a fraction of the total set of parameters per token.\nMixtral is pretrained with multilingual data using a context size of 32k tokens. It either matches or exceeds the performance of Llama 2 70B and GPT-3.5, over several benchmarks. In particular,\nMixture of Experts Layer i gating inputs af outputs router expert",
            "Since then, several lines of work explore Mixture-of-Experts with Transformer-based language models for machine translation Lepikhin et al. (2020), masked language modeling Fedus et al. (2021), general-purpose LLMs Du et al. (2022) and others. Most of these models follow traditional (dense) Transformer architecture for embeddings and attention layers, and only use Mixture for the feedforward (MLP) blocks and use a linear token-level gating function. A common observation across most of these works is that MoE models are cheaper to train and inference Fedus et al. (2021); Lepikhin et al. (2020), but require more parameters than a dense model with equivalent perplexity. Pre-trained Mixture-of-Experts LLMs have been openly available for over a year3. However, these models seem to have gained less traction than equivalent dense models, arguable because their sheer model size (over a trillion parameters) makes them difficult to use. Most recently, Mistral AI released a family of sparse Mixture of Experts models called Mixtral-8x7B with near state-of-the-art performance Mixtral AI team (2023). This model has already inspired several follow-up works and practical applications, but it still requires a high-end GPU accelerator.\n# 2.2 Post-training Quantization of LLMs",
            "# 6 Conclusion\nIn this paper, we introduced Mixtral 8x7B, the first mixture-of-experts network to reach a state-of-the- art performance among open-source models. Mixtral 8x7B Instruct outperforms Claude-2.1, Gem- ini Pro, and GPT-3.5 Turbo on human evaluation benchmarks. Because it only uses two experts at each time step, Mixtral only uses 13B active parameters per token while outperforming the previous best model using 70B parameters per token (Llama 2 70B). We are making our trained and fine-tuned mod- els publicly available under the Apache 2.0 license. By sharing our models, we aim to facilitate the de- velopment of new techniques and applications that can benefit a wide range of industries and domains.",
            "# 3.1 Multilingual benchmarks\nCompared to Mistral 7B, we significantly upsample the proportion of multilingual data during pretraining. The extra capacity allows Mixtral to perform well on multilingual benchmarks while maintaining a high accuracy in English. In particular, Mixtral significantly outperforms Llama 2 70B in French, German, Spanish, and Italian, as shown in Table 4.\nActive Params French Arc-c HellaS MMLU German Arc-c HellaS MMLU Spanish Arc-c HellaS MMLU Italian Arc-c HellaS MMLU 33B 70B 13B 42.9% 65.4% 49.0% 39.3% 68.1% 49.9% 49.9% 72.5% 64.3% 49.4% 70.9% 65.1% 58.2% 77.4% 70.9% 54.3% 73.0% 71.5% 55.4% 77.6% 72.5% 52.8% 75.1% 70.9% 41.1% 63.3% 48.7% 47.3% 68.7% 64.2% 45.7% 69.8% 52.3% 50.5% 74.5% 66.0%",
            "We illustrate an example of how LRU cache saves experts in Figure 1 (see caption). LRU is a very simple strategy that does not consider factors like expert activation frequencies, varying cache size between MoE layers, or any sequential patterns in expert activation. However, we found that even this simple strategy can significantly speed up inference for modern Mixture-of-Experts models such as Mixtral-8x7B (see Section 4 for detailed evaluation).\n# 3.2 Speculative Expert Loading\nWhile LRU caching can reduce the average expert loading time, most of the inference time is still spent waiting for the next expert to be loaded. The reason behind this is that, unlike with dense models, MoE offloading cannot effectively overlap expert loading with computation. To understand this problem, let us zoom into the process of generating a single token, layer-by-layer. The full compute workload starts by embedding the previous token via look-up, then alternates between running self-attention and MLP for each transformer block in the model. Finally, the outputs from the last transformer block are used to predict next token logits with a linear projection.",
            "Comparison with Llama 2 70B and GPT-3.5. In Table 3, we report the performance of Mixtral 8x7B compared to Llama 2 70B and GPT-3.5. We observe that Mixtral performs similarly or above the two other models. On MMLU, Mixtral obtains a better performance, despite its significantly smaller capacity (47B tokens compared to 70B). For MT Bench, we report the performance of the latest GPT-3.5-Turbo model available, gpt-3.5-turbo-1106.\n2Since Llama 2 34B was not open-sourced, we report results for Llama 1 34B.\n4"
        ],
        "response": "Mixtral 8x7B uses Sparse Mixture of Experts (SMoE) architecture, replacing traditional feedforward blocks with 8 expert groups per layer, while Mistral 7B uses dense feedforward blocks. Each token in Mixtral routes to two experts per layer, using 13B active parameters during inference, while Mistral 7B uses all parameters. Mixtral has 47B total parameters but only 13B are active per token, unlike Mistral 7B's fully utilized parameters [http://arxiv.org/pdf/2401.04088].",
        "reference": "The difference in architecture between Mixtral 8x7B and Mistral 7B lies in the number of feedforward blocks (experts) in each layer. Mixtral 8x7B has 8 feedforward blocks in each layer, while Mistral 7B has a different number of feedforward blocks. Additionally, during inference, Mixtral 8x7B uses 13B active parameters out of the total 47B parameters available for each token."
    },
    {
        "user_input": "How does Mixtral's performance in code and math benchmarks compare to Llama 2 70B?",
        "retrieved_contexts": [
            "Table 2: Comparison of Mixtral with Llama. Mixtral outperforms or matches Llama 2 70B performance on almost all popular benchmarks while using 5x fewer active parameters during inference.\n70 Mixtral 8x7B. âMixtral 8x7B Mixtral 8x7B 355 =o = Es & E60! Mistral 78 % 2681 Mistral 78 3 3 s0 5 = A % 66 50 g 4 45 64 78 138 348708 78 138 348708 78 138 348 70B S66 Mixtral 8x7B 50 Mixtral 8x7B 5 = 564 340 g al Mistral 78 ee Mistral 78 3 5 Â§ 30 5 eo â= Mistral Â° 20 âe LlaMA2 78 (138 348 70B 7B (138 348 708 7B Â«13B 34B 708 Active Params Active Params Active Params\nFigure 3: Results on MMLU, commonsense reasoning, world knowledge and reading comprehension, math and code for Mistral (7B/8x7B) vs Llama 2 (7B/13B/70B). Mixtral largely outperforms Llama 2 70B on all benchmarks, except on reading comprehension benchmarks while using 5x lower active parameters. It is also vastly superior to Llama 2 70B on code and math.",
            "Detailed results for Mixtral, Mistral 7B and Llama 2 7B/13B/70B and Llama 1 34B2 are reported in Table 2. Figure 2 compares the performance of Mixtral with the Llama models in different categories. Mixtral surpasses Llama 2 70B across most metrics. In particular, Mixtral displays a superior performance in code and mathematics benchmarks.\nSize and Efficiency. We compare our performance to the Llama 2 family, aiming to understand Mixtral modelsâ efficiency in the cost-performance spectrum (see Figure 3). As a sparse Mixture- of-Experts model, Mixtral only uses 13B active parameters for each token. With 5x lower active parameters, Mixtral is able to outperform Llama 2 70B across most categories.\nNote that this analysis focuses on the active parameter count (see Section 2.1), which is directly proportional to the inference compute cost, but does not consider the memory costs and hardware utilization. The memory costs for serving Mixtral are proportional to its sparse parameter count, 47B, which is still smaller than Llama 2 70B. As for device utilization, we note that the SMoEs layer introduces additional overhead due to the routing mechanism and due to the increased memory loads when running more than one expert per device. They are more suitable for batched workloads where one can reach a good degree of arithmetic intensity.",
            "Comparison with Llama 2 70B and GPT-3.5. In Table 3, we report the performance of Mixtral 8x7B compared to Llama 2 70B and GPT-3.5. We observe that Mixtral performs similarly or above the two other models. On MMLU, Mixtral obtains a better performance, despite its significantly smaller capacity (47B tokens compared to 70B). For MT Bench, we report the performance of the latest GPT-3.5-Turbo model available, gpt-3.5-turbo-1106.\n2Since Llama 2 34B was not open-sourced, we report results for Llama 1 34B.\n4",
            "Abstract\nWe introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine- tuned to follow instructions, Mixtral 8x7B â Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â chat model on human bench- marks. Both the base and instruct models are released under the Apache 2.0 license.\nCode: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/mixtral-of-experts/",
            "2Since Llama 2 34B was not open-sourced, we report results for Llama 1 34B.\n4\nLLaMA 2 70B GPT-3.5 MMLU (MCQ in 57 subjects) 69.9% 70.0% 70.6% HellaSwag (10-shot) 87.1% 85.5% 86.7% ARC Challenge (25-shot) 85.1% 85.2% 85.8% WinoGrande (5-shot) 83.2% 81.6% 81.2% MBPP (pass@1) 49.8% 52.2% 60.7% GSM-8K (5-shot) 53.6% 57.1% 58.4% MT Bench (for Instruct Models) 6.86 8.32 8.30\n# Mixtral 8x7B\nTable 3: Comparison of Mixtral with Llama 2 70B and GPT-3.5. Mixtral outperforms or matches Llama 2 70B and GPT-3.5 performance on most metrics.\nEvaluation Differences. On some benchmarks, there are some differences between our evaluation protocol and the one reported in the Llama 2 paper: 1) on MBPP, we use the hand-verified subset 2) on TriviaQA, we do not provide Wikipedia contexts.\n# 3.1 Multilingual benchmarks",
            "Llama 2 70B Mixtral 8x7B BBQ accuracy 51.5% 56.0% BOLD sentiment score (avg Â± std) gender profession religious_ideology political_ideology race 0.293 Â± 0.073 0.218 Â± 0.073 0.188 Â± 0.133 0.149 Â± 0.140 0.232 Â± 0.049 0.323 Â±0.045 0.243 Â± 0.087 0.144 Â± 0.089 0.186 Â± 0.146 0.232 Â± 0.052\nFigure 5: Bias Benchmarks. Compared Llama 2 70B, Mixtral presents less bias (higher accuracy on BBQ, lower std on BOLD) and displays more positive sentiment (higher avg on BOLD).\nWe benchmark Llama 2 and Mixtral on BBQ and BOLD with our evaluation framework and report the results in Table 5. Compared to Llama 2, Mixtral presents less bias on the BBQ benchmark (56.0% vs 51.5%). For each group in BOLD, a higher average sentiment score means more positive sentiments and a lower standard deviation indicates less bias within the group. Overall, Mixtral displays more positive sentiments than Llama 2, with similar variances within each group.\n# Instruction Fine-tuning",
            "# 3.1 Multilingual benchmarks\nCompared to Mistral 7B, we significantly upsample the proportion of multilingual data during pretraining. The extra capacity allows Mixtral to perform well on multilingual benchmarks while maintaining a high accuracy in English. In particular, Mixtral significantly outperforms Llama 2 70B in French, German, Spanish, and Italian, as shown in Table 4.\nActive Params French Arc-c HellaS MMLU German Arc-c HellaS MMLU Spanish Arc-c HellaS MMLU Italian Arc-c HellaS MMLU 33B 70B 13B 42.9% 65.4% 49.0% 39.3% 68.1% 49.9% 49.9% 72.5% 64.3% 49.4% 70.9% 65.1% 58.2% 77.4% 70.9% 54.3% 73.0% 71.5% 55.4% 77.6% 72.5% 52.8% 75.1% 70.9% 41.1% 63.3% 48.7% 47.3% 68.7% 64.2% 45.7% 69.8% 52.3% 50.5% 74.5% 66.0%",
            "Code: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/mixtral-of-experts/\n# Introduction\nIn this paper, we present Mixtral 8x7B, a sparse mixture of experts model (SMoE) with open weights, licensed under Apache 2.0. Mixtral outperforms Llama 2 70B and GPT-3.5 on most benchmarks. As it only uses a subset of its parameters for every token, Mixtral allows faster inference speed at low batch-sizes, and higher throughput at large batch-sizes.\nMixtral is a sparse mixture-of-experts network. It is a decoder-only model where the feedforward block picks from a set of 8 distinct groups of parameters. At every layer, for every token, a router network chooses two of these groups (the âexpertsâ) to process the token and combine their output additively. This technique increases the number of parameters of a model while controlling cost and latency, as the model only uses a fraction of the total set of parameters per token.\nMixtral is pretrained with multilingual data using a context size of 32k tokens. It either matches or exceeds the performance of Llama 2 70B and GPT-3.5, over several benchmarks. In particular,\nMixture of Experts Layer i gating inputs af outputs router expert",
            "# 6 Conclusion\nIn this paper, we introduced Mixtral 8x7B, the first mixture-of-experts network to reach a state-of-the- art performance among open-source models. Mixtral 8x7B Instruct outperforms Claude-2.1, Gem- ini Pro, and GPT-3.5 Turbo on human evaluation benchmarks. Because it only uses two experts at each time step, Mixtral only uses 13B active parameters per token while outperforming the previous best model using 70B parameters per token (Llama 2 70B). We are making our trained and fine-tuned mod- els publicly available under the Apache 2.0 license. By sharing our models, we aim to facilitate the de- velopment of new techniques and applications that can benefit a wide range of industries and domains.",
            "World Knowledge (5-shot): NaturalQuestions [20], TriviaQA [19] â¢ Reading Comprehension (0-shot): BoolQ [7], QuAC [5] â¢ Math: GSM8K [9] (8-shot) with maj@8 and MATH [17] (4-shot) with maj@4 â¢ Code: Humaneval [4] (0-shot) and MBPP [1] (3-shot) â¢ Popular aggregated results: MMLU [16] (5-shot), BBH [29] (3-shot), and AGI Eval [34]\n(3-5-shot, English multiple-choice questions only)\n80 SE Mistral 78 = LLaMA27B = Sl LLaMA134B, jam Mistral 78 = LlaMA27B Ss LLAMA 1348, cee Mixtral 8x78 Sm LLaMA213BÂ° mmm LLaMA2 70B je Mixtral 8x78 mm LlaMA2138 lm LLaMA2 708 70 50 60 50 20 40 10 BH Code MMU Knowledge Reasoning âComprehension AGI Eval Math âAccuracy (%)"
        ],
        "response": "Mixtral outperforms or matches Llama 2 70B on most benchmarks including code and math, using 5x fewer active parameters [source: http://arxiv.org/pdf/2401.04088]. In code benchmarks, Mixtral surpasses Llama 2 70B significantly [source: http://arxiv.org/pdf/2401.04088]. For math, Mixtral also demonstrates superior performance over Llama 2 70B [source: http://arxiv.org/pdf/2401.04088]. The gap in performance is particularly notable in mathematics and code generation tasks [source: http://arxiv.org/pdf/2401.04088]. This efficiency is due to Mixtral's sparse Mixture-of-Experts architecture, which only activates 13B parameters per token [source: http://arxiv.org/pdf/2401.04088]. Despite its smaller active parameter count, Mixtral achieves higher accuracy in code and math evaluations [source: http://arxiv.org/pdf/2401.04088]. The model's design allows it to outperform larger models like Llama 2 70B in these specific domains [source: http://arxiv.org/pdf/2401.04088]. In multilingual benchmarks, Mixtral also shows better performance than Llama 2 70B [source: http://arxiv.org/pdf/2401.04088]. The results are consistent across various evaluation protocols [source: http://arxiv.org/pdf/2401.04088].",
        "reference": "Mixtral's performance in code and math benchmarks is superior to Llama 2 70B."
    },
    {
        "user_input": "How does benchmarking the expert LRU cache and speculative loading affect the effectiveness of the Mixtral-8x7B-Instruct model on the OpenAssistant dataset?",
        "retrieved_contexts": [
            "For this evaluation, we run Mixtral-8x7B-Instruct model on the OpenAssistant dataset (KÃ¶pf et al., 2023). We test LRU caching by running the model on recorded conversations and measuring the recall (aka âhit ratioâ from caching perspective) for different cache sizes k. Next, we test speculative loading in isolation by âguessingâ which experts should be loaded (by applying the next layerâs gating function on current layer activations), then measuring how often the actual next experts get loaded this way. A recall of 1.0 corresponds to a situation where both (2) Mixtral active experts were pre-fetched. We test speculative loading in three settings: 1, 2 and 10 layers ahead.\n# 4.2 Mixed MoE Quantization",
            "7Notably, Google Colab RAM cannot fit Mixtral-8x7B with a reasonable compression rate 8This corresponds to tensor.pin_memory() command in PyTorch.\n5\niy & cache_size =3 cache_size = 2 cache_size =4 0.84 | PIO â prefetch 1 experts ~ escent ae | PRS aa 0.2} ââ prefetch 2 experts ââ prefetch 3 experts 0.0 00 0 5 10 15 20 25 30 0 5 10 15 20 25 30 Layer # Layer # S Fd Ed Cache hit rate Bd ES Prediction recall = ES Ss &\nFigure 2: (left) LRU cache hit ratio for different cache size k; (right) speculative loading recall when pre-loading a different number of experts. Regular lines represent loading 1 layer ahead; dashed line stands for 2 layers ahead; dotted line is 10 layers ahead.\n# 4.1 Expert LRU Cache and Speculative Loading\nIn this section, we benchmark the effectiveness of the two expert offloading strategies: LRU caching and and speculative loading, as defined in Sections 3.1 and 3.2 respectively. For this evaluation, we measure âexpert recallâ â the fraction of times when an expert needed for inference was already available on GPU.",
            "We illustrate an example of how LRU cache saves experts in Figure 1 (see caption). LRU is a very simple strategy that does not consider factors like expert activation frequencies, varying cache size between MoE layers, or any sequential patterns in expert activation. However, we found that even this simple strategy can significantly speed up inference for modern Mixture-of-Experts models such as Mixtral-8x7B (see Section 4 for detailed evaluation).\n# 3.2 Speculative Expert Loading\nWhile LRU caching can reduce the average expert loading time, most of the inference time is still spent waiting for the next expert to be loaded. The reason behind this is that, unlike with dense models, MoE offloading cannot effectively overlap expert loading with computation. To understand this problem, let us zoom into the process of generating a single token, layer-by-layer. The full compute workload starts by embedding the previous token via look-up, then alternates between running self-attention and MLP for each transformer block in the model. Finally, the outputs from the last transformer block are used to predict next token logits with a linear projection.",
            "Expert Offloading. As described earlier, we use LRU cache with an equal number k of cached experts per layer. For Mixtral-8x7B, we use k=2 for 12GB GPUs and k=4 for 16GB ones. We trigger speculative expert loading immediately after the system finished loading all experts for the current layer. The speculative expert loading fetches 1 â 2 most likely experts. The newly loaded experts do not replace the currently cached experts. If a speculatively loaded expert was later used during next layer inference, it will replace the least recently used expert from the next layerâs cache.\nMany consumer devices and free-tier cloud instances have limited host RAM that cannot fit the entire model7. In these cases, the experts must be split between host and device memory. To support this, our implementation of expert LRU cache splits experts between host and GPU devices. When loading and expert to the GPU cache, the system also offloads the least recently used on-device expert back to RAM so as to preserve memory parity.",
            "Figure 1: An example of expert loading pattern in Mixtral-8x7B-Instruct for select layers. Blue cells indicate that a certain expert was active when encoding a certain token; deeper blue indicates higher gating weight. Small gray squares show which experts are cached with an LRU cache for k=2.\n# 3.1 Expert Locality and LRU caching\nAs we discussed earlier in Section 2.1, Mixture-of-Experts language models were often observed to assign individual experts to distinct sub-tasks. However, this does not mean that the model uses the same expert over long stretches of tokens. Instead, some experts are active in short sequences of 2-4 tokens, while others are often used with âgapsâ, as shown in Figure 1.\nTo take advantage of this pattern, we can keep active experts in GPU memory as a âcacheâ for future tokens. If the same experts are activated again in future, they will be available instantaneously. Naturally, the number of experts that can be stored this way if very limited by the available GPU memory. For simplicity, we choose to always keep k least recently used experts as a type of LRU cache. If k is greater than the number of active experts, the cache will save experts from multiple previous tokens. For simplicity, we keep the same number of cached experts for each MoE layer.",
            "1When deployed in 16-bit precision, Falcon-180B needs approximately 360GB, while LLaMA-2 70B requires 140GB of combined accelerator memory.\nIn this work, we systematically develop techniques for running large MoE language models with limited GPU memory. Our main objective is inferencing (generating tokens) with Mixtral-8x7B- Instruct â a MoE-based chat assistant â on a desktop-grade hardware where only a fraction of experts fit into the accelerator memory. To that end:\nwe observe how MoE language model accesses its experts between tokens, and find several regularities: i) some experts are reused between adjacent tokens and ii) the model hidden states of early layers already âknowâ which experts are to be used at subsequent layers. â¢ we design a MoE-specific offloading strategy that takes advantage of these regularities: i) it uses LRU cache to significantly reduces GPU-RAM communication, leading to faster generation and ii) it guesses which experts are needed ahead of time to better overlap expert loading with computation.\nâ¢ we consider the specific scenario of running Mixtral-8x7B-Instruct on a T4, RTX 3060 and RTX 3080 Mobile and develop a practical combination of mixed quantization and the proposed offloading algorithm to run this model interactively at 2-3 tokens per second depending on the hardware. The source code with our implementation is available online2",
            "To speed up offloading in practice, we allocate all expert parameters in a contiguous memory buffer that can be moved as a single host-to-device copy. For host-side (RAM) experts, we pin8 this memory buffer for faster communication. Our implementation additionally allocates b=4 on-device buffers used to copy and prefetch experts asynchronously, without modifying existing experts. These buffers are shared between all MoE layers to reduce memory footprint. Overall, the system requires num_layers Ã num_experts expert memory buffers split between host and device memory and b=4 temporary buffers, the size of each buffer being equal to a single expert.\n# 4 Experiments\nIn this section, we verify our earlier hypotheses about MoE behavior and benchmark the inference latency in different conditions. We focus our evaluations on Mixtral-8x7B and Mixtral-8x7B-Instruct models since they represent the current state of the art among open-access MoE models. We organize this section as follows: Section 4.1 measures the effectiveness of expert caching and pre-loading in isolation, Section 4.2 compares different model compression algorithms and verifies our hypotheses from Section 3.3. Finally, Section 4.3 measures the inference latency in several hardware setups.",
            "# 6 Conclusion\nIn this paper, we introduced Mixtral 8x7B, the first mixture-of-experts network to reach a state-of-the- art performance among open-source models. Mixtral 8x7B Instruct outperforms Claude-2.1, Gem- ini Pro, and GPT-3.5 Turbo on human evaluation benchmarks. Because it only uses two experts at each time step, Mixtral only uses 13B active parameters per token while outperforming the previous best model using 70B parameters per token (Llama 2 70B). We are making our trained and fine-tuned mod- els publicly available under the Apache 2.0 license. By sharing our models, we aim to facilitate the de- velopment of new techniques and applications that can benefit a wide range of industries and domains.",
            "# 4.3 Practical offloading performance\nFinally we evaluate the performance of the Mixtral8x7B-Instruct model using the offloading tech- niquesproposed throughout this report. Based on the perplexity evaluations from the previous section, we chose 4-bit HQQ quantization for the shared attention layers and 2- or 3-bit quantization for experts. We evaluate this system by generating tokens via sampling on OpenAssistant (KÃ¶pf et al., 2023) conversations and measuring the average number of tokens generated per second with batch size 1. For this evaluation, we always sample proportionally to the predicted probabilities, i.e. without temperature or nucleus sampling.",
            "n-1 y= Ss Softmax(Top2(a - W,)); - SwiGLU;(a). i=0\nThis formulation is similar to the GShard architecture [21], with the exceptions that we replace all FFN sub-blocks by MoE layers while GShard replaces every other block, and that GShard uses a more elaborate gating strategy for the second expert assigned to each token.\n# 3 Results\nWe compare Mixtral to Llama, and re-run all benchmarks with our own evaluation pipeline for fair comparison. We measure performance on a wide variety of tasks categorized as follow:\nâ¢ Commonsense Reasoning (0-shot): Hellaswag [32], Winogrande [26], PIQA [3], SIQA [27], OpenbookQA [22], ARC-Easy, ARC-Challenge [8], CommonsenseQA [30]"
        ],
        "response": "Benchmarking LRU caching and speculative loading on Mixtral-8x7B-Instruct shows LRU cache hit ratio increases with cache size, reaching up to 0.84 for cache size 4. Speculative loading recall varies by layers ahead, with 1 layer ahead achieving higher recall than 2 or 10 layers. These techniques reduce GPU-RAM communication and overlap expert loading with computation, improving inference speed on limited hardware. [http://arxiv.org/pdf/2312.17238]",
        "reference": "Benchmarking the expert LRU cache and speculative loading affects the effectiveness of the Mixtral-8x7B-Instruct model on the OpenAssistant dataset by reducing latency if the speculative loading guess is accurate enough. The system combines caching and prefetching techniques and uses the Half Quadratic Quantization (HQQ) algorithm for quantization. Speculative expert loading is triggered after loading all experts for the current layer and fetches the 1-2 most likely experts. The implementation of expert LRU cache splits experts between host and GPU devices. The benchmarking process evaluates the effectiveness of these two expert offloading strategies."
    },
    {
        "user_input": "Can you explain the relationship between the expert recall rate and the effectiveness of the Mixtral-8x7B-Instruct model on the OpenAssistant dataset?",
        "retrieved_contexts": [
            "For this evaluation, we run Mixtral-8x7B-Instruct model on the OpenAssistant dataset (KÃ¶pf et al., 2023). We test LRU caching by running the model on recorded conversations and measuring the recall (aka âhit ratioâ from caching perspective) for different cache sizes k. Next, we test speculative loading in isolation by âguessingâ which experts should be loaded (by applying the next layerâs gating function on current layer activations), then measuring how often the actual next experts get loaded this way. A recall of 1.0 corresponds to a situation where both (2) Mixtral active experts were pre-fetched. We test speculative loading in three settings: 1, 2 and 10 layers ahead.\n# 4.2 Mixed MoE Quantization",
            "7Notably, Google Colab RAM cannot fit Mixtral-8x7B with a reasonable compression rate 8This corresponds to tensor.pin_memory() command in PyTorch.\n5\niy & cache_size =3 cache_size = 2 cache_size =4 0.84 | PIO â prefetch 1 experts ~ escent ae | PRS aa 0.2} ââ prefetch 2 experts ââ prefetch 3 experts 0.0 00 0 5 10 15 20 25 30 0 5 10 15 20 25 30 Layer # Layer # S Fd Ed Cache hit rate Bd ES Prediction recall = ES Ss &\nFigure 2: (left) LRU cache hit ratio for different cache size k; (right) speculative loading recall when pre-loading a different number of experts. Regular lines represent loading 1 layer ahead; dashed line stands for 2 layers ahead; dotted line is 10 layers ahead.\n# 4.1 Expert LRU Cache and Speculative Loading\nIn this section, we benchmark the effectiveness of the two expert offloading strategies: LRU caching and and speculative loading, as defined in Sections 3.1 and 3.2 respectively. For this evaluation, we measure âexpert recallâ â the fraction of times when an expert needed for inference was already available on GPU.",
            "# 6 Conclusion\nIn this paper, we introduced Mixtral 8x7B, the first mixture-of-experts network to reach a state-of-the- art performance among open-source models. Mixtral 8x7B Instruct outperforms Claude-2.1, Gem- ini Pro, and GPT-3.5 Turbo on human evaluation benchmarks. Because it only uses two experts at each time step, Mixtral only uses 13B active parameters per token while outperforming the previous best model using 70B parameters per token (Llama 2 70B). We are making our trained and fine-tuned mod- els publicly available under the Apache 2.0 license. By sharing our models, we aim to facilitate the de- velopment of new techniques and applications that can benefit a wide range of industries and domains.",
            "Abstract\nWe introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine- tuned to follow instructions, Mixtral 8x7B â Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â chat model on human bench- marks. Both the base and instruct models are released under the Apache 2.0 license.\nCode: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/mixtral-of-experts/",
            "To speed up offloading in practice, we allocate all expert parameters in a contiguous memory buffer that can be moved as a single host-to-device copy. For host-side (RAM) experts, we pin8 this memory buffer for faster communication. Our implementation additionally allocates b=4 on-device buffers used to copy and prefetch experts asynchronously, without modifying existing experts. These buffers are shared between all MoE layers to reduce memory footprint. Overall, the system requires num_layers Ã num_experts expert memory buffers split between host and device memory and b=4 temporary buffers, the size of each buffer being equal to a single expert.\n# 4 Experiments\nIn this section, we verify our earlier hypotheses about MoE behavior and benchmark the inference latency in different conditions. We focus our evaluations on Mixtral-8x7B and Mixtral-8x7B-Instruct models since they represent the current state of the art among open-access MoE models. We organize this section as follows: Section 4.1 measures the effectiveness of expert caching and pre-loading in isolation, Section 4.2 compares different model compression algorithms and verifies our hypotheses from Section 3.3. Finally, Section 4.3 measures the inference latency in several hardware setups.",
            "We also present Mixtral 8x7B â Instruct, a chat model fine-tuned to follow instructions using supervised fine-tuning and Direct Preference Optimization [25]. Its performance notably surpasses that of GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â chat model on human evaluation benchmarks. Mixtral â Instruct also demonstrates reduced biases, and a more balanced sentiment profile in benchmarks such as BBQ, and BOLD. We release both Mixtral 8x7B and Mixtral 8x7B â Instruct under the Apache 2.0 license1, free for academic and commercial usage, ensuring broad accessibility and potential for diverse applications. To enable the community to run Mixtral with a fully open-source stack, we submitted changes to the vLLM project, which integrates Megablocks CUDA kernels for efficient inference. Skypilot also allows the deployment of vLLM endpoints on any instance in the cloud.\n# 2 Architectural details\nMixtral is based on a transformer architecture [31] and uses the same modifications as described in [18], with the notable exceptions that Mix- tral supports a fully dense context length of 32k tokens, and the feed- forward blocks are replaced by Mixture-of-Expert layers (Section 2.1). The model architecture parameters are summarized in Table 1. Parameter Value",
            "# 4.3 Practical offloading performance\nFinally we evaluate the performance of the Mixtral8x7B-Instruct model using the offloading tech- niquesproposed throughout this report. Based on the perplexity evaluations from the previous section, we chose 4-bit HQQ quantization for the shared attention layers and 2- or 3-bit quantization for experts. We evaluate this system by generating tokens via sampling on OpenAssistant (KÃ¶pf et al., 2023) conversations and measuring the average number of tokens generated per second with batch size 1. For this evaluation, we always sample proportionally to the predicted probabilities, i.e. without temperature or nucleus sampling.",
            "n-1 y= Ss Softmax(Top2(a - W,)); - SwiGLU;(a). i=0\nThis formulation is similar to the GShard architecture [21], with the exceptions that we replace all FFN sub-blocks by MoE layers while GShard replaces every other block, and that GShard uses a more elaborate gating strategy for the second expert assigned to each token.\n# 3 Results\nWe compare Mixtral to Llama, and re-run all benchmarks with our own evaluation pipeline for fair comparison. We measure performance on a wide variety of tasks categorized as follow:\nâ¢ Commonsense Reasoning (0-shot): Hellaswag [32], Winogrande [26], PIQA [3], SIQA [27], OpenbookQA [22], ARC-Easy, ARC-Challenge [8], CommonsenseQA [30]",
            "1When deployed in 16-bit precision, Falcon-180B needs approximately 360GB, while LLaMA-2 70B requires 140GB of combined accelerator memory.\nIn this work, we systematically develop techniques for running large MoE language models with limited GPU memory. Our main objective is inferencing (generating tokens) with Mixtral-8x7B- Instruct â a MoE-based chat assistant â on a desktop-grade hardware where only a fraction of experts fit into the accelerator memory. To that end:\nwe observe how MoE language model accesses its experts between tokens, and find several regularities: i) some experts are reused between adjacent tokens and ii) the model hidden states of early layers already âknowâ which experts are to be used at subsequent layers. â¢ we design a MoE-specific offloading strategy that takes advantage of these regularities: i) it uses LRU cache to significantly reduces GPU-RAM communication, leading to faster generation and ii) it guesses which experts are needed ahead of time to better overlap expert loading with computation.\nâ¢ we consider the specific scenario of running Mixtral-8x7B-Instruct on a T4, RTX 3060 and RTX 3080 Mobile and develop a practical combination of mixed quantization and the proposed offloading algorithm to run this model interactively at 2-3 tokens per second depending on the hardware. The source code with our implementation is available online2",
            "# 4.2 Mixed MoE Quantization\nNext, we test how different Quantization schemes affect MoE performance and size. We also use Mixtral-8x7B, but this time, we use non-instruction-tuned variant since it fits better with the available benchmarks. We measure WikiText2 perpliexity Merity et al. (2016), C4 perplexity Raffel et al. (2020), as well as 5-shot MMLU accuracy Hendrycks et al. (2021). Our objective for this section is to find the best trade off between size and performance for offloading with the target setups. Note that out of 46.7B total parameters in the Mixtral-8x7B model, the experts constitute 45.1B (96.6%). The rest of the model parameters are allocated to embeddings, self-attention layers, MoE gates and minor layers such as LayerNorm."
        ],
        "response": "The expert recall rate directly impacts the effectiveness of Mixtral-8x7B-Instruct on the OpenAssistant dataset by measuring how often required experts are pre-loaded into GPU memory, reducing latency and improving throughput. Higher recall rates correlate with faster token generation and better system performance, as seen in LRU caching and speculative loading experiments. For instance, a recall of 1.0 indicates all necessary experts were pre-fetched, minimizing offloading delays [source: http://arxiv.org/pdf/2312.17238]. The model's sparse activation of two experts per token allows efficient memory usage, but recall optimization is critical for real-time applications. Quantization techniques further enhance this by compressing expert parameters while maintaining performance. The study demonstrates that speculative loading up to 10 layers ahead can improve recall, though diminishing returns occur at higher layers. Overall, expert recall is a key metric for balancing model size and inference speed in MoE architectures [source: http://arxiv.org/pdf/2312.17238].",
        "reference": "The given context does not provide any information about the relationship between the expert recall rate and the effectiveness of the Mixtral-8x7B-Instruct model on the OpenAssistant dataset."
    },
    {
        "user_input": "What is the purpose of LRU caching in Mixture-of-Experts language models?",
        "retrieved_contexts": [
            "Figure 1: An example of expert loading pattern in Mixtral-8x7B-Instruct for select layers. Blue cells indicate that a certain expert was active when encoding a certain token; deeper blue indicates higher gating weight. Small gray squares show which experts are cached with an LRU cache for k=2.\n# 3.1 Expert Locality and LRU caching\nAs we discussed earlier in Section 2.1, Mixture-of-Experts language models were often observed to assign individual experts to distinct sub-tasks. However, this does not mean that the model uses the same expert over long stretches of tokens. Instead, some experts are active in short sequences of 2-4 tokens, while others are often used with âgapsâ, as shown in Figure 1.\nTo take advantage of this pattern, we can keep active experts in GPU memory as a âcacheâ for future tokens. If the same experts are activated again in future, they will be available instantaneously. Naturally, the number of experts that can be stored this way if very limited by the available GPU memory. For simplicity, we choose to always keep k least recently used experts as a type of LRU cache. If k is greater than the number of active experts, the cache will save experts from multiple previous tokens. For simplicity, we keep the same number of cached experts for each MoE layer.",
            "We illustrate an example of how LRU cache saves experts in Figure 1 (see caption). LRU is a very simple strategy that does not consider factors like expert activation frequencies, varying cache size between MoE layers, or any sequential patterns in expert activation. However, we found that even this simple strategy can significantly speed up inference for modern Mixture-of-Experts models such as Mixtral-8x7B (see Section 4 for detailed evaluation).\n# 3.2 Speculative Expert Loading\nWhile LRU caching can reduce the average expert loading time, most of the inference time is still spent waiting for the next expert to be loaded. The reason behind this is that, unlike with dense models, MoE offloading cannot effectively overlap expert loading with computation. To understand this problem, let us zoom into the process of generating a single token, layer-by-layer. The full compute workload starts by embedding the previous token via look-up, then alternates between running self-attention and MLP for each transformer block in the model. Finally, the outputs from the last transformer block are used to predict next token logits with a linear projection.",
            "For this evaluation, we run Mixtral-8x7B-Instruct model on the OpenAssistant dataset (KÃ¶pf et al., 2023). We test LRU caching by running the model on recorded conversations and measuring the recall (aka âhit ratioâ from caching perspective) for different cache sizes k. Next, we test speculative loading in isolation by âguessingâ which experts should be loaded (by applying the next layerâs gating function on current layer activations), then measuring how often the actual next experts get loaded this way. A recall of 1.0 corresponds to a situation where both (2) Mixtral active experts were pre-fetched. We test speculative loading in three settings: 1, 2 and 10 layers ahead.\n# 4.2 Mixed MoE Quantization",
            "7Notably, Google Colab RAM cannot fit Mixtral-8x7B with a reasonable compression rate 8This corresponds to tensor.pin_memory() command in PyTorch.\n5\niy & cache_size =3 cache_size = 2 cache_size =4 0.84 | PIO â prefetch 1 experts ~ escent ae | PRS aa 0.2} ââ prefetch 2 experts ââ prefetch 3 experts 0.0 00 0 5 10 15 20 25 30 0 5 10 15 20 25 30 Layer # Layer # S Fd Ed Cache hit rate Bd ES Prediction recall = ES Ss &\nFigure 2: (left) LRU cache hit ratio for different cache size k; (right) speculative loading recall when pre-loading a different number of experts. Regular lines represent loading 1 layer ahead; dashed line stands for 2 layers ahead; dotted line is 10 layers ahead.\n# 4.1 Expert LRU Cache and Speculative Loading\nIn this section, we benchmark the effectiveness of the two expert offloading strategies: LRU caching and and speculative loading, as defined in Sections 3.1 and 3.2 respectively. For this evaluation, we measure âexpert recallâ â the fraction of times when an expert needed for inference was already available on GPU.",
            "Expert Offloading. As described earlier, we use LRU cache with an equal number k of cached experts per layer. For Mixtral-8x7B, we use k=2 for 12GB GPUs and k=4 for 16GB ones. We trigger speculative expert loading immediately after the system finished loading all experts for the current layer. The speculative expert loading fetches 1 â 2 most likely experts. The newly loaded experts do not replace the currently cached experts. If a speculatively loaded expert was later used during next layer inference, it will replace the least recently used expert from the next layerâs cache.\nMany consumer devices and free-tier cloud instances have limited host RAM that cannot fit the entire model7. In these cases, the experts must be split between host and device memory. To support this, our implementation of expert LRU cache splits experts between host and GPU devices. When loading and expert to the GPU cache, the system also offloads the least recently used on-device expert back to RAM so as to preserve memory parity.",
            "Since then, several lines of work explore Mixture-of-Experts with Transformer-based language models for machine translation Lepikhin et al. (2020), masked language modeling Fedus et al. (2021), general-purpose LLMs Du et al. (2022) and others. Most of these models follow traditional (dense) Transformer architecture for embeddings and attention layers, and only use Mixture for the feedforward (MLP) blocks and use a linear token-level gating function. A common observation across most of these works is that MoE models are cheaper to train and inference Fedus et al. (2021); Lepikhin et al. (2020), but require more parameters than a dense model with equivalent perplexity. Pre-trained Mixture-of-Experts LLMs have been openly available for over a year3. However, these models seem to have gained less traction than equivalent dense models, arguable because their sheer model size (over a trillion parameters) makes them difficult to use. Most recently, Mistral AI released a family of sparse Mixture of Experts models called Mixtral-8x7B with near state-of-the-art performance Mixtral AI team (2023). This model has already inspired several follow-up works and practical applications, but it still requires a high-end GPU accelerator.\n# 2.2 Post-training Quantization of LLMs",
            "1When deployed in 16-bit precision, Falcon-180B needs approximately 360GB, while LLaMA-2 70B requires 140GB of combined accelerator memory.\nIn this work, we systematically develop techniques for running large MoE language models with limited GPU memory. Our main objective is inferencing (generating tokens) with Mixtral-8x7B- Instruct â a MoE-based chat assistant â on a desktop-grade hardware where only a fraction of experts fit into the accelerator memory. To that end:\nwe observe how MoE language model accesses its experts between tokens, and find several regularities: i) some experts are reused between adjacent tokens and ii) the model hidden states of early layers already âknowâ which experts are to be used at subsequent layers. â¢ we design a MoE-specific offloading strategy that takes advantage of these regularities: i) it uses LRU cache to significantly reduces GPU-RAM communication, leading to faster generation and ii) it guesses which experts are needed ahead of time to better overlap expert loading with computation.\nâ¢ we consider the specific scenario of running Mixtral-8x7B-Instruct on a T4, RTX 3060 and RTX 3080 Mobile and develop a practical combination of mixed quantization and the proposed offloading algorithm to run this model interactively at 2-3 tokens per second depending on the hardware. The source code with our implementation is available online2",
            "# 5 Conclusion and Future Work\nIn this work, we explore strategies for accelerating Mixture-of-Experts based language models on consumer hardware with limited GPU memory. We propose a MoE-centric approach to offloading\n7\nand explore how mixed quantization affects perplexity and performance on language understanding tasks. We evaluate the proposed strategies and show that they produce a significant increase in generation speed compared to naÂ¨ve approaches on consumer-grade hardware, including free-tier Google Colab.\nOur method provides a practical solution for inferencing large MoE language models on resource- constricted hardware, enabling broader access to these powerful models for research and development. As future work, we plan to explore further offloading strategies, based on speculative expert predic- tion.\n# Acknowledgements\nAuthors would like to acknowledge mobicham@ for helpful discussions on Mixtral quantization.\n# References\nAminabadi, R. Y., Rajbhandari, S., Awan, A. A., Li, C., Li, D., Zheng, E., Ruwase, O., Smith, S., Zhang, M., Rasley, J., and He, Y. Deepspeed-inference: Enabling efficient inference of transformer models at unprecedented scale. In Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis, SC â22. IEEE Press, 2022. ISBN 9784665454445.",
            "# Denis Mazur Moscow Institute of Physics and Technology Yandex Researchcore denismazur8@gmail.com\n# Abstract\nWith the widespread adoption of Large Language Models (LLMs), many deep learning practitioners are looking for strategies of running these models more efficiently. One such strategy is to use sparse Mixture-of-Experts (MoE) â a type of model architectures where only a fraction of model layers are active for any given input. This property allows MoE-based language models to generate tokens faster than their âdenseâ counterparts, but it also increases model size due to having multiple âexpertsâ. Unfortunately, this makes state-of-the-art MoE language models difficult to run without high-end GPUs. In this work, we study the problem of running large MoE language models on consumer hardware with limited accelerator memory. We build upon parameter offloading algorithms and propose a novel strategy that accelerates offloading by taking advantage of innate properties of MoE LLMs. Using this strategy, we build can run Mixtral-8x7B with mixed quantization on desktop hardware and free-tier Google Colab instances.\n# Introduction",
            "Table 1: Perplexity and model size evaluation of Mixtral-8x7B with different quantization for shared attention (Attn quant) and experts (Experts quant) layers. For comprarison, a Mistral-7B 4-bit quantized model has Wiki2 perplexity 5.03, C4 perplexity 7.56 and MMLU score 61.3%. See Section 4.2 for details. Green values correspond to the configurations we chose for full system evaluation.\n6\nAlgorithm 2-bit Experts 3-bit Experts A100 3080 Mobile 3060 T4 (Colab) A100 3080 Mobile 3060 T4 (Cloud) 3.061 Full algorithm 2.918 W/o expert pre-loading 2.265 W/o LRU cache & pre-loading Naive offloading (accelerate) 1.392 2.655 2.227 1.758 1.059 2.278 2.051 1.547 0.919 2.092 1.567 1.168 0.661 2.845 2.683 2.055 1.246 2.475 2.024 1.595 0.914 2.038 1.857 1.346 1.791 1.603 1.365 1.061 0.580"
        ],
        "response": "The purpose of LRU caching in Mixture-of-Experts language models is to keep active experts in GPU memory for future token processing, reducing expert loading time by reusing recently used experts. This strategy exploits the pattern where experts are active in short sequences of 2-4 tokens or with gaps, as shown in Figure 1. The LRU cache stores the k least recently used experts per layer, significantly speeding up inference for models like Mixtral-8x7B [source: http://arxiv.org/pdf/2312.17238]. Even a simple strategy like LRU can achieve high cache hit ratios, as demonstrated in Figure 2, where k=2 yields an 84% hit rate. This approach is particularly effective on consumer hardware with limited GPU memory, enabling practical deployment of large MoE models [source: http://arxiv.org/pdf/2312.17238].",
        "reference": "The purpose of LRU caching in Mixture-of-Experts language models is to speed up inference by keeping active experts in GPU memory as a cache for future tokens. This allows for faster access to the necessary experts, even if the number of experts that can be stored in the cache is limited by the available GPU memory."
    },
    {
        "user_input": "What is the key difference between encoding the input prompt and generating tokens in generative inference?",
        "retrieved_contexts": [
            "The generative inference workload consists of two phases: 1) encoding the input prompt and 2) generating tokens conditioned on that prompt. The key difference between these two phases is that prompt tokens are encoded in parallel (layer-by-layer), whereas the generation runs sequentially (token-by-token and layer-by-layer). In general, phase 1 works relatively well with existing Mixture- of-Experts algorithms, since each layer can only be loaded once for the entire prompt. In turn, when generating tokens, one must load layer once per each token generated. In practice, this means that inference speed is limited by how fast one can fetch parameters from system memory.\nBelow, we look for patterns in how the MoE model loads its experts and propose ways to exploit these patterns to speed up inference time.\n4To learn more about these methods, please refer to surveys such as Gholami et al. (2021); Liang et al. (2021) 5As opposed to running a processing a large batch of texts over many seconds, as in Sheng et al. (2023)\n3\nSelected experts for Mixtral-8x7B-Instruct woe 0 (top) and 15 ae =n a oa ao a âme: a n: ee Layer 15 expert # Layer 0 expert # MAUR STARR O However about |= and 4 training data owerful language model based trained Trans former f architecture",
            "# 2.4 Hardware Setup\nWhile our analysis is not specific to any hardware setup, we target the hardware specifications of cheap / free-tier cloud instances Google (2023) and the upper half of gaming computers Steam (2023): i) enough system memory to hold model parameters, ii) a GPU with 11-16GB VRAM and iii) host-to-device communication at 8-16GB/s (PCIe Gen.3). If we examine popular open-access MoE models (Mixtral-8x7B and switch-c-2048), we find that all non-experts can fit a fraction of available GPU memory. In turn, the experts that constitute vast majority of model parameters do not fit even with quantization. Finally, even if we could fit the model parameters in memory, running generative inference requires additional memory for layer activations and past attention keys & values.\n# 3 Method\nIn this work, we aim to systematically find the optimal way to inference modern Mixture-of-Experts LLMs on desktop or low-end cloud instances. More specifically, we focus on the task of generating tokens interactively, i.e. generate multiple tokens per second at batch size 15.",
            "Our approach engenders a more heterogeneous generation of hallucinations, mitigating the bias that may arise from the use of a single model and promoting equity within the dataset. This is due to the varying architectures and training corpora inherent to different LLMs. Furthermore, we have adopted an unconstrained generation methodology for the continuation of natural language content. This entails directly inputting the text to be continued into the model without any restrictive prompt thereby obtaining organic results. For each input example, we concurrently generate five candidate continuations. To maintain consistency across all models, we employ uniform parameter settings, with a temperature coefficient set at 1.0 and max new tokens limited to 1024.\n# C. Hallucination Ranking\nGiven the unconstrained nature of our generation paradigm, the task of discerning whether the generated content is indeed hallucinated presents a significant challenge. Upon generating the continuations, a straightforward reliance on human verifi- cation is infeasible. An exclusive dependence on human anno- tation would incur substantial costs and may not be sustainable at scale, whereas a purely machine-based approach, such as utilizing GPT4, could potentially yield less accurate results.",
            "However, both discriminative and selective evaluations en- counter a substantial challenge. They are predicated on the assumption that âLLMsâs capacity to produce reliable text is contingent upon their discernment between hallucinated and non-hallucinated content.â These methods do not simulate the evaluation of the modelâs output for hallucinations. Conse- quently, generative evaluation is crucial as it directly evaluates the presence of hallucinations in the text generated by the LLM. Specifically, the LLM under evaluation is provided with an initial text and is then tasked with generating a continuation. Subsequently, various reference-based techniques are utilized to determine if the continuation includes hallucinations. How- ever, the challenge arises from the fact that it is not feasible to automatically and accurately ascertain if newly generated text is hallucinated; if it were, annotated datasets would be redun- dant. In scenarios of unrestrained text generation, this issue becomes increasingly complex. This complexity stems from the fact that text generated without constraints may introduce a multitude of entities and facts absent in the reference material, complicating the verification of their accuracy. Despite these hurdles, generative evaluation continues to be a predominant strategy in Natural Language Generation (NLG) tasks [26].",
            "on ChatGPT, from 65 to 56 on GPT-4, and notably from 26 to 15 on Llama2.\n# C.2 Results on visual attack\nWe evaluate the efficacy of EmotionAttack across four distinct models: LLaVa-13b 28, blip2- opt 25, blip2-t5 25, and CogVLM 46. Our experimentation encompasses a set of 16 tasks from Instruction Induction 17 and an additional 11 tasks sourced from BIG-Bench-Hard 44. These tasks are deliberately diverse, varying in difficulty and perspective, covering domains such as math problem-solving, semantic comprehension, logical reasoning, and casual inference.\nBaselines To benchmark the performance of our vision attack method, we juxtapose it against the original prompt setting. Given that certain AI models necessitate image inputs, we employ a small black picture accompanied by the original prompt as a baseline for these specific models.\nThe outcomes of our experiments across four distinct language models(LMs) on 27 tasks are presented in Table 8. The numerical values depict the averages across the 27 tasks for each specific model within its designated setting. The key findings are outlined below:",
            "Kaya Stechly, Matthew Marquez, and Subbarao Kambhampati. Gpt-4 doesnât know itâs wrong: An analysis of iterative prompting for reasoning problems. arXiv preprint arXiv:2310.12397, 2023.\nYashar Talebirad and Amirhossein Nadiri. Multi-agent collaboration: Harnessing the power of intelligent llm agents. arXiv preprint arXiv:2306.03314, 2023.\n16\nMiles Turpin, Julian Michael, Ethan Perez, and Samuel R Bowman. Language models donât al- ways say what they think: Unfaithful explanations in chain-of-thought prompting. arXiv preprint arXiv:2305.04388, 2023.\nPriyan Vaithilingam, Tianyi Zhang, and Elena L Glassman. Expectation vs. experience: Evaluating the usability of code generation tools powered by large language models. In Chi conference on human factors in computing systems extended abstracts, pp. 1â7, 2022.\nKarthik Valmeekam, Matthew Marquez, and Subbarao Kambhampati. Can large language models really improve by self-critiquing their own plans? arXiv preprint arXiv:2310.08118, 2023.",
            "# 3.1 Motivation: Selection as a Means of Compression\nWe argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact, we can view the tradeoï¬s of popular sequence models from this point of view. For example, attention is both eï¬ective and ineï¬cient because it explicitly does not compress context at all. This can be seen from the fact that autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are eï¬cient because they have a ï¬nite state, implying constant-time inference and linear-time training. However, their eï¬ectiveness is limited by how well this state has compressed the context.\nTo understand this principle, we focus on two running examples of synthetic tasks (Figure 2).\nâ¢ The Selective Copying task modiï¬es the popular Copying task (Arjovsky, Shah, and Bengio 2016) by varying the position of the tokens to memorize. It requires content-aware reasoning to be able to memorize the relevant tokens (colored) and ï¬lter out the irrelevant ones (white).",
            "Abstract\nWe introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine- tuned to follow instructions, Mixtral 8x7B â Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â chat model on human bench- marks. Both the base and instruct models are released under the Apache 2.0 license.\nCode: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/mixtral-of-experts/",
            "Perfectly solved by LTI (e.g. convolutional) models that do not need to look at the actual inputs\nHi i Hl ] Bw H a H > BH\nFigure 2: (Left) The standard version of the Copying task involves constant spacing between input and output elements and is easily solved by time-invariant models such as linear recurrences and global convolutions. (Right Top) The Selective Copying task has random spacing in between inputs and requires time-varying models that can selectively remember or ignore inputs depending on their content. (Right Bottom) The Induction Heads task is an example of associative recall that requires retrieving an answer based on context, a key ability for LLMs.",
            "Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformersâ computational ineï¬ciency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of eï¬cient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simpliï¬ed end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5Ã higher throughput than Transformers) and linear scaling in sequence"
        ],
        "response": "The key difference is that prompt encoding is parallel (layer-by-layer), while token generation is sequential (token-by-token and layer-by-layer). This makes inference speed dependent on memory bandwidth during generation. [http://arxiv.org/pdf/2312.17238]",
        "reference": "The key difference between encoding the input prompt and generating tokens in generative inference is that encoding the prompt is done in parallel, layer-by-layer, while generating tokens is done sequentially, token-by-token and layer-by-layer. Additionally, generating tokens requires loading the layer once per each token generated. This difference affects the inference speed, as it is limited by how fast one can fetch parameters from system memory."
    },
    {
        "user_input": "How can the model size of large language models be reduced through quantization?",
        "retrieved_contexts": [
            "# 2.2 Post-training Quantization of LLMs\nA natural way to circumvent this is to reduce the model size through quantization (Nagel et al., 2020; Gholami et al., 2021; Frantar et al., 2022), sparsification Frantar & Alistarh (2023a); Ma et al. (2023),\n2https://github.com/dvmazur/mixtral-offloading 3https://huggingface.co/google/switch-c-2048, released in November 15th, 2022\n2\nfactorization Hsu et al. (2022), or a combination thereof. These compression types are not specific to LLMs and are based on much older methods outside the scope of our work4. However, recent works found that there are unique challenges to quantizing very large transformer-based language models due to emergent outliersDettmers et al. (2022); Lin et al. (2023); Dettmers et al. (2023).",
            "Badri, H. and Shaji, A. Half-quadratic quantization of large machine learning models, November 2023. URL https://mobiusml.github.io/hqq_blog/.\nBiderman, S., Schoelkopf, H., Anthony, Q., Bradley, H., OâBrien, K., Hallahan, E., Khan, M. A., Purohit, S., Prashanth, U. S., Raff, E., et al. Pythia: A suite for analyzing large language models across training and scaling. arXiv preprint arXiv:2304.01373, 2023.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. In Conference on Neural Information Processing Systems (NeurIPS), 2020.\nChee, J., Cai, Y., Kuleshov, V., and Sa, C. D. Quip: 2-bit quantization of large language models with guarantees, 2023.",
            "# Denis Mazur Moscow Institute of Physics and Technology Yandex Researchcore denismazur8@gmail.com\n# Abstract\nWith the widespread adoption of Large Language Models (LLMs), many deep learning practitioners are looking for strategies of running these models more efficiently. One such strategy is to use sparse Mixture-of-Experts (MoE) â a type of model architectures where only a fraction of model layers are active for any given input. This property allows MoE-based language models to generate tokens faster than their âdenseâ counterparts, but it also increases model size due to having multiple âexpertsâ. Unfortunately, this makes state-of-the-art MoE language models difficult to run without high-end GPUs. In this work, we study the problem of running large MoE language models on consumer hardware with limited accelerator memory. We build upon parameter offloading algorithms and propose a novel strategy that accelerates offloading by taking advantage of innate properties of MoE LLMs. Using this strategy, we build can run Mixtral-8x7B with mixed quantization on desktop hardware and free-tier Google Colab instances.\n# Introduction",
            "1When deployed in 16-bit precision, Falcon-180B needs approximately 360GB, while LLaMA-2 70B requires 140GB of combined accelerator memory.\nIn this work, we systematically develop techniques for running large MoE language models with limited GPU memory. Our main objective is inferencing (generating tokens) with Mixtral-8x7B- Instruct â a MoE-based chat assistant â on a desktop-grade hardware where only a fraction of experts fit into the accelerator memory. To that end:\nwe observe how MoE language model accesses its experts between tokens, and find several regularities: i) some experts are reused between adjacent tokens and ii) the model hidden states of early layers already âknowâ which experts are to be used at subsequent layers. â¢ we design a MoE-specific offloading strategy that takes advantage of these regularities: i) it uses LRU cache to significantly reduces GPU-RAM communication, leading to faster generation and ii) it guesses which experts are needed ahead of time to better overlap expert loading with computation.\nâ¢ we consider the specific scenario of running Mixtral-8x7B-Instruct on a T4, RTX 3060 and RTX 3080 Mobile and develop a practical combination of mixed quantization and the proposed offloading algorithm to run this model interactively at 2-3 tokens per second depending on the hardware. The source code with our implementation is available online2",
            "# 5 Conclusion and Future Work\nIn this work, we explore strategies for accelerating Mixture-of-Experts based language models on consumer hardware with limited GPU memory. We propose a MoE-centric approach to offloading\n7\nand explore how mixed quantization affects perplexity and performance on language understanding tasks. We evaluate the proposed strategies and show that they produce a significant increase in generation speed compared to naÂ¨ve approaches on consumer-grade hardware, including free-tier Google Colab.\nOur method provides a practical solution for inferencing large MoE language models on resource- constricted hardware, enabling broader access to these powerful models for research and development. As future work, we plan to explore further offloading strategies, based on speculative expert predic- tion.\n# Acknowledgements\nAuthors would like to acknowledge mobicham@ for helpful discussions on Mixtral quantization.\n# References\nAminabadi, R. Y., Rajbhandari, S., Awan, A. A., Li, C., Li, D., Zheng, E., Ruwase, O., Smith, S., Zhang, M., Rasley, J., and He, Y. Deepspeed-inference: Enabling efficient inference of transformer models at unprecedented scale. In Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis, SC â22. IEEE Press, 2022. ISBN 9784665454445.",
            "MoE quantization. As we described earlier in Section 2.2, there are multiple weight quantization algorithms optimized for LLMs. Model compression has a natural synergy with offloading because compressed models take less time to load onto GPU. In our experitments, we also observed that MoE models get better quality-size trade-offs when quantizing experts to a lower bitwidth, while keeping all non-expert layers at 4-bit.\nWe use Half Quadratic Quantization (HQQ) (Badri & Shaji, 2023) â a data-free quantization algorithm that supports a variety of bit rates. However, we chose this algorithm only for convenience, because it was already well tested for Mixtral models. Since our analysis does not rely on any specific choice of quantization, we believe that if we chose another quantization algorithm (e.g. GPTQ or AWQ) our conclusions would be similar. In our early experiments, we also tried the sub-1-bit quantization from QMoE Frantar & Alistarh (2023b) that worked well on the Switch-c-2048 model. However, we found that sub-1-bit compression caused too significant a loss in perplexity for Mixtral-8x7B models.",
            "â¢ Audio and Genomics. Mamba out-performs prior state-of-the-art models such as SaShiMi, Hyena, and Transform- ers on modeling audio waveforms and DNA sequences, both in pretraining quality and downstream metrics (e.g. reducing FID on a challenging speech generation dataset by more than half). In both settings, its performance improves with longer context up to million-length sequences.\nâ¢ Language Modeling. Mamba is the ï¬rst linear-time sequence model that truly achieves Transformer-quality performance, both in pretraining perplexity and downstream evaluations. With scaling laws up to 1B parameters, we show that Mamba exceeds the performance of a large range of baselines, including very strong modern Transformer training recipes based on LLaMa (Touvron et al. 2023). Our Mamba language model has 5Ã generation throughput compared to Transformers of similar size, and Mamba-3Bâs quality matches that of Transformers twice its size (e.g. 4 points higher avg. on common sense reasoning compared to Pythia-3B and even exceeding Pythia-7B).\nModel code and pre-trained checkpoints are open-sourced at https://github.com/state-spaces/mamba.\n2\n# Selective State Space Model\n# with Hardware-aware State Expansion\n# A\nvuvy GPU SRAM Selection Mechanism es\nSelection Mechanism",
            "Generally speaking, the optimal compression rate for most LLMs is 4 bits per parameter Dettmers & Zettlemoyer (2022). While there are more extreme algorithms for 3- and even 2-bit compression Chee et al. (2023); Lin et al. (2023); Dettmers et al. (2023), they are typically inferior to choosing a smaller model and quantizing it to around 4 bits. Most recently, there has been several concurrent works for quantizing Mixture-of-Experts models (Kim et al., 2023; Frantar & Alistarh, 2023b).\n# Inference with Parameter Offloading\nA recent line of work explores inferencing and training large models with limited accelerator mem- ory by âoffloadingâ their parameters to another, cheaper memory, such as system RAM or even SSD (Pudipeddi et al., 2020; Ren et al., 2021). This technique works by loading model parameters just-in-time when they are needed for computation. Since most deep learning models use layers in a fixed order, offloading can pre-dispatch the next layer parameters in the background, ahead of time.",
            "Table 2: Inference speed for Mixtral-8x7B in low-tier , measured in tokens per second.\nAs discussed earlier, we use HQQ Badri & Shaji (2023) data-free quantization algorithm and consider the following quantization schemes:\n1. FP16 (no quantization) 2. HQQ 4-bit with group size 64, scale group size 256 3. HQQ 3-bit with group size 64, scale group size 128 4. HQQ 2-bit with group size 16, scale group size 128\nNote that the actual model size with n-bit quantization is larger than n bits per parameter. This is because the quantized data format also stores quantization scale and zero point for each group of weights. Notably, the above 2-bit quantization scheme uses, on average, 2.6 bits per parameter due to a large number of quantization schemes. We also keep embeddings, logits, MoE gates and normalization layers in 16-bit format.\nTable 1 summarizes our results: overall, it seems advantageous to quantize experts to 3 or 2 bits while keeping attention layers to a higher bitwidth (16 or 4 bits). Based on these evaluations, we chose two quantization schemes (highlighted in green) that offer favourable performance-size trade-offs within the target hardware constraints.\n# 4.3 Practical offloading performance",
            "R. Guo, P. Sun, E. Lindgren, Q. Geng, D. Simcha, F. Chern, and S. Kumar. Accelerating large-scale inference with anisotropic vector quantization. In Proceedings of the 37th International Conference on Machine Learning, 2020.\nY. K. Jang and N. I. Cho. Self-supervised product quantization for deep un- supervised image retrieval. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 12085â12094, October 2021.\nH. JÂ´egou, M. Douze, and C. Schmid. Product quantization for nearest neigh- bor search. IEEE Transactions on Pattern Analysis and Machine Intelli- gence, 33(1):117â128, 2011.\nJ. Johnson, M. Douze, and H. JÂ´egou. Billion-scale similarity search with gpus. IEEE Transactions on Big Data, 7(3):535â547, 2021.\nY. Kalantidis and Y. Avrithis. Locally optimized product quantization for approximate nearest neighbor search. In 2014 IEEE Conference on Com- puter Vision and Pattern Recognition, pages 2329â2336, 2014."
        ],
        "response": "Model size reduction through quantization involves converting weights to lower bit representations, such as 4-bit or 2-bit formats, which decreases memory footprint and storage requirements [http://arxiv.org/pdf/2312.17238]. For example, Mixtral-8x7B achieves optimal trade-offs by quantizing experts to 3 or 2 bits while keeping attention layers at 16 or 4 bits [http://arxiv.org/pdf/2312.17238]. Half Quadratic Quantization (HQQ) is a data-free algorithm supporting various bit rates, tested on Mixtral models [http://arxiv.org/pdf/2312.17238]. However, sub-1-bit quantization may cause significant perplexity loss in large models like Mixtral-8x7B [http://arxiv.org/pdf/2312.17238]. The optimal compression rate for most LLMs is 4 bits per parameter, with extreme 3-bit or 2-bit algorithms often inferior to smaller models at 4 bits [http://arxiv.org/pdf/2312.17238]. Mixture-of-Experts models benefit from specialized quantization strategies due to their unique architecture [http://arxiv.org/pdf/2312.17238]. Quantization enables faster parameter offloading by reducing model size, improving inference speed on limited hardware [http://arxiv.org/pdf/2312.17238]. The actual model size with n-bit quantization exceeds n bits per parameter due to stored scales and zero points [http://arxiv.org/pdf/2312.17238]. Mixed quantization schemes balance performance and memory efficiency, as demonstrated in practical deployments [http://arxiv.org/pdf/2312.17238].",
        "reference": "The model size of large language models can be reduced through quantization by using compression techniques such as reducing the number of bits per parameter. The optimal compression rate for most large language models is 4 bits per parameter, although there are more extreme algorithms for 3- and 2-bit compression. However, choosing a smaller model and quantizing it to around 4 bits is typically a better option. Additionally, there have been recent works on quantizing Mixture-of-Experts models."
    },
    {
        "user_input": "Alternatively, how can the model size be reduced through sparsification while maintaining performance?",
        "retrieved_contexts": [
            "# 2.2 Post-training Quantization of LLMs\nA natural way to circumvent this is to reduce the model size through quantization (Nagel et al., 2020; Gholami et al., 2021; Frantar et al., 2022), sparsification Frantar & Alistarh (2023a); Ma et al. (2023),\n2https://github.com/dvmazur/mixtral-offloading 3https://huggingface.co/google/switch-c-2048, released in November 15th, 2022\n2\nfactorization Hsu et al. (2022), or a combination thereof. These compression types are not specific to LLMs and are based on much older methods outside the scope of our work4. However, recent works found that there are unique challenges to quantizing very large transformer-based language models due to emergent outliersDettmers et al. (2022); Lin et al. (2023); Dettmers et al. (2023).",
            "# 3.1 Motivation: Selection as a Means of Compression\nWe argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact, we can view the tradeoï¬s of popular sequence models from this point of view. For example, attention is both eï¬ective and ineï¬cient because it explicitly does not compress context at all. This can be seen from the fact that autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are eï¬cient because they have a ï¬nite state, implying constant-time inference and linear-time training. However, their eï¬ectiveness is limited by how well this state has compressed the context.\nTo understand this principle, we focus on two running examples of synthetic tasks (Figure 2).\nâ¢ The Selective Copying task modiï¬es the popular Copying task (Arjovsky, Shah, and Bengio 2016) by varying the position of the tokens to memorize. It requires content-aware reasoning to be able to memorize the relevant tokens (colored) and ï¬lter out the irrelevant ones (white).",
            "We illustrate an example of how LRU cache saves experts in Figure 1 (see caption). LRU is a very simple strategy that does not consider factors like expert activation frequencies, varying cache size between MoE layers, or any sequential patterns in expert activation. However, we found that even this simple strategy can significantly speed up inference for modern Mixture-of-Experts models such as Mixtral-8x7B (see Section 4 for detailed evaluation).\n# 3.2 Speculative Expert Loading\nWhile LRU caching can reduce the average expert loading time, most of the inference time is still spent waiting for the next expert to be loaded. The reason behind this is that, unlike with dense models, MoE offloading cannot effectively overlap expert loading with computation. To understand this problem, let us zoom into the process of generating a single token, layer-by-layer. The full compute workload starts by embedding the previous token via look-up, then alternates between running self-attention and MLP for each transformer block in the model. Finally, the outputs from the last transformer block are used to predict next token logits with a linear projection.",
            "â¢ Sparse Transformer (Child et al. 2019) showed a proof-of-concept of using a strided sparse attention Transformer to model audio waveforms of length 220 = 1048576, although did not discuss performance tradeoï¬s when controlling for computation and model size.\nIn contrast, we believe this work presents one of the ï¬rst approaches to meaningfully demonstrate increasing performance with longer context.\n# C Mechanics of Selective SSMs\nProof of Theorem 1. Consider a selective SSM (Algorithm 2) with ð = 1, A = â1, B = 1, ð â = ð«ððð¾ðºð(ð¥), ðâ = ððð¿ððððð. The corresponding continuous-time SSM (1) is\nâ(ð¡) = ââ(ð¡) + ð¥(ð¡)\nwhich is also called a leaky integrator.\n27\nThe discretization step size is\nThe discretization step size is",
            "â¢ As suggestions for LLM developers, it is im- portant to perform the data decontamination checking between pre-training data and any related data (e.g., training and test sets) when using evaluation benchmarks. In addition, it is also necessary to report the contamination analysis on the evaluated benchmarks as ref- erence. We also suggest reporting the detailed composition of the pre-training data.\nâ¢ As suggestions for benchmark maintainers, we suggest that a diverse set of test prompts should be employed for reducing the influ- ence of the prompt sensitivity. It is also mean- ingful to conduct the contamination analysis between the benchmark data and existing pre- training corpus, alerting any potential contam- ination risks. For evaluation, each submission is suggested to be accompanied with a special contamination analysis report.\n# 2 Empirical Study about Benchmark Leakage\nDuring pre-training, the data contamination or leak- age about possible evaluation benchmarks, is likely\nto be unconsciously triggered (Oren et al., 2023; Sainz et al., 2023). It would violate regular eval- uation settings for assessing zero/few-shot gener- alization capability, thus affecting the capability assessment of LLMs. To better understand the potential influence of the benchmark leakage is- sue, we conduct an empirical study that continually trains small-sized LLMs on three settings with dif- ferent levels of information leakage.\n# 2.1 Experimental Setup",
            "The main idea is to leverage properties of modern accelerators (GPUs) to materialize the state â only in more eï¬cient levels of the memory hierarchy. In particular, most operations (except matrix multiplication) are bounded by memory bandwidth (Dao, Fu, Ermon, et al. 2022; Ivanov et al. 2021; Williams, Waterman, and Patterson 2009). This includes our scan operation, and we use kernel fusion to reduce the amount of memory IOs, leading to a signiï¬cant speedup compared to a standard implementation.\nConcretely, instead of preparing the scan input (A, B) of size (ð±, ð», ð³, ð½) in GPU HBM (high-bandwidth memory), we load the SSM parameters (â, A, B, C) directly from slow HBM to fast SRAM, perform the discretization and recurrence in SRAM, and then write the ï¬nal outputs of size (ð±, ð», ð³) back to HBM.\nTo avoid the sequential recurrence, we observe that despite not being linear it can still be parallelized with a work-eï¬cient parallel scan algorithm (Blelloch 1990; Martin and Cundy 2018; Smith, Warrington, and Linderman 2023).",
            "Generally speaking, the optimal compression rate for most LLMs is 4 bits per parameter Dettmers & Zettlemoyer (2022). While there are more extreme algorithms for 3- and even 2-bit compression Chee et al. (2023); Lin et al. (2023); Dettmers et al. (2023), they are typically inferior to choosing a smaller model and quantizing it to around 4 bits. Most recently, there has been several concurrent works for quantizing Mixture-of-Experts models (Kim et al., 2023; Frantar & Alistarh, 2023b).\n# Inference with Parameter Offloading\nA recent line of work explores inferencing and training large models with limited accelerator mem- ory by âoffloadingâ their parameters to another, cheaper memory, such as system RAM or even SSD (Pudipeddi et al., 2020; Ren et al., 2021). This technique works by loading model parameters just-in-time when they are needed for computation. Since most deep learning models use layers in a fixed order, offloading can pre-dispatch the next layer parameters in the background, ahead of time.",
            "â¢ Audio and Genomics. Mamba out-performs prior state-of-the-art models such as SaShiMi, Hyena, and Transform- ers on modeling audio waveforms and DNA sequences, both in pretraining quality and downstream metrics (e.g. reducing FID on a challenging speech generation dataset by more than half). In both settings, its performance improves with longer context up to million-length sequences.\nâ¢ Language Modeling. Mamba is the ï¬rst linear-time sequence model that truly achieves Transformer-quality performance, both in pretraining perplexity and downstream evaluations. With scaling laws up to 1B parameters, we show that Mamba exceeds the performance of a large range of baselines, including very strong modern Transformer training recipes based on LLaMa (Touvron et al. 2023). Our Mamba language model has 5Ã generation throughput compared to Transformers of similar size, and Mamba-3Bâs quality matches that of Transformers twice its size (e.g. 4 points higher avg. on common sense reasoning compared to Pythia-3B and even exceeding Pythia-7B).\nModel code and pre-trained checkpoints are open-sourced at https://github.com/state-spaces/mamba.\n2\n# Selective State Space Model\n# with Hardware-aware State Expansion\n# A\nvuvy GPU SRAM Selection Mechanism es\nSelection Mechanism",
            "There is good reason for the uptick in research activity. Graph algorithms are among the most successful algorithms there are for top-k vector retrieval. They are often remarkably fast during retrieval and produce accurate solution sets.\nThat success makes it all the more enticing to improve their other charac- teristics. For example, graph indices are often large, requiring far too much memory. Incorporating compression into graphs, therefore, is a low-hanging fruit that has been explored [Singh et al., 2021] but needs further investiga- tion. More importantly, finding an even sparser graph without losing accuracy is key in reducing the size of the graph to begin with, and that boils down to designing better heuristics.\nHeuristics play a key role in the construction time of graph indices too. Building a graph index for a collection of billions of points, for example, is not feasible for the variant of the Vamana algorithm that offers theoretical guarantees. Heuristics introduced in that work lost all such guarantees, but made the graph more practical.\nEnhancing the capabilities of graph indices too is an important practical consideration. For example, when the graph is too large and, so, must rest on disk, optimizing disk access is essential in maintaining the speed of query processing [Jayaram Subramanya et al., 2019]. When the collection of vectors",
            "MoE quantization. As we described earlier in Section 2.2, there are multiple weight quantization algorithms optimized for LLMs. Model compression has a natural synergy with offloading because compressed models take less time to load onto GPU. In our experitments, we also observed that MoE models get better quality-size trade-offs when quantizing experts to a lower bitwidth, while keeping all non-expert layers at 4-bit.\nWe use Half Quadratic Quantization (HQQ) (Badri & Shaji, 2023) â a data-free quantization algorithm that supports a variety of bit rates. However, we chose this algorithm only for convenience, because it was already well tested for Mixtral models. Since our analysis does not rely on any specific choice of quantization, we believe that if we chose another quantization algorithm (e.g. GPTQ or AWQ) our conclusions would be similar. In our early experiments, we also tried the sub-1-bit quantization from QMoE Frantar & Alistarh (2023b) that worked well on the Switch-c-2048 model. However, we found that sub-1-bit compression caused too significant a loss in perplexity for Mixtral-8x7B models."
        ],
        "response": "Sparsification reduces model size by pruning redundant parameters or neurons, maintaining performance through iterative retraining and magnitude-based weight pruning Frantar & Alistarh (2023a); Ma et al. (2023) [http://arxiv.org/pdf/2312.17238]. For Mixture-of-Experts models, selective expert loading via LRU caching can optimize memory usage without significant performance loss [http://arxiv.org/pdf/2312.17238]. Graph-based sparsification techniques improve retrieval speed while reducing memory footprint, though heuristic design remains critical for accuracy preservation [http://arxiv.org/pdf/2401.09350]. Sub-1-bit quantization experiments on MoE models show significant perplexity degradation, suggesting trade-offs between compression and performance [http://arxiv.org/pdf/2312.17238].",
        "reference": "The model size can be reduced through sparsification while maintaining performance. This can be achieved through techniques such as quantization, sparsification, factorization, or a combination of these methods. Recent works have shown that quantizing very large transformer-based language models can be challenging due to emergent outliers. However, the optimal compression rate for most language models is around 4 bits per parameter. Additionally, there have been concurrent works exploring the quantization of Mixture-of-Experts models. Another approach to reducing model size is \"offloading\" parameters to cheaper memory, such as system RAM or SSD, which allows for training and inference with limited accelerator memory."
    },
    {
        "user_input": "How does the sequential generation of tokens in interactive inference impact the speed of offloading in Mixture-of-Experts language models?",
        "retrieved_contexts": [
            "# 5 Conclusion and Future Work\nIn this work, we explore strategies for accelerating Mixture-of-Experts based language models on consumer hardware with limited GPU memory. We propose a MoE-centric approach to offloading\n7\nand explore how mixed quantization affects perplexity and performance on language understanding tasks. We evaluate the proposed strategies and show that they produce a significant increase in generation speed compared to naÂ¨ve approaches on consumer-grade hardware, including free-tier Google Colab.\nOur method provides a practical solution for inferencing large MoE language models on resource- constricted hardware, enabling broader access to these powerful models for research and development. As future work, we plan to explore further offloading strategies, based on speculative expert predic- tion.\n# Acknowledgements\nAuthors would like to acknowledge mobicham@ for helpful discussions on Mixtral quantization.\n# References\nAminabadi, R. Y., Rajbhandari, S., Awan, A. A., Li, C., Li, D., Zheng, E., Ruwase, O., Smith, S., Zhang, M., Rasley, J., and He, Y. Deepspeed-inference: Enabling efficient inference of transformer models at unprecedented scale. In Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis, SC â22. IEEE Press, 2022. ISBN 9784665454445.",
            "The generative inference workload consists of two phases: 1) encoding the input prompt and 2) generating tokens conditioned on that prompt. The key difference between these two phases is that prompt tokens are encoded in parallel (layer-by-layer), whereas the generation runs sequentially (token-by-token and layer-by-layer). In general, phase 1 works relatively well with existing Mixture- of-Experts algorithms, since each layer can only be loaded once for the entire prompt. In turn, when generating tokens, one must load layer once per each token generated. In practice, this means that inference speed is limited by how fast one can fetch parameters from system memory.\nBelow, we look for patterns in how the MoE model loads its experts and propose ways to exploit these patterns to speed up inference time.\n4To learn more about these methods, please refer to surveys such as Gholami et al. (2021); Liang et al. (2021) 5As opposed to running a processing a large batch of texts over many seconds, as in Sheng et al. (2023)\n3\nSelected experts for Mixtral-8x7B-Instruct woe 0 (top) and 15 ae =n a oa ao a âme: a n: ee Layer 15 expert # Layer 0 expert # MAUR STARR O However about |= and 4 training data owerful language model based trained Trans former f architecture",
            "We illustrate an example of how LRU cache saves experts in Figure 1 (see caption). LRU is a very simple strategy that does not consider factors like expert activation frequencies, varying cache size between MoE layers, or any sequential patterns in expert activation. However, we found that even this simple strategy can significantly speed up inference for modern Mixture-of-Experts models such as Mixtral-8x7B (see Section 4 for detailed evaluation).\n# 3.2 Speculative Expert Loading\nWhile LRU caching can reduce the average expert loading time, most of the inference time is still spent waiting for the next expert to be loaded. The reason behind this is that, unlike with dense models, MoE offloading cannot effectively overlap expert loading with computation. To understand this problem, let us zoom into the process of generating a single token, layer-by-layer. The full compute workload starts by embedding the previous token via look-up, then alternates between running self-attention and MLP for each transformer block in the model. Finally, the outputs from the last transformer block are used to predict next token logits with a linear projection.",
            "1When deployed in 16-bit precision, Falcon-180B needs approximately 360GB, while LLaMA-2 70B requires 140GB of combined accelerator memory.\nIn this work, we systematically develop techniques for running large MoE language models with limited GPU memory. Our main objective is inferencing (generating tokens) with Mixtral-8x7B- Instruct â a MoE-based chat assistant â on a desktop-grade hardware where only a fraction of experts fit into the accelerator memory. To that end:\nwe observe how MoE language model accesses its experts between tokens, and find several regularities: i) some experts are reused between adjacent tokens and ii) the model hidden states of early layers already âknowâ which experts are to be used at subsequent layers. â¢ we design a MoE-specific offloading strategy that takes advantage of these regularities: i) it uses LRU cache to significantly reduces GPU-RAM communication, leading to faster generation and ii) it guesses which experts are needed ahead of time to better overlap expert loading with computation.\nâ¢ we consider the specific scenario of running Mixtral-8x7B-Instruct on a T4, RTX 3060 and RTX 3080 Mobile and develop a practical combination of mixed quantization and the proposed offloading algorithm to run this model interactively at 2-3 tokens per second depending on the hardware. The source code with our implementation is available online2",
            "This technique works particularly well when processing large batches of data, during train- ing Pudipeddi et al. (2020); Ren et al. (2021) or large-batch non-interactive inference Aminabadi et al. (2022); Sheng et al. (2023), where each layer processes a lot of tokens each time the layer is loaded from RAM. In turn, when doing interactive inference (e.g. as a chat assistants), offloading works significantly slower than on-device inference. This is because interactive inference generates tokens autoregressively, from left to right. This way, the inference system processes one or few tokens at a time, and therefore spends most of the time waiting for next layerâs parameters to be loaded.\n# 2.4 Hardware Setup",
            "# Denis Mazur Moscow Institute of Physics and Technology Yandex Researchcore denismazur8@gmail.com\n# Abstract\nWith the widespread adoption of Large Language Models (LLMs), many deep learning practitioners are looking for strategies of running these models more efficiently. One such strategy is to use sparse Mixture-of-Experts (MoE) â a type of model architectures where only a fraction of model layers are active for any given input. This property allows MoE-based language models to generate tokens faster than their âdenseâ counterparts, but it also increases model size due to having multiple âexpertsâ. Unfortunately, this makes state-of-the-art MoE language models difficult to run without high-end GPUs. In this work, we study the problem of running large MoE language models on consumer hardware with limited accelerator memory. We build upon parameter offloading algorithms and propose a novel strategy that accelerates offloading by taking advantage of innate properties of MoE LLMs. Using this strategy, we build can run Mixtral-8x7B with mixed quantization on desktop hardware and free-tier Google Colab instances.\n# Introduction",
            "# 2.4 Hardware Setup\nWhile our analysis is not specific to any hardware setup, we target the hardware specifications of cheap / free-tier cloud instances Google (2023) and the upper half of gaming computers Steam (2023): i) enough system memory to hold model parameters, ii) a GPU with 11-16GB VRAM and iii) host-to-device communication at 8-16GB/s (PCIe Gen.3). If we examine popular open-access MoE models (Mixtral-8x7B and switch-c-2048), we find that all non-experts can fit a fraction of available GPU memory. In turn, the experts that constitute vast majority of model parameters do not fit even with quantization. Finally, even if we could fit the model parameters in memory, running generative inference requires additional memory for layer activations and past attention keys & values.\n# 3 Method\nIn this work, we aim to systematically find the optimal way to inference modern Mixture-of-Experts LLMs on desktop or low-end cloud instances. More specifically, we focus on the task of generating tokens interactively, i.e. generate multiple tokens per second at batch size 15.",
            "For regular (dense) models, this architecture allows for efficient offloading schedule that pre-loads the next transformer layer ahead of time, while the previous layer is still running. Unfortunately, this schedule is no longer possible for Mixture-of-Experts models, where MoE MLP layers choose which experts to load just-in-time for computation. This is because the system cannot pre-fetch the next layer until it learns which experts should be loaded. Modern open-access MoE language models choose active experts using the final outputs of the previous layer, which means they cannot be pre-fetched them in parallel with previous layer. While it is not possible6 to pre-reliably prefetch the next set of experts ahead of time, the system could still try to guess the likely next experts and load them speculatively, while processing the previous layer. It the guess is correct, it will speed up the next layer inference; if not, it can load the actual next layerâs experts later. In other words, this type of speculative loading does not change the final model predictions, but may reduce latency if the guess is accurate enough.\n6More specifically, not possible without changing the model architecture, which would require re-training\n4",
            "# 4.3 Practical offloading performance\nFinally we evaluate the performance of the Mixtral8x7B-Instruct model using the offloading tech- niquesproposed throughout this report. Based on the perplexity evaluations from the previous section, we chose 4-bit HQQ quantization for the shared attention layers and 2- or 3-bit quantization for experts. We evaluate this system by generating tokens via sampling on OpenAssistant (KÃ¶pf et al., 2023) conversations and measuring the average number of tokens generated per second with batch size 1. For this evaluation, we always sample proportionally to the predicted probabilities, i.e. without temperature or nucleus sampling.",
            "[10] Jwala Dhamala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada Pruksachatkun, Kai-Wei Chang, and Rahul Gupta. Bold: Dataset and metrics for measuring biases in open-ended language generation. In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency, pages 862â872, 2021.\n[11] Artyom Eliseev and Denis Mazur. Fast inference of mixture-of-experts language models with offloading. arXiv preprint arXiv:2312.17238, 2023.\n[12] William Fedus, Jeff Dean, and Barret Zoph. A review of sparse expert models in deep learning. arXiv preprint arXiv:2209.01667, 2022.\n[13] Trevor Gale, Deepak Narayanan, Cliff Young, and Matei Zaharia. Megablocks: Efficient sparse training with mixture-of-experts. arXiv preprint arXiv:2211.15841, 2022."
        ],
        "response": "Sequential token generation in interactive inference limits offloading speed in MoE models because experts are loaded just-in-time per token, preventing overlapping computation with memory fetches. This results in most time spent waiting for expert loading rather than computation. Speculative expert loading can mitigate this by guessing next experts ahead of time, but it relies on accurate predictions. LRU caching reduces redundant expert loading but doesn't solve the fundamental sequential bottleneck. The problem is exacerbated by MoE's architecture where expert selection depends on previous layer outputs, making pre-fetching unreliable. Mixed quantization helps fit more experts in GPU memory but doesn't address the sequential dependency. On consumer hardware with 11-16GB VRAM, this limits generation to 2-3 tokens per second. This is significantly slower than non-interactive inference where batch processing allows better memory-compute overlap. [source: http://arxiv.org/pdf/2312.17238]",
        "reference": "The sequential generation of tokens in interactive inference impacts the speed of offloading in Mixture-of-Experts language models by slowing down the process. In interactive inference, tokens are generated autoregressively from left to right, processing one or a few tokens at a time. This means that the inference system spends most of the time waiting for the next layer's parameters to be loaded. Unlike regular (dense) models, where the next layer can be pre-loaded while the previous layer is still running, Mixture-of-Experts models cannot pre-fetch the next layer until it learns which experts should be loaded. Therefore, the speed of offloading is limited by how fast the parameters can be fetched from system memory."
    },
    {
        "user_input": "In comparison, how does on-device inference affect the speed of offloading in Mixture-of-Experts language models?",
        "retrieved_contexts": [
            "# 5 Conclusion and Future Work\nIn this work, we explore strategies for accelerating Mixture-of-Experts based language models on consumer hardware with limited GPU memory. We propose a MoE-centric approach to offloading\n7\nand explore how mixed quantization affects perplexity and performance on language understanding tasks. We evaluate the proposed strategies and show that they produce a significant increase in generation speed compared to naÂ¨ve approaches on consumer-grade hardware, including free-tier Google Colab.\nOur method provides a practical solution for inferencing large MoE language models on resource- constricted hardware, enabling broader access to these powerful models for research and development. As future work, we plan to explore further offloading strategies, based on speculative expert predic- tion.\n# Acknowledgements\nAuthors would like to acknowledge mobicham@ for helpful discussions on Mixtral quantization.\n# References\nAminabadi, R. Y., Rajbhandari, S., Awan, A. A., Li, C., Li, D., Zheng, E., Ruwase, O., Smith, S., Zhang, M., Rasley, J., and He, Y. Deepspeed-inference: Enabling efficient inference of transformer models at unprecedented scale. In Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis, SC â22. IEEE Press, 2022. ISBN 9784665454445.",
            "To speed up offloading in practice, we allocate all expert parameters in a contiguous memory buffer that can be moved as a single host-to-device copy. For host-side (RAM) experts, we pin8 this memory buffer for faster communication. Our implementation additionally allocates b=4 on-device buffers used to copy and prefetch experts asynchronously, without modifying existing experts. These buffers are shared between all MoE layers to reduce memory footprint. Overall, the system requires num_layers Ã num_experts expert memory buffers split between host and device memory and b=4 temporary buffers, the size of each buffer being equal to a single expert.\n# 4 Experiments\nIn this section, we verify our earlier hypotheses about MoE behavior and benchmark the inference latency in different conditions. We focus our evaluations on Mixtral-8x7B and Mixtral-8x7B-Instruct models since they represent the current state of the art among open-access MoE models. We organize this section as follows: Section 4.1 measures the effectiveness of expert caching and pre-loading in isolation, Section 4.2 compares different model compression algorithms and verifies our hypotheses from Section 3.3. Finally, Section 4.3 measures the inference latency in several hardware setups.",
            "For regular (dense) models, this architecture allows for efficient offloading schedule that pre-loads the next transformer layer ahead of time, while the previous layer is still running. Unfortunately, this schedule is no longer possible for Mixture-of-Experts models, where MoE MLP layers choose which experts to load just-in-time for computation. This is because the system cannot pre-fetch the next layer until it learns which experts should be loaded. Modern open-access MoE language models choose active experts using the final outputs of the previous layer, which means they cannot be pre-fetched them in parallel with previous layer. While it is not possible6 to pre-reliably prefetch the next set of experts ahead of time, the system could still try to guess the likely next experts and load them speculatively, while processing the previous layer. It the guess is correct, it will speed up the next layer inference; if not, it can load the actual next layerâs experts later. In other words, this type of speculative loading does not change the final model predictions, but may reduce latency if the guess is accurate enough.\n6More specifically, not possible without changing the model architecture, which would require re-training\n4",
            "Expert Offloading. As described earlier, we use LRU cache with an equal number k of cached experts per layer. For Mixtral-8x7B, we use k=2 for 12GB GPUs and k=4 for 16GB ones. We trigger speculative expert loading immediately after the system finished loading all experts for the current layer. The speculative expert loading fetches 1 â 2 most likely experts. The newly loaded experts do not replace the currently cached experts. If a speculatively loaded expert was later used during next layer inference, it will replace the least recently used expert from the next layerâs cache.\nMany consumer devices and free-tier cloud instances have limited host RAM that cannot fit the entire model7. In these cases, the experts must be split between host and device memory. To support this, our implementation of expert LRU cache splits experts between host and GPU devices. When loading and expert to the GPU cache, the system also offloads the least recently used on-device expert back to RAM so as to preserve memory parity.",
            "# 4.3 Practical offloading performance\nFinally we evaluate the performance of the Mixtral8x7B-Instruct model using the offloading tech- niquesproposed throughout this report. Based on the perplexity evaluations from the previous section, we chose 4-bit HQQ quantization for the shared attention layers and 2- or 3-bit quantization for experts. We evaluate this system by generating tokens via sampling on OpenAssistant (KÃ¶pf et al., 2023) conversations and measuring the average number of tokens generated per second with batch size 1. For this evaluation, we always sample proportionally to the predicted probabilities, i.e. without temperature or nucleus sampling.",
            "The generative inference workload consists of two phases: 1) encoding the input prompt and 2) generating tokens conditioned on that prompt. The key difference between these two phases is that prompt tokens are encoded in parallel (layer-by-layer), whereas the generation runs sequentially (token-by-token and layer-by-layer). In general, phase 1 works relatively well with existing Mixture- of-Experts algorithms, since each layer can only be loaded once for the entire prompt. In turn, when generating tokens, one must load layer once per each token generated. In practice, this means that inference speed is limited by how fast one can fetch parameters from system memory.\nBelow, we look for patterns in how the MoE model loads its experts and propose ways to exploit these patterns to speed up inference time.\n4To learn more about these methods, please refer to surveys such as Gholami et al. (2021); Liang et al. (2021) 5As opposed to running a processing a large batch of texts over many seconds, as in Sheng et al. (2023)\n3\nSelected experts for Mixtral-8x7B-Instruct woe 0 (top) and 15 ae =n a oa ao a âme: a n: ee Layer 15 expert # Layer 0 expert # MAUR STARR O However about |= and 4 training data owerful language model based trained Trans former f architecture",
            "We illustrate an example of how LRU cache saves experts in Figure 1 (see caption). LRU is a very simple strategy that does not consider factors like expert activation frequencies, varying cache size between MoE layers, or any sequential patterns in expert activation. However, we found that even this simple strategy can significantly speed up inference for modern Mixture-of-Experts models such as Mixtral-8x7B (see Section 4 for detailed evaluation).\n# 3.2 Speculative Expert Loading\nWhile LRU caching can reduce the average expert loading time, most of the inference time is still spent waiting for the next expert to be loaded. The reason behind this is that, unlike with dense models, MoE offloading cannot effectively overlap expert loading with computation. To understand this problem, let us zoom into the process of generating a single token, layer-by-layer. The full compute workload starts by embedding the previous token via look-up, then alternates between running self-attention and MLP for each transformer block in the model. Finally, the outputs from the last transformer block are used to predict next token logits with a linear projection.",
            "# Denis Mazur Moscow Institute of Physics and Technology Yandex Researchcore denismazur8@gmail.com\n# Abstract\nWith the widespread adoption of Large Language Models (LLMs), many deep learning practitioners are looking for strategies of running these models more efficiently. One such strategy is to use sparse Mixture-of-Experts (MoE) â a type of model architectures where only a fraction of model layers are active for any given input. This property allows MoE-based language models to generate tokens faster than their âdenseâ counterparts, but it also increases model size due to having multiple âexpertsâ. Unfortunately, this makes state-of-the-art MoE language models difficult to run without high-end GPUs. In this work, we study the problem of running large MoE language models on consumer hardware with limited accelerator memory. We build upon parameter offloading algorithms and propose a novel strategy that accelerates offloading by taking advantage of innate properties of MoE LLMs. Using this strategy, we build can run Mixtral-8x7B with mixed quantization on desktop hardware and free-tier Google Colab instances.\n# Introduction",
            "1When deployed in 16-bit precision, Falcon-180B needs approximately 360GB, while LLaMA-2 70B requires 140GB of combined accelerator memory.\nIn this work, we systematically develop techniques for running large MoE language models with limited GPU memory. Our main objective is inferencing (generating tokens) with Mixtral-8x7B- Instruct â a MoE-based chat assistant â on a desktop-grade hardware where only a fraction of experts fit into the accelerator memory. To that end:\nwe observe how MoE language model accesses its experts between tokens, and find several regularities: i) some experts are reused between adjacent tokens and ii) the model hidden states of early layers already âknowâ which experts are to be used at subsequent layers. â¢ we design a MoE-specific offloading strategy that takes advantage of these regularities: i) it uses LRU cache to significantly reduces GPU-RAM communication, leading to faster generation and ii) it guesses which experts are needed ahead of time to better overlap expert loading with computation.\nâ¢ we consider the specific scenario of running Mixtral-8x7B-Instruct on a T4, RTX 3060 and RTX 3080 Mobile and develop a practical combination of mixed quantization and the proposed offloading algorithm to run this model interactively at 2-3 tokens per second depending on the hardware. The source code with our implementation is available online2",
            "[10] Jwala Dhamala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada Pruksachatkun, Kai-Wei Chang, and Rahul Gupta. Bold: Dataset and metrics for measuring biases in open-ended language generation. In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency, pages 862â872, 2021.\n[11] Artyom Eliseev and Denis Mazur. Fast inference of mixture-of-experts language models with offloading. arXiv preprint arXiv:2312.17238, 2023.\n[12] William Fedus, Jeff Dean, and Barret Zoph. A review of sparse expert models in deep learning. arXiv preprint arXiv:2209.01667, 2022.\n[13] Trevor Gale, Deepak Narayanan, Cliff Young, and Matei Zaharia. Megablocks: Efficient sparse training with mixture-of-experts. arXiv preprint arXiv:2211.15841, 2022."
        ],
        "response": "On-device inference does not directly affect offloading speed in Mixture-of-Experts language models, as offloading involves transferring data between host and device memory. The system uses LRU caching and speculative loading to minimize latency, but these techniques are independent of on-device processing speed. The primary factor influencing offloading speed is the efficiency of memory transfers and caching strategies, not the on-device computation speed [http://arxiv.org/pdf/2312.17238].",
        "reference": "On-device inference is faster than offloading in Mixture-of-Experts language models. This is because during interactive inference, where tokens are generated autoregressively from left to right, the inference system processes one or few tokens at a time. As a result, most of the time is spent waiting for the next layer's parameters to be loaded. While LRU caching can reduce the average expert loading time, the majority of the inference time is still spent waiting for the next expert to be loaded. Unlike dense models, MoE offloading cannot effectively overlap expert loading with computation, making on-device inference faster in comparison."
    },
    {
        "user_input": "What are the architectural differences in terms of feedforward blocks and active parameters used during inference between Mixtral 8x7B and Mistral 7B?",
        "retrieved_contexts": [
            "Abstract\nWe introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine- tuned to follow instructions, Mixtral 8x7B â Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â chat model on human bench- marks. Both the base and instruct models are released under the Apache 2.0 license.\nCode: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/mixtral-of-experts/",
            "Table 2: Comparison of Mixtral with Llama. Mixtral outperforms or matches Llama 2 70B performance on almost all popular benchmarks while using 5x fewer active parameters during inference.\n70 Mixtral 8x7B. âMixtral 8x7B Mixtral 8x7B 355 =o = Es & E60! Mistral 78 % 2681 Mistral 78 3 3 s0 5 = A % 66 50 g 4 45 64 78 138 348708 78 138 348708 78 138 348 70B S66 Mixtral 8x7B 50 Mixtral 8x7B 5 = 564 340 g al Mistral 78 ee Mistral 78 3 5 Â§ 30 5 eo â= Mistral Â° 20 âe LlaMA2 78 (138 348 70B 7B (138 348 708 7B Â«13B 34B 708 Active Params Active Params Active Params\nFigure 3: Results on MMLU, commonsense reasoning, world knowledge and reading comprehension, math and code for Mistral (7B/8x7B) vs Llama 2 (7B/13B/70B). Mixtral largely outperforms Llama 2 70B on all benchmarks, except on reading comprehension benchmarks while using 5x lower active parameters. It is also vastly superior to Llama 2 70B on code and math.",
            "We also present Mixtral 8x7B â Instruct, a chat model fine-tuned to follow instructions using supervised fine-tuning and Direct Preference Optimization [25]. Its performance notably surpasses that of GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â chat model on human evaluation benchmarks. Mixtral â Instruct also demonstrates reduced biases, and a more balanced sentiment profile in benchmarks such as BBQ, and BOLD. We release both Mixtral 8x7B and Mixtral 8x7B â Instruct under the Apache 2.0 license1, free for academic and commercial usage, ensuring broad accessibility and potential for diverse applications. To enable the community to run Mixtral with a fully open-source stack, we submitted changes to the vLLM project, which integrates Megablocks CUDA kernels for efficient inference. Skypilot also allows the deployment of vLLM endpoints on any instance in the cloud.\n# 2 Architectural details\nMixtral is based on a transformer architecture [31] and uses the same modifications as described in [18], with the notable exceptions that Mix- tral supports a fully dense context length of 32k tokens, and the feed- forward blocks are replaced by Mixture-of-Expert layers (Section 2.1). The model architecture parameters are summarized in Table 1. Parameter Value",
            "Detailed results for Mixtral, Mistral 7B and Llama 2 7B/13B/70B and Llama 1 34B2 are reported in Table 2. Figure 2 compares the performance of Mixtral with the Llama models in different categories. Mixtral surpasses Llama 2 70B across most metrics. In particular, Mixtral displays a superior performance in code and mathematics benchmarks.\nSize and Efficiency. We compare our performance to the Llama 2 family, aiming to understand Mixtral modelsâ efficiency in the cost-performance spectrum (see Figure 3). As a sparse Mixture- of-Experts model, Mixtral only uses 13B active parameters for each token. With 5x lower active parameters, Mixtral is able to outperform Llama 2 70B across most categories.\nNote that this analysis focuses on the active parameter count (see Section 2.1), which is directly proportional to the inference compute cost, but does not consider the memory costs and hardware utilization. The memory costs for serving Mixtral are proportional to its sparse parameter count, 47B, which is still smaller than Llama 2 70B. As for device utilization, we note that the SMoEs layer introduces additional overhead due to the routing mechanism and due to the increased memory loads when running more than one expert per device. They are more suitable for batched workloads where one can reach a good degree of arithmetic intensity.",
            "Code: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/mixtral-of-experts/\n# Introduction\nIn this paper, we present Mixtral 8x7B, a sparse mixture of experts model (SMoE) with open weights, licensed under Apache 2.0. Mixtral outperforms Llama 2 70B and GPT-3.5 on most benchmarks. As it only uses a subset of its parameters for every token, Mixtral allows faster inference speed at low batch-sizes, and higher throughput at large batch-sizes.\nMixtral is a sparse mixture-of-experts network. It is a decoder-only model where the feedforward block picks from a set of 8 distinct groups of parameters. At every layer, for every token, a router network chooses two of these groups (the âexpertsâ) to process the token and combine their output additively. This technique increases the number of parameters of a model while controlling cost and latency, as the model only uses a fraction of the total set of parameters per token.\nMixtral is pretrained with multilingual data using a context size of 32k tokens. It either matches or exceeds the performance of Llama 2 70B and GPT-3.5, over several benchmarks. In particular,\nMixture of Experts Layer i gating inputs af outputs router expert",
            "Since then, several lines of work explore Mixture-of-Experts with Transformer-based language models for machine translation Lepikhin et al. (2020), masked language modeling Fedus et al. (2021), general-purpose LLMs Du et al. (2022) and others. Most of these models follow traditional (dense) Transformer architecture for embeddings and attention layers, and only use Mixture for the feedforward (MLP) blocks and use a linear token-level gating function. A common observation across most of these works is that MoE models are cheaper to train and inference Fedus et al. (2021); Lepikhin et al. (2020), but require more parameters than a dense model with equivalent perplexity. Pre-trained Mixture-of-Experts LLMs have been openly available for over a year3. However, these models seem to have gained less traction than equivalent dense models, arguable because their sheer model size (over a trillion parameters) makes them difficult to use. Most recently, Mistral AI released a family of sparse Mixture of Experts models called Mixtral-8x7B with near state-of-the-art performance Mixtral AI team (2023). This model has already inspired several follow-up works and practical applications, but it still requires a high-end GPU accelerator.\n# 2.2 Post-training Quantization of LLMs",
            "# 6 Conclusion\nIn this paper, we introduced Mixtral 8x7B, the first mixture-of-experts network to reach a state-of-the- art performance among open-source models. Mixtral 8x7B Instruct outperforms Claude-2.1, Gem- ini Pro, and GPT-3.5 Turbo on human evaluation benchmarks. Because it only uses two experts at each time step, Mixtral only uses 13B active parameters per token while outperforming the previous best model using 70B parameters per token (Llama 2 70B). We are making our trained and fine-tuned mod- els publicly available under the Apache 2.0 license. By sharing our models, we aim to facilitate the de- velopment of new techniques and applications that can benefit a wide range of industries and domains.",
            "# 3.1 Multilingual benchmarks\nCompared to Mistral 7B, we significantly upsample the proportion of multilingual data during pretraining. The extra capacity allows Mixtral to perform well on multilingual benchmarks while maintaining a high accuracy in English. In particular, Mixtral significantly outperforms Llama 2 70B in French, German, Spanish, and Italian, as shown in Table 4.\nActive Params French Arc-c HellaS MMLU German Arc-c HellaS MMLU Spanish Arc-c HellaS MMLU Italian Arc-c HellaS MMLU 33B 70B 13B 42.9% 65.4% 49.0% 39.3% 68.1% 49.9% 49.9% 72.5% 64.3% 49.4% 70.9% 65.1% 58.2% 77.4% 70.9% 54.3% 73.0% 71.5% 55.4% 77.6% 72.5% 52.8% 75.1% 70.9% 41.1% 63.3% 48.7% 47.3% 68.7% 64.2% 45.7% 69.8% 52.3% 50.5% 74.5% 66.0%",
            "Comparison with Llama 2 70B and GPT-3.5. In Table 3, we report the performance of Mixtral 8x7B compared to Llama 2 70B and GPT-3.5. We observe that Mixtral performs similarly or above the two other models. On MMLU, Mixtral obtains a better performance, despite its significantly smaller capacity (47B tokens compared to 70B). For MT Bench, we report the performance of the latest GPT-3.5-Turbo model available, gpt-3.5-turbo-1106.\n2Since Llama 2 34B was not open-sourced, we report results for Llama 1 34B.\n4",
            "We illustrate an example of how LRU cache saves experts in Figure 1 (see caption). LRU is a very simple strategy that does not consider factors like expert activation frequencies, varying cache size between MoE layers, or any sequential patterns in expert activation. However, we found that even this simple strategy can significantly speed up inference for modern Mixture-of-Experts models such as Mixtral-8x7B (see Section 4 for detailed evaluation).\n# 3.2 Speculative Expert Loading\nWhile LRU caching can reduce the average expert loading time, most of the inference time is still spent waiting for the next expert to be loaded. The reason behind this is that, unlike with dense models, MoE offloading cannot effectively overlap expert loading with computation. To understand this problem, let us zoom into the process of generating a single token, layer-by-layer. The full compute workload starts by embedding the previous token via look-up, then alternates between running self-attention and MLP for each transformer block in the model. Finally, the outputs from the last transformer block are used to predict next token logits with a linear projection."
        ],
        "response": "Mixtral 8x7B has 8 feedforward blocks per layer, while Mistral 7B has a single dense feedforward block. Each token in Mixtral uses 2 experts and 13B active parameters during inference, whereas Mistral 7B uses all its 7B parameters. Mixtral's total parameters are 47B, but only 13B are active per token. Mistral 7B uses all 7B parameters for every token. [source: http://arxiv.org/pdf/2401.04088]",
        "reference": "The architectural difference between Mixtral 8x7B and Mistral 7B lies in the number of feedforward blocks (experts) used in each layer. Mixtral 8x7B has 8 feedforward blocks in each layer, while Mistral 7B does not specify the number of feedforward blocks. Additionally, during inference, each token in both models has access to 47B parameters, but only uses 13B active parameters."
    },
    {
        "user_input": "When is offloading used on the A100 server in hardware configurations?",
        "retrieved_contexts": [
            "We consider four hardware configurations: a free-tier Colab instance with a T4 GPU (16GB VRAM, PCIe Gen.3), a past generation gaming laptop with RTX 3080 Mobile (16GB, PCIe Gen.4), a mid- range gaming desktop with RTX 3060 (12GB, PCIe Gen.3) and a high-end data-center server with A100-80GB-SXM. Note that the A100 server could run the model without offloading. We use offloading on A100 mostly to provide a reference for other setups. Finally, when evaluating 3-bit models, we use a cloud T4 from Microsoft Azure because the free-tier colab instances did not have enough RAM for this specific configuration. We use k = 2 for RTX 3060 and k = 4 for all other GPUs.\nAs shown in Table 2, all evaluated setups can generate 2-4 tokens per second with the full algorithm. Using pre-loading appears to be most beneficial on RTX 3060, possibly due to lower LRU cache size. Cursiously, RTX 3060 (desktop) performs nearly equally with a much higher end 3080 Mobile. We attribute this to the fact that both GPUs are still bottlenecked by host-to-device bandwidth, limited by the PCIe architecture. Finally, all schemes significantly outperform naive offloading that loads the entire MoE layer.\n# 5 Conclusion and Future Work",
            "# 4.3 Practical offloading performance\nFinally we evaluate the performance of the Mixtral8x7B-Instruct model using the offloading tech- niquesproposed throughout this report. Based on the perplexity evaluations from the previous section, we chose 4-bit HQQ quantization for the shared attention layers and 2- or 3-bit quantization for experts. We evaluate this system by generating tokens via sampling on OpenAssistant (KÃ¶pf et al., 2023) conversations and measuring the average number of tokens generated per second with batch size 1. For this evaluation, we always sample proportionally to the predicted probabilities, i.e. without temperature or nucleus sampling.",
            "1When deployed in 16-bit precision, Falcon-180B needs approximately 360GB, while LLaMA-2 70B requires 140GB of combined accelerator memory.\nIn this work, we systematically develop techniques for running large MoE language models with limited GPU memory. Our main objective is inferencing (generating tokens) with Mixtral-8x7B- Instruct â a MoE-based chat assistant â on a desktop-grade hardware where only a fraction of experts fit into the accelerator memory. To that end:\nwe observe how MoE language model accesses its experts between tokens, and find several regularities: i) some experts are reused between adjacent tokens and ii) the model hidden states of early layers already âknowâ which experts are to be used at subsequent layers. â¢ we design a MoE-specific offloading strategy that takes advantage of these regularities: i) it uses LRU cache to significantly reduces GPU-RAM communication, leading to faster generation and ii) it guesses which experts are needed ahead of time to better overlap expert loading with computation.\nâ¢ we consider the specific scenario of running Mixtral-8x7B-Instruct on a T4, RTX 3060 and RTX 3080 Mobile and develop a practical combination of mixed quantization and the proposed offloading algorithm to run this model interactively at 2-3 tokens per second depending on the hardware. The source code with our implementation is available online2",
            "To speed up offloading in practice, we allocate all expert parameters in a contiguous memory buffer that can be moved as a single host-to-device copy. For host-side (RAM) experts, we pin8 this memory buffer for faster communication. Our implementation additionally allocates b=4 on-device buffers used to copy and prefetch experts asynchronously, without modifying existing experts. These buffers are shared between all MoE layers to reduce memory footprint. Overall, the system requires num_layers Ã num_experts expert memory buffers split between host and device memory and b=4 temporary buffers, the size of each buffer being equal to a single expert.\n# 4 Experiments\nIn this section, we verify our earlier hypotheses about MoE behavior and benchmark the inference latency in different conditions. We focus our evaluations on Mixtral-8x7B and Mixtral-8x7B-Instruct models since they represent the current state of the art among open-access MoE models. We organize this section as follows: Section 4.1 measures the effectiveness of expert caching and pre-loading in isolation, Section 4.2 compares different model compression algorithms and verifies our hypotheses from Section 3.3. Finally, Section 4.3 measures the inference latency in several hardware setups.",
            "# Denis Mazur Moscow Institute of Physics and Technology Yandex Researchcore denismazur8@gmail.com\n# Abstract\nWith the widespread adoption of Large Language Models (LLMs), many deep learning practitioners are looking for strategies of running these models more efficiently. One such strategy is to use sparse Mixture-of-Experts (MoE) â a type of model architectures where only a fraction of model layers are active for any given input. This property allows MoE-based language models to generate tokens faster than their âdenseâ counterparts, but it also increases model size due to having multiple âexpertsâ. Unfortunately, this makes state-of-the-art MoE language models difficult to run without high-end GPUs. In this work, we study the problem of running large MoE language models on consumer hardware with limited accelerator memory. We build upon parameter offloading algorithms and propose a novel strategy that accelerates offloading by taking advantage of innate properties of MoE LLMs. Using this strategy, we build can run Mixtral-8x7B with mixed quantization on desktop hardware and free-tier Google Colab instances.\n# Introduction",
            "Expert Offloading. As described earlier, we use LRU cache with an equal number k of cached experts per layer. For Mixtral-8x7B, we use k=2 for 12GB GPUs and k=4 for 16GB ones. We trigger speculative expert loading immediately after the system finished loading all experts for the current layer. The speculative expert loading fetches 1 â 2 most likely experts. The newly loaded experts do not replace the currently cached experts. If a speculatively loaded expert was later used during next layer inference, it will replace the least recently used expert from the next layerâs cache.\nMany consumer devices and free-tier cloud instances have limited host RAM that cannot fit the entire model7. In these cases, the experts must be split between host and device memory. To support this, our implementation of expert LRU cache splits experts between host and GPU devices. When loading and expert to the GPU cache, the system also offloads the least recently used on-device expert back to RAM so as to preserve memory parity.",
            "Table 1: Perplexity and model size evaluation of Mixtral-8x7B with different quantization for shared attention (Attn quant) and experts (Experts quant) layers. For comprarison, a Mistral-7B 4-bit quantized model has Wiki2 perplexity 5.03, C4 perplexity 7.56 and MMLU score 61.3%. See Section 4.2 for details. Green values correspond to the configurations we chose for full system evaluation.\n6\nAlgorithm 2-bit Experts 3-bit Experts A100 3080 Mobile 3060 T4 (Colab) A100 3080 Mobile 3060 T4 (Cloud) 3.061 Full algorithm 2.918 W/o expert pre-loading 2.265 W/o LRU cache & pre-loading Naive offloading (accelerate) 1.392 2.655 2.227 1.758 1.059 2.278 2.051 1.547 0.919 2.092 1.567 1.168 0.661 2.845 2.683 2.055 1.246 2.475 2.024 1.595 0.914 2.038 1.857 1.346 1.791 1.603 1.365 1.061 0.580",
            "# 5 Conclusion and Future Work\nIn this work, we explore strategies for accelerating Mixture-of-Experts based language models on consumer hardware with limited GPU memory. We propose a MoE-centric approach to offloading\n7\nand explore how mixed quantization affects perplexity and performance on language understanding tasks. We evaluate the proposed strategies and show that they produce a significant increase in generation speed compared to naÂ¨ve approaches on consumer-grade hardware, including free-tier Google Colab.\nOur method provides a practical solution for inferencing large MoE language models on resource- constricted hardware, enabling broader access to these powerful models for research and development. As future work, we plan to explore further offloading strategies, based on speculative expert predic- tion.\n# Acknowledgements\nAuthors would like to acknowledge mobicham@ for helpful discussions on Mixtral quantization.\n# References\nAminabadi, R. Y., Rajbhandari, S., Awan, A. A., Li, C., Li, D., Zheng, E., Ruwase, O., Smith, S., Zhang, M., Rasley, J., and He, Y. Deepspeed-inference: Enabling efficient inference of transformer models at unprecedented scale. In Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis, SC â22. IEEE Press, 2022. ISBN 9784665454445.",
            "This technique works particularly well when processing large batches of data, during train- ing Pudipeddi et al. (2020); Ren et al. (2021) or large-batch non-interactive inference Aminabadi et al. (2022); Sheng et al. (2023), where each layer processes a lot of tokens each time the layer is loaded from RAM. In turn, when doing interactive inference (e.g. as a chat assistants), offloading works significantly slower than on-device inference. This is because interactive inference generates tokens autoregressively, from left to right. This way, the inference system processes one or few tokens at a time, and therefore spends most of the time waiting for next layerâs parameters to be loaded.\n# 2.4 Hardware Setup",
            "We propose a new class of selective state space models, that improves on prior work on several axes to achieve the modeling power of Transformers while scaling linearly in sequence length.\nSelection Mechanism. First, we identify a key limitation of prior models: the ability to eï¬ciently select data in an input-dependent manner (i.e. focus on or ignore particular inputs). Building on intuition based on important synthetic tasks such as selective copy and induction heads, we design a simple selection mechanism by parameterizing the SSM parameters based on the input. This allows the model to ï¬lter out irrelevant information and remember relevant information indeï¬nitely.\nHardware-aware Algorithm. This simple change poses a technical challenge for the computation of the model; in fact, all prior SSMs models must be time- and input-invariant in order to be computationally eï¬cient. We overcome this with a hardware-aware algorithm that computes the model recurrently with a scan instead of convolution, but does not materialize the expanded state in order to avoid IO access between diï¬erent levels of the GPU memory hierarchy. The resulting implementation is faster than previous methods both in theory (scaling linearly in sequence length, compared to pseudo-linear for all convolution-based SSMs) and on modern hardware (up to 3Ã faster on A100 GPUs)."
        ],
        "response": "Offloading on the A100 server is used primarily to provide a reference for other hardware setups, though it can run the model without offloading. It is employed in the context of evaluating Mixtral-8x7B-Instruct with mixed quantization, where 4-bit HQQ is used for shared attention layers and 2- or 3-bit for experts. The A100 uses k=4 for expert caching, and offloading is part of a strategy to compare performance with consumer-grade GPUs. The technique includes speculative expert loading and LRU caching to optimize memory usage and speed. [http://arxiv.org/pdf/2312.17238]",
        "reference": "Offloading is used on the A100 server in hardware configurations to provide a reference for other setups."
    },
    {
        "user_input": "How does Mixtral compare to Llama 2 70B in code benchmarks?",
        "retrieved_contexts": [
            "Table 2: Comparison of Mixtral with Llama. Mixtral outperforms or matches Llama 2 70B performance on almost all popular benchmarks while using 5x fewer active parameters during inference.\n70 Mixtral 8x7B. âMixtral 8x7B Mixtral 8x7B 355 =o = Es & E60! Mistral 78 % 2681 Mistral 78 3 3 s0 5 = A % 66 50 g 4 45 64 78 138 348708 78 138 348708 78 138 348 70B S66 Mixtral 8x7B 50 Mixtral 8x7B 5 = 564 340 g al Mistral 78 ee Mistral 78 3 5 Â§ 30 5 eo â= Mistral Â° 20 âe LlaMA2 78 (138 348 70B 7B (138 348 708 7B Â«13B 34B 708 Active Params Active Params Active Params\nFigure 3: Results on MMLU, commonsense reasoning, world knowledge and reading comprehension, math and code for Mistral (7B/8x7B) vs Llama 2 (7B/13B/70B). Mixtral largely outperforms Llama 2 70B on all benchmarks, except on reading comprehension benchmarks while using 5x lower active parameters. It is also vastly superior to Llama 2 70B on code and math.",
            "Detailed results for Mixtral, Mistral 7B and Llama 2 7B/13B/70B and Llama 1 34B2 are reported in Table 2. Figure 2 compares the performance of Mixtral with the Llama models in different categories. Mixtral surpasses Llama 2 70B across most metrics. In particular, Mixtral displays a superior performance in code and mathematics benchmarks.\nSize and Efficiency. We compare our performance to the Llama 2 family, aiming to understand Mixtral modelsâ efficiency in the cost-performance spectrum (see Figure 3). As a sparse Mixture- of-Experts model, Mixtral only uses 13B active parameters for each token. With 5x lower active parameters, Mixtral is able to outperform Llama 2 70B across most categories.\nNote that this analysis focuses on the active parameter count (see Section 2.1), which is directly proportional to the inference compute cost, but does not consider the memory costs and hardware utilization. The memory costs for serving Mixtral are proportional to its sparse parameter count, 47B, which is still smaller than Llama 2 70B. As for device utilization, we note that the SMoEs layer introduces additional overhead due to the routing mechanism and due to the increased memory loads when running more than one expert per device. They are more suitable for batched workloads where one can reach a good degree of arithmetic intensity.",
            "Llama 2 70B Mixtral 8x7B BBQ accuracy 51.5% 56.0% BOLD sentiment score (avg Â± std) gender profession religious_ideology political_ideology race 0.293 Â± 0.073 0.218 Â± 0.073 0.188 Â± 0.133 0.149 Â± 0.140 0.232 Â± 0.049 0.323 Â±0.045 0.243 Â± 0.087 0.144 Â± 0.089 0.186 Â± 0.146 0.232 Â± 0.052\nFigure 5: Bias Benchmarks. Compared Llama 2 70B, Mixtral presents less bias (higher accuracy on BBQ, lower std on BOLD) and displays more positive sentiment (higher avg on BOLD).\nWe benchmark Llama 2 and Mixtral on BBQ and BOLD with our evaluation framework and report the results in Table 5. Compared to Llama 2, Mixtral presents less bias on the BBQ benchmark (56.0% vs 51.5%). For each group in BOLD, a higher average sentiment score means more positive sentiments and a lower standard deviation indicates less bias within the group. Overall, Mixtral displays more positive sentiments than Llama 2, with similar variances within each group.\n# Instruction Fine-tuning",
            "Comparison with Llama 2 70B and GPT-3.5. In Table 3, we report the performance of Mixtral 8x7B compared to Llama 2 70B and GPT-3.5. We observe that Mixtral performs similarly or above the two other models. On MMLU, Mixtral obtains a better performance, despite its significantly smaller capacity (47B tokens compared to 70B). For MT Bench, we report the performance of the latest GPT-3.5-Turbo model available, gpt-3.5-turbo-1106.\n2Since Llama 2 34B was not open-sourced, we report results for Llama 1 34B.\n4",
            "Abstract\nWe introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine- tuned to follow instructions, Mixtral 8x7B â Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â chat model on human bench- marks. Both the base and instruct models are released under the Apache 2.0 license.\nCode: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/mixtral-of-experts/",
            "2Since Llama 2 34B was not open-sourced, we report results for Llama 1 34B.\n4\nLLaMA 2 70B GPT-3.5 MMLU (MCQ in 57 subjects) 69.9% 70.0% 70.6% HellaSwag (10-shot) 87.1% 85.5% 86.7% ARC Challenge (25-shot) 85.1% 85.2% 85.8% WinoGrande (5-shot) 83.2% 81.6% 81.2% MBPP (pass@1) 49.8% 52.2% 60.7% GSM-8K (5-shot) 53.6% 57.1% 58.4% MT Bench (for Instruct Models) 6.86 8.32 8.30\n# Mixtral 8x7B\nTable 3: Comparison of Mixtral with Llama 2 70B and GPT-3.5. Mixtral outperforms or matches Llama 2 70B and GPT-3.5 performance on most metrics.\nEvaluation Differences. On some benchmarks, there are some differences between our evaluation protocol and the one reported in the Llama 2 paper: 1) on MBPP, we use the hand-verified subset 2) on TriviaQA, we do not provide Wikipedia contexts.\n# 3.1 Multilingual benchmarks",
            "# 3.1 Multilingual benchmarks\nCompared to Mistral 7B, we significantly upsample the proportion of multilingual data during pretraining. The extra capacity allows Mixtral to perform well on multilingual benchmarks while maintaining a high accuracy in English. In particular, Mixtral significantly outperforms Llama 2 70B in French, German, Spanish, and Italian, as shown in Table 4.\nActive Params French Arc-c HellaS MMLU German Arc-c HellaS MMLU Spanish Arc-c HellaS MMLU Italian Arc-c HellaS MMLU 33B 70B 13B 42.9% 65.4% 49.0% 39.3% 68.1% 49.9% 49.9% 72.5% 64.3% 49.4% 70.9% 65.1% 58.2% 77.4% 70.9% 54.3% 73.0% 71.5% 55.4% 77.6% 72.5% 52.8% 75.1% 70.9% 41.1% 63.3% 48.7% 47.3% 68.7% 64.2% 45.7% 69.8% 52.3% 50.5% 74.5% 66.0%",
            "Code: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/mixtral-of-experts/\n# Introduction\nIn this paper, we present Mixtral 8x7B, a sparse mixture of experts model (SMoE) with open weights, licensed under Apache 2.0. Mixtral outperforms Llama 2 70B and GPT-3.5 on most benchmarks. As it only uses a subset of its parameters for every token, Mixtral allows faster inference speed at low batch-sizes, and higher throughput at large batch-sizes.\nMixtral is a sparse mixture-of-experts network. It is a decoder-only model where the feedforward block picks from a set of 8 distinct groups of parameters. At every layer, for every token, a router network chooses two of these groups (the âexpertsâ) to process the token and combine their output additively. This technique increases the number of parameters of a model while controlling cost and latency, as the model only uses a fraction of the total set of parameters per token.\nMixtral is pretrained with multilingual data using a context size of 32k tokens. It either matches or exceeds the performance of Llama 2 70B and GPT-3.5, over several benchmarks. In particular,\nMixture of Experts Layer i gating inputs af outputs router expert",
            "Table 4: Comparison of Mixtral with Llama on Multilingual Benchmarks. On ARC Challenge, Hellaswag, and MMLU, Mixtral outperforms Llama 2 70B on 4 languages: French, German, Spanish, and Italian.\n# 3.2 Long range performance\nTo assess the capabilities of Mixtral to tackle long context, we evaluate it on the passkey retrieval task introduced in [23], a synthetic task designed to measure the ability of the model to retrieve a passkey inserted randomly in a long prompt. Results in Figure 4 (Left) show that Mixtral achieves a 100% retrieval accuracy regardless of the context length or the position of passkey in the sequence. Figure 4 (Right) shows that the perplexity of Mixtral on a subset of the proof-pile dataset [2] decreases monotonically as the size of the context increases.\nPasskey Performance ry 0.8 0.6 04 0.2 0.0 OK 4K 8K 12K 16K 20K 24K 28K Seq Len Passkey Loc\n3.8 â Mixtral_8x7B 3.5 32 > $3.0 i] 228 fos a 2.0 0 5k 10k 15k 20k 25k 30k Context length",
            "Mixture of Experts Layer i gating inputs af outputs router expert\nFigure 1: Mixture of Experts Layer. Each input vector is assigned to 2 of the 8 experts by a router. The layerâs output is the weighted sum of the outputs of the two selected experts. In Mixtral, an expert is a standard feedforward block as in a vanilla transformer architecture.\nMixtral demonstrates superior capabilities in mathematics, code generation, and tasks that require multilingual understanding, significantly outperforming Llama 2 70B in these domains. Experiments show that Mixtral is able to successfully retrieve information from its context window of 32k tokens, regardless of the sequence length and the location of the information in the sequence."
        ],
        "response": "Mixtral 8x7B outperforms Llama 2 70B on code benchmarks, as shown in Table 3 and Figure 3. Specifically, Mixtral achieves 52.2% pass@1 on MBPP compared to Llama 2 70B's 49.8%. It also surpasses Llama 2 70B on GSM-8K with 57.1% accuracy versus 53.6%. Mixtral uses only 13B active parameters during inference, making it more efficient. These results highlight Mixtral's superior performance in code and mathematics. [http://arxiv.org/pdf/2401.04088]",
        "reference": "Mixtral outperforms Llama 2 70B in code benchmarks."
    },
    {
        "user_input": "In terms of mathematics benchmarks, how does Mixtral perform compared to Llama 2 70B?",
        "retrieved_contexts": [
            "Detailed results for Mixtral, Mistral 7B and Llama 2 7B/13B/70B and Llama 1 34B2 are reported in Table 2. Figure 2 compares the performance of Mixtral with the Llama models in different categories. Mixtral surpasses Llama 2 70B across most metrics. In particular, Mixtral displays a superior performance in code and mathematics benchmarks.\nSize and Efficiency. We compare our performance to the Llama 2 family, aiming to understand Mixtral modelsâ efficiency in the cost-performance spectrum (see Figure 3). As a sparse Mixture- of-Experts model, Mixtral only uses 13B active parameters for each token. With 5x lower active parameters, Mixtral is able to outperform Llama 2 70B across most categories.\nNote that this analysis focuses on the active parameter count (see Section 2.1), which is directly proportional to the inference compute cost, but does not consider the memory costs and hardware utilization. The memory costs for serving Mixtral are proportional to its sparse parameter count, 47B, which is still smaller than Llama 2 70B. As for device utilization, we note that the SMoEs layer introduces additional overhead due to the routing mechanism and due to the increased memory loads when running more than one expert per device. They are more suitable for batched workloads where one can reach a good degree of arithmetic intensity.",
            "Table 2: Comparison of Mixtral with Llama. Mixtral outperforms or matches Llama 2 70B performance on almost all popular benchmarks while using 5x fewer active parameters during inference.\n70 Mixtral 8x7B. âMixtral 8x7B Mixtral 8x7B 355 =o = Es & E60! Mistral 78 % 2681 Mistral 78 3 3 s0 5 = A % 66 50 g 4 45 64 78 138 348708 78 138 348708 78 138 348 70B S66 Mixtral 8x7B 50 Mixtral 8x7B 5 = 564 340 g al Mistral 78 ee Mistral 78 3 5 Â§ 30 5 eo â= Mistral Â° 20 âe LlaMA2 78 (138 348 70B 7B (138 348 708 7B Â«13B 34B 708 Active Params Active Params Active Params\nFigure 3: Results on MMLU, commonsense reasoning, world knowledge and reading comprehension, math and code for Mistral (7B/8x7B) vs Llama 2 (7B/13B/70B). Mixtral largely outperforms Llama 2 70B on all benchmarks, except on reading comprehension benchmarks while using 5x lower active parameters. It is also vastly superior to Llama 2 70B on code and math.",
            "Comparison with Llama 2 70B and GPT-3.5. In Table 3, we report the performance of Mixtral 8x7B compared to Llama 2 70B and GPT-3.5. We observe that Mixtral performs similarly or above the two other models. On MMLU, Mixtral obtains a better performance, despite its significantly smaller capacity (47B tokens compared to 70B). For MT Bench, we report the performance of the latest GPT-3.5-Turbo model available, gpt-3.5-turbo-1106.\n2Since Llama 2 34B was not open-sourced, we report results for Llama 1 34B.\n4",
            "Abstract\nWe introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine- tuned to follow instructions, Mixtral 8x7B â Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â chat model on human bench- marks. Both the base and instruct models are released under the Apache 2.0 license.\nCode: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/mixtral-of-experts/",
            "Llama 2 70B Mixtral 8x7B BBQ accuracy 51.5% 56.0% BOLD sentiment score (avg Â± std) gender profession religious_ideology political_ideology race 0.293 Â± 0.073 0.218 Â± 0.073 0.188 Â± 0.133 0.149 Â± 0.140 0.232 Â± 0.049 0.323 Â±0.045 0.243 Â± 0.087 0.144 Â± 0.089 0.186 Â± 0.146 0.232 Â± 0.052\nFigure 5: Bias Benchmarks. Compared Llama 2 70B, Mixtral presents less bias (higher accuracy on BBQ, lower std on BOLD) and displays more positive sentiment (higher avg on BOLD).\nWe benchmark Llama 2 and Mixtral on BBQ and BOLD with our evaluation framework and report the results in Table 5. Compared to Llama 2, Mixtral presents less bias on the BBQ benchmark (56.0% vs 51.5%). For each group in BOLD, a higher average sentiment score means more positive sentiments and a lower standard deviation indicates less bias within the group. Overall, Mixtral displays more positive sentiments than Llama 2, with similar variances within each group.\n# Instruction Fine-tuning",
            "2Since Llama 2 34B was not open-sourced, we report results for Llama 1 34B.\n4\nLLaMA 2 70B GPT-3.5 MMLU (MCQ in 57 subjects) 69.9% 70.0% 70.6% HellaSwag (10-shot) 87.1% 85.5% 86.7% ARC Challenge (25-shot) 85.1% 85.2% 85.8% WinoGrande (5-shot) 83.2% 81.6% 81.2% MBPP (pass@1) 49.8% 52.2% 60.7% GSM-8K (5-shot) 53.6% 57.1% 58.4% MT Bench (for Instruct Models) 6.86 8.32 8.30\n# Mixtral 8x7B\nTable 3: Comparison of Mixtral with Llama 2 70B and GPT-3.5. Mixtral outperforms or matches Llama 2 70B and GPT-3.5 performance on most metrics.\nEvaluation Differences. On some benchmarks, there are some differences between our evaluation protocol and the one reported in the Llama 2 paper: 1) on MBPP, we use the hand-verified subset 2) on TriviaQA, we do not provide Wikipedia contexts.\n# 3.1 Multilingual benchmarks",
            "# 3.1 Multilingual benchmarks\nCompared to Mistral 7B, we significantly upsample the proportion of multilingual data during pretraining. The extra capacity allows Mixtral to perform well on multilingual benchmarks while maintaining a high accuracy in English. In particular, Mixtral significantly outperforms Llama 2 70B in French, German, Spanish, and Italian, as shown in Table 4.\nActive Params French Arc-c HellaS MMLU German Arc-c HellaS MMLU Spanish Arc-c HellaS MMLU Italian Arc-c HellaS MMLU 33B 70B 13B 42.9% 65.4% 49.0% 39.3% 68.1% 49.9% 49.9% 72.5% 64.3% 49.4% 70.9% 65.1% 58.2% 77.4% 70.9% 54.3% 73.0% 71.5% 55.4% 77.6% 72.5% 52.8% 75.1% 70.9% 41.1% 63.3% 48.7% 47.3% 68.7% 64.2% 45.7% 69.8% 52.3% 50.5% 74.5% 66.0%",
            "Table 4: Comparison of Mixtral with Llama on Multilingual Benchmarks. On ARC Challenge, Hellaswag, and MMLU, Mixtral outperforms Llama 2 70B on 4 languages: French, German, Spanish, and Italian.\n# 3.2 Long range performance\nTo assess the capabilities of Mixtral to tackle long context, we evaluate it on the passkey retrieval task introduced in [23], a synthetic task designed to measure the ability of the model to retrieve a passkey inserted randomly in a long prompt. Results in Figure 4 (Left) show that Mixtral achieves a 100% retrieval accuracy regardless of the context length or the position of passkey in the sequence. Figure 4 (Right) shows that the perplexity of Mixtral on a subset of the proof-pile dataset [2] decreases monotonically as the size of the context increases.\nPasskey Performance ry 0.8 0.6 04 0.2 0.0 OK 4K 8K 12K 16K 20K 24K 28K Seq Len Passkey Loc\n3.8 â Mixtral_8x7B 3.5 32 > $3.0 i] 228 fos a 2.0 0 5k 10k 15k 20k 25k 30k Context length",
            "# 6 Conclusion\nIn this paper, we introduced Mixtral 8x7B, the first mixture-of-experts network to reach a state-of-the- art performance among open-source models. Mixtral 8x7B Instruct outperforms Claude-2.1, Gem- ini Pro, and GPT-3.5 Turbo on human evaluation benchmarks. Because it only uses two experts at each time step, Mixtral only uses 13B active parameters per token while outperforming the previous best model using 70B parameters per token (Llama 2 70B). We are making our trained and fine-tuned mod- els publicly available under the Apache 2.0 license. By sharing our models, we aim to facilitate the de- velopment of new techniques and applications that can benefit a wide range of industries and domains.",
            "Code: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/mixtral-of-experts/\n# Introduction\nIn this paper, we present Mixtral 8x7B, a sparse mixture of experts model (SMoE) with open weights, licensed under Apache 2.0. Mixtral outperforms Llama 2 70B and GPT-3.5 on most benchmarks. As it only uses a subset of its parameters for every token, Mixtral allows faster inference speed at low batch-sizes, and higher throughput at large batch-sizes.\nMixtral is a sparse mixture-of-experts network. It is a decoder-only model where the feedforward block picks from a set of 8 distinct groups of parameters. At every layer, for every token, a router network chooses two of these groups (the âexpertsâ) to process the token and combine their output additively. This technique increases the number of parameters of a model while controlling cost and latency, as the model only uses a fraction of the total set of parameters per token.\nMixtral is pretrained with multilingual data using a context size of 32k tokens. It either matches or exceeds the performance of Llama 2 70B and GPT-3.5, over several benchmarks. In particular,\nMixture of Experts Layer i gating inputs af outputs router expert"
        ],
        "response": "Mixtral outperforms Llama 2 70B across most mathematics benchmarks [http://arxiv.org/pdf/2401.04088]. Mixtral displays superior performance in code and mathematics benchmarks compared to Llama 2 70B [http://arxiv.org/pdf/2401.04088]. Mixtral vastly outperforms Llama 2 70B on mathematics and code generation benchmarks [http://arxiv.org/pdf/2401.04088]. Mixtral achieves higher accuracy on GSM-8K and MBPP benchmarks than Llama 2 70B [http://arxiv.org/pdf/2401.04088]. Mixtral uses only 13B active parameters per token while outperforming Llama 2 70B which uses 70B parameters [http://arxiv.org/pdf/2401.04088]. Mixtral's sparse Mixture-of-Experts architecture enables efficient inference with lower active parameters [http://arxiv.org/pdf/2401.04088]. Mixtral's performance on mathematics benchmarks is significantly better than Llama 2 70B [http://arxiv.org/pdf/2401.04088]. Mixtral's results on GSM-8K are 57.1% compared to Llama 2 70B's 53.6% [http://arxiv.org/pdf/2401.04088]. Mixtral's MBPP accuracy is 52.2% versus Llama 2 70B's 49.8% [http://arxiv.org/pdf/2401.04088]. Mixtral's efficiency in mathematics benchmarks is achieved with 5x fewer active parameters [http://arxiv.org/pdf/2401.04088].",
        "reference": "Mixtral performs better than Llama 2 70B in terms of mathematics benchmarks."
    },
    {
        "user_input": "What is the relationship between benchmarking the expert LRU cache and speculative loading, and the expert recall rate in the Mixtral-8x7B-Instruct model on the OpenAssistant dataset?",
        "retrieved_contexts": [
            "For this evaluation, we run Mixtral-8x7B-Instruct model on the OpenAssistant dataset (KÃ¶pf et al., 2023). We test LRU caching by running the model on recorded conversations and measuring the recall (aka âhit ratioâ from caching perspective) for different cache sizes k. Next, we test speculative loading in isolation by âguessingâ which experts should be loaded (by applying the next layerâs gating function on current layer activations), then measuring how often the actual next experts get loaded this way. A recall of 1.0 corresponds to a situation where both (2) Mixtral active experts were pre-fetched. We test speculative loading in three settings: 1, 2 and 10 layers ahead.\n# 4.2 Mixed MoE Quantization",
            "7Notably, Google Colab RAM cannot fit Mixtral-8x7B with a reasonable compression rate 8This corresponds to tensor.pin_memory() command in PyTorch.\n5\niy & cache_size =3 cache_size = 2 cache_size =4 0.84 | PIO â prefetch 1 experts ~ escent ae | PRS aa 0.2} ââ prefetch 2 experts ââ prefetch 3 experts 0.0 00 0 5 10 15 20 25 30 0 5 10 15 20 25 30 Layer # Layer # S Fd Ed Cache hit rate Bd ES Prediction recall = ES Ss &\nFigure 2: (left) LRU cache hit ratio for different cache size k; (right) speculative loading recall when pre-loading a different number of experts. Regular lines represent loading 1 layer ahead; dashed line stands for 2 layers ahead; dotted line is 10 layers ahead.\n# 4.1 Expert LRU Cache and Speculative Loading\nIn this section, we benchmark the effectiveness of the two expert offloading strategies: LRU caching and and speculative loading, as defined in Sections 3.1 and 3.2 respectively. For this evaluation, we measure âexpert recallâ â the fraction of times when an expert needed for inference was already available on GPU.",
            "Expert Offloading. As described earlier, we use LRU cache with an equal number k of cached experts per layer. For Mixtral-8x7B, we use k=2 for 12GB GPUs and k=4 for 16GB ones. We trigger speculative expert loading immediately after the system finished loading all experts for the current layer. The speculative expert loading fetches 1 â 2 most likely experts. The newly loaded experts do not replace the currently cached experts. If a speculatively loaded expert was later used during next layer inference, it will replace the least recently used expert from the next layerâs cache.\nMany consumer devices and free-tier cloud instances have limited host RAM that cannot fit the entire model7. In these cases, the experts must be split between host and device memory. To support this, our implementation of expert LRU cache splits experts between host and GPU devices. When loading and expert to the GPU cache, the system also offloads the least recently used on-device expert back to RAM so as to preserve memory parity.",
            "We illustrate an example of how LRU cache saves experts in Figure 1 (see caption). LRU is a very simple strategy that does not consider factors like expert activation frequencies, varying cache size between MoE layers, or any sequential patterns in expert activation. However, we found that even this simple strategy can significantly speed up inference for modern Mixture-of-Experts models such as Mixtral-8x7B (see Section 4 for detailed evaluation).\n# 3.2 Speculative Expert Loading\nWhile LRU caching can reduce the average expert loading time, most of the inference time is still spent waiting for the next expert to be loaded. The reason behind this is that, unlike with dense models, MoE offloading cannot effectively overlap expert loading with computation. To understand this problem, let us zoom into the process of generating a single token, layer-by-layer. The full compute workload starts by embedding the previous token via look-up, then alternates between running self-attention and MLP for each transformer block in the model. Finally, the outputs from the last transformer block are used to predict next token logits with a linear projection.",
            "Figure 1: An example of expert loading pattern in Mixtral-8x7B-Instruct for select layers. Blue cells indicate that a certain expert was active when encoding a certain token; deeper blue indicates higher gating weight. Small gray squares show which experts are cached with an LRU cache for k=2.\n# 3.1 Expert Locality and LRU caching\nAs we discussed earlier in Section 2.1, Mixture-of-Experts language models were often observed to assign individual experts to distinct sub-tasks. However, this does not mean that the model uses the same expert over long stretches of tokens. Instead, some experts are active in short sequences of 2-4 tokens, while others are often used with âgapsâ, as shown in Figure 1.\nTo take advantage of this pattern, we can keep active experts in GPU memory as a âcacheâ for future tokens. If the same experts are activated again in future, they will be available instantaneously. Naturally, the number of experts that can be stored this way if very limited by the available GPU memory. For simplicity, we choose to always keep k least recently used experts as a type of LRU cache. If k is greater than the number of active experts, the cache will save experts from multiple previous tokens. For simplicity, we keep the same number of cached experts for each MoE layer.",
            "To speed up offloading in practice, we allocate all expert parameters in a contiguous memory buffer that can be moved as a single host-to-device copy. For host-side (RAM) experts, we pin8 this memory buffer for faster communication. Our implementation additionally allocates b=4 on-device buffers used to copy and prefetch experts asynchronously, without modifying existing experts. These buffers are shared between all MoE layers to reduce memory footprint. Overall, the system requires num_layers Ã num_experts expert memory buffers split between host and device memory and b=4 temporary buffers, the size of each buffer being equal to a single expert.\n# 4 Experiments\nIn this section, we verify our earlier hypotheses about MoE behavior and benchmark the inference latency in different conditions. We focus our evaluations on Mixtral-8x7B and Mixtral-8x7B-Instruct models since they represent the current state of the art among open-access MoE models. We organize this section as follows: Section 4.1 measures the effectiveness of expert caching and pre-loading in isolation, Section 4.2 compares different model compression algorithms and verifies our hypotheses from Section 3.3. Finally, Section 4.3 measures the inference latency in several hardware setups.",
            "1When deployed in 16-bit precision, Falcon-180B needs approximately 360GB, while LLaMA-2 70B requires 140GB of combined accelerator memory.\nIn this work, we systematically develop techniques for running large MoE language models with limited GPU memory. Our main objective is inferencing (generating tokens) with Mixtral-8x7B- Instruct â a MoE-based chat assistant â on a desktop-grade hardware where only a fraction of experts fit into the accelerator memory. To that end:\nwe observe how MoE language model accesses its experts between tokens, and find several regularities: i) some experts are reused between adjacent tokens and ii) the model hidden states of early layers already âknowâ which experts are to be used at subsequent layers. â¢ we design a MoE-specific offloading strategy that takes advantage of these regularities: i) it uses LRU cache to significantly reduces GPU-RAM communication, leading to faster generation and ii) it guesses which experts are needed ahead of time to better overlap expert loading with computation.\nâ¢ we consider the specific scenario of running Mixtral-8x7B-Instruct on a T4, RTX 3060 and RTX 3080 Mobile and develop a practical combination of mixed quantization and the proposed offloading algorithm to run this model interactively at 2-3 tokens per second depending on the hardware. The source code with our implementation is available online2",
            "n-1 y= Ss Softmax(Top2(a - W,)); - SwiGLU;(a). i=0\nThis formulation is similar to the GShard architecture [21], with the exceptions that we replace all FFN sub-blocks by MoE layers while GShard replaces every other block, and that GShard uses a more elaborate gating strategy for the second expert assigned to each token.\n# 3 Results\nWe compare Mixtral to Llama, and re-run all benchmarks with our own evaluation pipeline for fair comparison. We measure performance on a wide variety of tasks categorized as follow:\nâ¢ Commonsense Reasoning (0-shot): Hellaswag [32], Winogrande [26], PIQA [3], SIQA [27], OpenbookQA [22], ARC-Easy, ARC-Challenge [8], CommonsenseQA [30]",
            "# 6 Conclusion\nIn this paper, we introduced Mixtral 8x7B, the first mixture-of-experts network to reach a state-of-the- art performance among open-source models. Mixtral 8x7B Instruct outperforms Claude-2.1, Gem- ini Pro, and GPT-3.5 Turbo on human evaluation benchmarks. Because it only uses two experts at each time step, Mixtral only uses 13B active parameters per token while outperforming the previous best model using 70B parameters per token (Llama 2 70B). We are making our trained and fine-tuned mod- els publicly available under the Apache 2.0 license. By sharing our models, we aim to facilitate the de- velopment of new techniques and applications that can benefit a wide range of industries and domains.",
            "Table 1: Perplexity and model size evaluation of Mixtral-8x7B with different quantization for shared attention (Attn quant) and experts (Experts quant) layers. For comprarison, a Mistral-7B 4-bit quantized model has Wiki2 perplexity 5.03, C4 perplexity 7.56 and MMLU score 61.3%. See Section 4.2 for details. Green values correspond to the configurations we chose for full system evaluation.\n6\nAlgorithm 2-bit Experts 3-bit Experts A100 3080 Mobile 3060 T4 (Colab) A100 3080 Mobile 3060 T4 (Cloud) 3.061 Full algorithm 2.918 W/o expert pre-loading 2.265 W/o LRU cache & pre-loading Naive offloading (accelerate) 1.392 2.655 2.227 1.758 1.059 2.278 2.051 1.547 0.919 2.092 1.567 1.168 0.661 2.845 2.683 2.055 1.246 2.475 2.024 1.595 0.914 2.038 1.857 1.346 1.791 1.603 1.365 1.061 0.580"
        ],
        "response": "The relationship between benchmarking expert LRU cache and speculative loading, and expert recall rate is evaluated by measuring the fraction of times an expert needed for inference is already available on GPU. For LRU caching, recall rates vary with cache size k, with higher k improving recall but limited by GPU memory. Speculative loading pre-fetches 1-2 likely experts based on next layer's gating function, with recall measured at 1, 2, and 10 layers ahead. A recall of 1.0 indicates all Mixtral active experts were pre-fetched [source: http://arxiv.org/pdf/2312.17238]. The effectiveness of both strategies is tested on the OpenAssistant dataset for Mixtral-8x7B-Instruct [source: http://arxiv.org/pdf/2312.17238].",
        "reference": "The relationship between benchmarking the expert LRU cache and speculative loading and the expert recall rate in the Mixtral-8x7B-Instruct model on the OpenAssistant dataset is not mentioned in the given context."
    },
    {
        "user_input": "How does the use of sparse Mixture-of-Experts (MoE) in language models contribute to faster token generation?",
        "retrieved_contexts": [
            "# Denis Mazur Moscow Institute of Physics and Technology Yandex Researchcore denismazur8@gmail.com\n# Abstract\nWith the widespread adoption of Large Language Models (LLMs), many deep learning practitioners are looking for strategies of running these models more efficiently. One such strategy is to use sparse Mixture-of-Experts (MoE) â a type of model architectures where only a fraction of model layers are active for any given input. This property allows MoE-based language models to generate tokens faster than their âdenseâ counterparts, but it also increases model size due to having multiple âexpertsâ. Unfortunately, this makes state-of-the-art MoE language models difficult to run without high-end GPUs. In this work, we study the problem of running large MoE language models on consumer hardware with limited accelerator memory. We build upon parameter offloading algorithms and propose a novel strategy that accelerates offloading by taking advantage of innate properties of MoE LLMs. Using this strategy, we build can run Mixtral-8x7B with mixed quantization on desktop hardware and free-tier Google Colab instances.\n# Introduction",
            "1When deployed in 16-bit precision, Falcon-180B needs approximately 360GB, while LLaMA-2 70B requires 140GB of combined accelerator memory.\nIn this work, we systematically develop techniques for running large MoE language models with limited GPU memory. Our main objective is inferencing (generating tokens) with Mixtral-8x7B- Instruct â a MoE-based chat assistant â on a desktop-grade hardware where only a fraction of experts fit into the accelerator memory. To that end:\nwe observe how MoE language model accesses its experts between tokens, and find several regularities: i) some experts are reused between adjacent tokens and ii) the model hidden states of early layers already âknowâ which experts are to be used at subsequent layers. â¢ we design a MoE-specific offloading strategy that takes advantage of these regularities: i) it uses LRU cache to significantly reduces GPU-RAM communication, leading to faster generation and ii) it guesses which experts are needed ahead of time to better overlap expert loading with computation.\nâ¢ we consider the specific scenario of running Mixtral-8x7B-Instruct on a T4, RTX 3060 and RTX 3080 Mobile and develop a practical combination of mixed quantization and the proposed offloading algorithm to run this model interactively at 2-3 tokens per second depending on the hardware. The source code with our implementation is available online2",
            "Shazeer et al. (2017) builds on this idea to train a sparsely gated Mixture-of-Experts to serve as a language model. The full model consists of a recurrent neural network backbone and a MoE module with up to 131072 experts. When processing a given token, a linear gating function select 4 most suitable experts based on the latest hidden state. The resulting model (including the gating function and experts) is trained end-to-end to minimize cross-entropy, with an additional regularizer to promote equal expert utilization. Shazeer et al. (2017) observed that the MoE model not only improves perplexity, but also learns interpretable expert specializations: some experts would âspecializeâ on prepositions, while others learn to express a particular concept (e.g. speed).",
            "Since then, several lines of work explore Mixture-of-Experts with Transformer-based language models for machine translation Lepikhin et al. (2020), masked language modeling Fedus et al. (2021), general-purpose LLMs Du et al. (2022) and others. Most of these models follow traditional (dense) Transformer architecture for embeddings and attention layers, and only use Mixture for the feedforward (MLP) blocks and use a linear token-level gating function. A common observation across most of these works is that MoE models are cheaper to train and inference Fedus et al. (2021); Lepikhin et al. (2020), but require more parameters than a dense model with equivalent perplexity. Pre-trained Mixture-of-Experts LLMs have been openly available for over a year3. However, these models seem to have gained less traction than equivalent dense models, arguable because their sheer model size (over a trillion parameters) makes them difficult to use. Most recently, Mistral AI released a family of sparse Mixture of Experts models called Mixtral-8x7B with near state-of-the-art performance Mixtral AI team (2023). This model has already inspired several follow-up works and practical applications, but it still requires a high-end GPU accelerator.\n# 2.2 Post-training Quantization of LLMs",
            "# 5 Conclusion and Future Work\nIn this work, we explore strategies for accelerating Mixture-of-Experts based language models on consumer hardware with limited GPU memory. We propose a MoE-centric approach to offloading\n7\nand explore how mixed quantization affects perplexity and performance on language understanding tasks. We evaluate the proposed strategies and show that they produce a significant increase in generation speed compared to naÂ¨ve approaches on consumer-grade hardware, including free-tier Google Colab.\nOur method provides a practical solution for inferencing large MoE language models on resource- constricted hardware, enabling broader access to these powerful models for research and development. As future work, we plan to explore further offloading strategies, based on speculative expert predic- tion.\n# Acknowledgements\nAuthors would like to acknowledge mobicham@ for helpful discussions on Mixtral quantization.\n# References\nAminabadi, R. Y., Rajbhandari, S., Awan, A. A., Li, C., Li, D., Zheng, E., Ruwase, O., Smith, S., Zhang, M., Rasley, J., and He, Y. Deepspeed-inference: Enabling efficient inference of transformer models at unprecedented scale. In Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis, SC â22. IEEE Press, 2022. ISBN 9784665454445.",
            "Several recent works modify transformer architecture by introducing sparse Mixture-of-Experts blocks (Jacobs et al., 1991; Shazeer et al., 2017). MoE blocks contain multiple âexpertsâ (layers), as well as a âgating functionâ that selects which experts are used on a given input. As a result, the MoE block uses a small portion of all âexpertsâ for any single forward pass, allowing for more compute-efficient training Fedus et al. (2021); Du et al. (2022). Notably, MoEs are among the largest Fedus et al. (2021) and among the best Mixtral AI team (2023) of available LLMs. While Mixture-of-Experts models can be more efficient than their dense counterparts, many techniques for efficient LLM inference were not designed with MoE in mind and perform suboptimally on modern large language models that use mixture-of-experts layers.\n1When deployed in 16-bit precision, Falcon-180B needs approximately 360GB, while LLaMA-2 70B requires 140GB of combined accelerator memory.",
            "Code: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/mixtral-of-experts/\n# Introduction\nIn this paper, we present Mixtral 8x7B, a sparse mixture of experts model (SMoE) with open weights, licensed under Apache 2.0. Mixtral outperforms Llama 2 70B and GPT-3.5 on most benchmarks. As it only uses a subset of its parameters for every token, Mixtral allows faster inference speed at low batch-sizes, and higher throughput at large batch-sizes.\nMixtral is a sparse mixture-of-experts network. It is a decoder-only model where the feedforward block picks from a set of 8 distinct groups of parameters. At every layer, for every token, a router network chooses two of these groups (the âexpertsâ) to process the token and combine their output additively. This technique increases the number of parameters of a model while controlling cost and latency, as the model only uses a fraction of the total set of parameters per token.\nMixtral is pretrained with multilingual data using a context size of 32k tokens. It either matches or exceeds the performance of Llama 2 70B and GPT-3.5, over several benchmarks. In particular,\nMixture of Experts Layer i gating inputs af outputs router expert",
            "The generative inference workload consists of two phases: 1) encoding the input prompt and 2) generating tokens conditioned on that prompt. The key difference between these two phases is that prompt tokens are encoded in parallel (layer-by-layer), whereas the generation runs sequentially (token-by-token and layer-by-layer). In general, phase 1 works relatively well with existing Mixture- of-Experts algorithms, since each layer can only be loaded once for the entire prompt. In turn, when generating tokens, one must load layer once per each token generated. In practice, this means that inference speed is limited by how fast one can fetch parameters from system memory.\nBelow, we look for patterns in how the MoE model loads its experts and propose ways to exploit these patterns to speed up inference time.\n4To learn more about these methods, please refer to surveys such as Gholami et al. (2021); Liang et al. (2021) 5As opposed to running a processing a large batch of texts over many seconds, as in Sheng et al. (2023)\n3\nSelected experts for Mixtral-8x7B-Instruct woe 0 (top) and 15 ae =n a oa ao a âme: a n: ee Layer 15 expert # Layer 0 expert # MAUR STARR O However about |= and 4 training data owerful language model based trained Trans former f architecture",
            "[10] Jwala Dhamala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada Pruksachatkun, Kai-Wei Chang, and Rahul Gupta. Bold: Dataset and metrics for measuring biases in open-ended language generation. In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency, pages 862â872, 2021.\n[11] Artyom Eliseev and Denis Mazur. Fast inference of mixture-of-experts language models with offloading. arXiv preprint arXiv:2312.17238, 2023.\n[12] William Fedus, Jeff Dean, and Barret Zoph. A review of sparse expert models in deep learning. arXiv preprint arXiv:2209.01667, 2022.\n[13] Trevor Gale, Deepak Narayanan, Cliff Young, and Matei Zaharia. Megablocks: Efficient sparse training with mixture-of-experts. arXiv preprint arXiv:2211.15841, 2022.",
            "We illustrate an example of how LRU cache saves experts in Figure 1 (see caption). LRU is a very simple strategy that does not consider factors like expert activation frequencies, varying cache size between MoE layers, or any sequential patterns in expert activation. However, we found that even this simple strategy can significantly speed up inference for modern Mixture-of-Experts models such as Mixtral-8x7B (see Section 4 for detailed evaluation).\n# 3.2 Speculative Expert Loading\nWhile LRU caching can reduce the average expert loading time, most of the inference time is still spent waiting for the next expert to be loaded. The reason behind this is that, unlike with dense models, MoE offloading cannot effectively overlap expert loading with computation. To understand this problem, let us zoom into the process of generating a single token, layer-by-layer. The full compute workload starts by embedding the previous token via look-up, then alternates between running self-attention and MLP for each transformer block in the model. Finally, the outputs from the last transformer block are used to predict next token logits with a linear projection."
        ],
        "response": "Sparse Mixture-of-Experts (MoE) models use only a subset of experts per token, reducing computational load and accelerating token generation. [http://arxiv.org/pdf/2312.17238] This sparsity allows MoE models to process tokens faster than dense models while maintaining high performance. [http://arxiv.org/pdf/2312.17238] The gating function selects the most relevant experts for each token, ensuring efficient resource utilization. [http://arxiv.org/pdf/2312.17238] Additionally, techniques like LRU caching and speculative loading minimize GPU-RAM communication, further speeding up inference. [http://arxiv.org/pdf/2312.17238] These optimizations enable MoE models like Mixtral-8x7B to generate tokens quickly on limited hardware. [http://arxiv.org/pdf/2312.17238]",
        "reference": "The use of sparse Mixture-of-Experts (MoE) in language models contributes to faster token generation because only a fraction of model layers are active for any given input. This means that MoE-based language models can generate tokens faster than their \"dense\" counterparts. The MoE blocks in the model contain multiple \"experts\" (layers) and a \"gating function\" that selects which experts are used for a given input. By using a small portion of all \"experts\" for each forward pass, the MoE block allows for more compute-efficient training, leading to faster token generation."
    },
    {
        "user_input": "What impact does the use of sparse Mixture-of-Experts (MoE) have on the size of language models?",
        "retrieved_contexts": [
            "# Denis Mazur Moscow Institute of Physics and Technology Yandex Researchcore denismazur8@gmail.com\n# Abstract\nWith the widespread adoption of Large Language Models (LLMs), many deep learning practitioners are looking for strategies of running these models more efficiently. One such strategy is to use sparse Mixture-of-Experts (MoE) â a type of model architectures where only a fraction of model layers are active for any given input. This property allows MoE-based language models to generate tokens faster than their âdenseâ counterparts, but it also increases model size due to having multiple âexpertsâ. Unfortunately, this makes state-of-the-art MoE language models difficult to run without high-end GPUs. In this work, we study the problem of running large MoE language models on consumer hardware with limited accelerator memory. We build upon parameter offloading algorithms and propose a novel strategy that accelerates offloading by taking advantage of innate properties of MoE LLMs. Using this strategy, we build can run Mixtral-8x7B with mixed quantization on desktop hardware and free-tier Google Colab instances.\n# Introduction",
            "Several recent works modify transformer architecture by introducing sparse Mixture-of-Experts blocks (Jacobs et al., 1991; Shazeer et al., 2017). MoE blocks contain multiple âexpertsâ (layers), as well as a âgating functionâ that selects which experts are used on a given input. As a result, the MoE block uses a small portion of all âexpertsâ for any single forward pass, allowing for more compute-efficient training Fedus et al. (2021); Du et al. (2022). Notably, MoEs are among the largest Fedus et al. (2021) and among the best Mixtral AI team (2023) of available LLMs. While Mixture-of-Experts models can be more efficient than their dense counterparts, many techniques for efficient LLM inference were not designed with MoE in mind and perform suboptimally on modern large language models that use mixture-of-experts layers.\n1When deployed in 16-bit precision, Falcon-180B needs approximately 360GB, while LLaMA-2 70B requires 140GB of combined accelerator memory.",
            "Since then, several lines of work explore Mixture-of-Experts with Transformer-based language models for machine translation Lepikhin et al. (2020), masked language modeling Fedus et al. (2021), general-purpose LLMs Du et al. (2022) and others. Most of these models follow traditional (dense) Transformer architecture for embeddings and attention layers, and only use Mixture for the feedforward (MLP) blocks and use a linear token-level gating function. A common observation across most of these works is that MoE models are cheaper to train and inference Fedus et al. (2021); Lepikhin et al. (2020), but require more parameters than a dense model with equivalent perplexity. Pre-trained Mixture-of-Experts LLMs have been openly available for over a year3. However, these models seem to have gained less traction than equivalent dense models, arguable because their sheer model size (over a trillion parameters) makes them difficult to use. Most recently, Mistral AI released a family of sparse Mixture of Experts models called Mixtral-8x7B with near state-of-the-art performance Mixtral AI team (2023). This model has already inspired several follow-up works and practical applications, but it still requires a high-end GPU accelerator.\n# 2.2 Post-training Quantization of LLMs",
            "Shazeer et al. (2017) builds on this idea to train a sparsely gated Mixture-of-Experts to serve as a language model. The full model consists of a recurrent neural network backbone and a MoE module with up to 131072 experts. When processing a given token, a linear gating function select 4 most suitable experts based on the latest hidden state. The resulting model (including the gating function and experts) is trained end-to-end to minimize cross-entropy, with an additional regularizer to promote equal expert utilization. Shazeer et al. (2017) observed that the MoE model not only improves perplexity, but also learns interpretable expert specializations: some experts would âspecializeâ on prepositions, while others learn to express a particular concept (e.g. speed).",
            "# 2 Background & Related Work\n# 2.1 Mixture-of-Experts\nThe recent surge in MoE language models builds on a relatively old idea (Jacobs et al., 1991; Jordan & Jacobs, 1994) of training ensembles of specialized models (âexpertsâ) and a gating function to select the right expert for the task. To achieve specialization, Mixture-of-Experts learn by simultaneously i) training the gating function to choose the best experts and ii) training the experts themselves on samples assigned to them by the gating function. Since then, many different MoE variants emerged, including mixture of SVM models (Collobert et al., 2002), Dirichlet processes (Shahbaba & Neal, 2009) and various neural networks.",
            "Abstract\nWe introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine- tuned to follow instructions, Mixtral 8x7B â Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B â chat model on human bench- marks. Both the base and instruct models are released under the Apache 2.0 license.\nCode: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/mixtral-of-experts/",
            "[10] Jwala Dhamala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada Pruksachatkun, Kai-Wei Chang, and Rahul Gupta. Bold: Dataset and metrics for measuring biases in open-ended language generation. In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency, pages 862â872, 2021.\n[11] Artyom Eliseev and Denis Mazur. Fast inference of mixture-of-experts language models with offloading. arXiv preprint arXiv:2312.17238, 2023.\n[12] William Fedus, Jeff Dean, and Barret Zoph. A review of sparse expert models in deep learning. arXiv preprint arXiv:2209.01667, 2022.\n[13] Trevor Gale, Deepak Narayanan, Cliff Young, and Matei Zaharia. Megablocks: Efficient sparse training with mixture-of-experts. arXiv preprint arXiv:2211.15841, 2022.",
            "Code: https://github.com/mistralai/mistral-src Webpage: https://mistral.ai/news/mixtral-of-experts/\n# Introduction\nIn this paper, we present Mixtral 8x7B, a sparse mixture of experts model (SMoE) with open weights, licensed under Apache 2.0. Mixtral outperforms Llama 2 70B and GPT-3.5 on most benchmarks. As it only uses a subset of its parameters for every token, Mixtral allows faster inference speed at low batch-sizes, and higher throughput at large batch-sizes.\nMixtral is a sparse mixture-of-experts network. It is a decoder-only model where the feedforward block picks from a set of 8 distinct groups of parameters. At every layer, for every token, a router network chooses two of these groups (the âexpertsâ) to process the token and combine their output additively. This technique increases the number of parameters of a model while controlling cost and latency, as the model only uses a fraction of the total set of parameters per token.\nMixtral is pretrained with multilingual data using a context size of 32k tokens. It either matches or exceeds the performance of Llama 2 70B and GPT-3.5, over several benchmarks. In particular,\nMixture of Experts Layer i gating inputs af outputs router expert",
            "dim n_layers head_dim hidden_dim n_heads n_kv_heads context_len vocab_size num_experts top_k_experts\n# 2.1 Sparse Mixture of Experts\nWe present a brief overview of the Mixture of Experts layer (Figure 1). For a more in-depth overview, see [12]. The output of the MoE module for a given input x is determined by the weighted sum of the outputs of the expert networks, where the weights are given by the gating networkâs output. i.e. given n expert networks {E0, Ei, ..., Enâ1}, the output of the expert layer is given by: Table 1: Model architecture.\n# j nâ\nG(x)i Â· Ei(x). i=0\nHere, G(x)i denotes the n-dimensional output of the gating network for the i-th expert, and Ei(x) is the output of the i-th expert network. If the gating vector is sparse, we can avoid computing the outputs of experts whose gates are zero. There are multiple alternative ways of implementing G(x) [6, 15, 35], but a simple and performant one is implemented by taking the softmax over the Top-K logits of a linear layer [28]. We use\nG(x) := Softmax(TopK(x Â· Wg)),",
            "# 5 Conclusion and Future Work\nIn this work, we explore strategies for accelerating Mixture-of-Experts based language models on consumer hardware with limited GPU memory. We propose a MoE-centric approach to offloading\n7\nand explore how mixed quantization affects perplexity and performance on language understanding tasks. We evaluate the proposed strategies and show that they produce a significant increase in generation speed compared to naÂ¨ve approaches on consumer-grade hardware, including free-tier Google Colab.\nOur method provides a practical solution for inferencing large MoE language models on resource- constricted hardware, enabling broader access to these powerful models for research and development. As future work, we plan to explore further offloading strategies, based on speculative expert predic- tion.\n# Acknowledgements\nAuthors would like to acknowledge mobicham@ for helpful discussions on Mixtral quantization.\n# References\nAminabadi, R. Y., Rajbhandari, S., Awan, A. A., Li, C., Li, D., Zheng, E., Ruwase, O., Smith, S., Zhang, M., Rasley, J., and He, Y. Deepspeed-inference: Enabling efficient inference of transformer models at unprecedented scale. In Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis, SC â22. IEEE Press, 2022. ISBN 9784665454445."
        ],
        "response": "No response generated",
        "reference": "The use of sparse Mixture-of-Experts (MoE) increases the size of language models due to having multiple \"experts.\""
    },
    {
        "user_input": "How does LRU caching improve the inference speed of Mixture-of-Experts language models?",
        "retrieved_contexts": [
            "We illustrate an example of how LRU cache saves experts in Figure 1 (see caption). LRU is a very simple strategy that does not consider factors like expert activation frequencies, varying cache size between MoE layers, or any sequential patterns in expert activation. However, we found that even this simple strategy can significantly speed up inference for modern Mixture-of-Experts models such as Mixtral-8x7B (see Section 4 for detailed evaluation).\n# 3.2 Speculative Expert Loading\nWhile LRU caching can reduce the average expert loading time, most of the inference time is still spent waiting for the next expert to be loaded. The reason behind this is that, unlike with dense models, MoE offloading cannot effectively overlap expert loading with computation. To understand this problem, let us zoom into the process of generating a single token, layer-by-layer. The full compute workload starts by embedding the previous token via look-up, then alternates between running self-attention and MLP for each transformer block in the model. Finally, the outputs from the last transformer block are used to predict next token logits with a linear projection.",
            "Figure 1: An example of expert loading pattern in Mixtral-8x7B-Instruct for select layers. Blue cells indicate that a certain expert was active when encoding a certain token; deeper blue indicates higher gating weight. Small gray squares show which experts are cached with an LRU cache for k=2.\n# 3.1 Expert Locality and LRU caching\nAs we discussed earlier in Section 2.1, Mixture-of-Experts language models were often observed to assign individual experts to distinct sub-tasks. However, this does not mean that the model uses the same expert over long stretches of tokens. Instead, some experts are active in short sequences of 2-4 tokens, while others are often used with âgapsâ, as shown in Figure 1.\nTo take advantage of this pattern, we can keep active experts in GPU memory as a âcacheâ for future tokens. If the same experts are activated again in future, they will be available instantaneously. Naturally, the number of experts that can be stored this way if very limited by the available GPU memory. For simplicity, we choose to always keep k least recently used experts as a type of LRU cache. If k is greater than the number of active experts, the cache will save experts from multiple previous tokens. For simplicity, we keep the same number of cached experts for each MoE layer.",
            "7Notably, Google Colab RAM cannot fit Mixtral-8x7B with a reasonable compression rate 8This corresponds to tensor.pin_memory() command in PyTorch.\n5\niy & cache_size =3 cache_size = 2 cache_size =4 0.84 | PIO â prefetch 1 experts ~ escent ae | PRS aa 0.2} ââ prefetch 2 experts ââ prefetch 3 experts 0.0 00 0 5 10 15 20 25 30 0 5 10 15 20 25 30 Layer # Layer # S Fd Ed Cache hit rate Bd ES Prediction recall = ES Ss &\nFigure 2: (left) LRU cache hit ratio for different cache size k; (right) speculative loading recall when pre-loading a different number of experts. Regular lines represent loading 1 layer ahead; dashed line stands for 2 layers ahead; dotted line is 10 layers ahead.\n# 4.1 Expert LRU Cache and Speculative Loading\nIn this section, we benchmark the effectiveness of the two expert offloading strategies: LRU caching and and speculative loading, as defined in Sections 3.1 and 3.2 respectively. For this evaluation, we measure âexpert recallâ â the fraction of times when an expert needed for inference was already available on GPU.",
            "Expert Offloading. As described earlier, we use LRU cache with an equal number k of cached experts per layer. For Mixtral-8x7B, we use k=2 for 12GB GPUs and k=4 for 16GB ones. We trigger speculative expert loading immediately after the system finished loading all experts for the current layer. The speculative expert loading fetches 1 â 2 most likely experts. The newly loaded experts do not replace the currently cached experts. If a speculatively loaded expert was later used during next layer inference, it will replace the least recently used expert from the next layerâs cache.\nMany consumer devices and free-tier cloud instances have limited host RAM that cannot fit the entire model7. In these cases, the experts must be split between host and device memory. To support this, our implementation of expert LRU cache splits experts between host and GPU devices. When loading and expert to the GPU cache, the system also offloads the least recently used on-device expert back to RAM so as to preserve memory parity.",
            "For this evaluation, we run Mixtral-8x7B-Instruct model on the OpenAssistant dataset (KÃ¶pf et al., 2023). We test LRU caching by running the model on recorded conversations and measuring the recall (aka âhit ratioâ from caching perspective) for different cache sizes k. Next, we test speculative loading in isolation by âguessingâ which experts should be loaded (by applying the next layerâs gating function on current layer activations), then measuring how often the actual next experts get loaded this way. A recall of 1.0 corresponds to a situation where both (2) Mixtral active experts were pre-fetched. We test speculative loading in three settings: 1, 2 and 10 layers ahead.\n# 4.2 Mixed MoE Quantization",
            "# 5 Conclusion and Future Work\nIn this work, we explore strategies for accelerating Mixture-of-Experts based language models on consumer hardware with limited GPU memory. We propose a MoE-centric approach to offloading\n7\nand explore how mixed quantization affects perplexity and performance on language understanding tasks. We evaluate the proposed strategies and show that they produce a significant increase in generation speed compared to naÂ¨ve approaches on consumer-grade hardware, including free-tier Google Colab.\nOur method provides a practical solution for inferencing large MoE language models on resource- constricted hardware, enabling broader access to these powerful models for research and development. As future work, we plan to explore further offloading strategies, based on speculative expert predic- tion.\n# Acknowledgements\nAuthors would like to acknowledge mobicham@ for helpful discussions on Mixtral quantization.\n# References\nAminabadi, R. Y., Rajbhandari, S., Awan, A. A., Li, C., Li, D., Zheng, E., Ruwase, O., Smith, S., Zhang, M., Rasley, J., and He, Y. Deepspeed-inference: Enabling efficient inference of transformer models at unprecedented scale. In Proceedings of the International Conference on High Performance Computing, Networking, Storage and Analysis, SC â22. IEEE Press, 2022. ISBN 9784665454445.",
            "To speed up offloading in practice, we allocate all expert parameters in a contiguous memory buffer that can be moved as a single host-to-device copy. For host-side (RAM) experts, we pin8 this memory buffer for faster communication. Our implementation additionally allocates b=4 on-device buffers used to copy and prefetch experts asynchronously, without modifying existing experts. These buffers are shared between all MoE layers to reduce memory footprint. Overall, the system requires num_layers Ã num_experts expert memory buffers split between host and device memory and b=4 temporary buffers, the size of each buffer being equal to a single expert.\n# 4 Experiments\nIn this section, we verify our earlier hypotheses about MoE behavior and benchmark the inference latency in different conditions. We focus our evaluations on Mixtral-8x7B and Mixtral-8x7B-Instruct models since they represent the current state of the art among open-access MoE models. We organize this section as follows: Section 4.1 measures the effectiveness of expert caching and pre-loading in isolation, Section 4.2 compares different model compression algorithms and verifies our hypotheses from Section 3.3. Finally, Section 4.3 measures the inference latency in several hardware setups.",
            "The generative inference workload consists of two phases: 1) encoding the input prompt and 2) generating tokens conditioned on that prompt. The key difference between these two phases is that prompt tokens are encoded in parallel (layer-by-layer), whereas the generation runs sequentially (token-by-token and layer-by-layer). In general, phase 1 works relatively well with existing Mixture- of-Experts algorithms, since each layer can only be loaded once for the entire prompt. In turn, when generating tokens, one must load layer once per each token generated. In practice, this means that inference speed is limited by how fast one can fetch parameters from system memory.\nBelow, we look for patterns in how the MoE model loads its experts and propose ways to exploit these patterns to speed up inference time.\n4To learn more about these methods, please refer to surveys such as Gholami et al. (2021); Liang et al. (2021) 5As opposed to running a processing a large batch of texts over many seconds, as in Sheng et al. (2023)\n3\nSelected experts for Mixtral-8x7B-Instruct woe 0 (top) and 15 ae =n a oa ao a âme: a n: ee Layer 15 expert # Layer 0 expert # MAUR STARR O However about |= and 4 training data owerful language model based trained Trans former f architecture",
            "Shazeer et al. (2017) builds on this idea to train a sparsely gated Mixture-of-Experts to serve as a language model. The full model consists of a recurrent neural network backbone and a MoE module with up to 131072 experts. When processing a given token, a linear gating function select 4 most suitable experts based on the latest hidden state. The resulting model (including the gating function and experts) is trained end-to-end to minimize cross-entropy, with an additional regularizer to promote equal expert utilization. Shazeer et al. (2017) observed that the MoE model not only improves perplexity, but also learns interpretable expert specializations: some experts would âspecializeâ on prepositions, while others learn to express a particular concept (e.g. speed).",
            "# Denis Mazur Moscow Institute of Physics and Technology Yandex Researchcore denismazur8@gmail.com\n# Abstract\nWith the widespread adoption of Large Language Models (LLMs), many deep learning practitioners are looking for strategies of running these models more efficiently. One such strategy is to use sparse Mixture-of-Experts (MoE) â a type of model architectures where only a fraction of model layers are active for any given input. This property allows MoE-based language models to generate tokens faster than their âdenseâ counterparts, but it also increases model size due to having multiple âexpertsâ. Unfortunately, this makes state-of-the-art MoE language models difficult to run without high-end GPUs. In this work, we study the problem of running large MoE language models on consumer hardware with limited accelerator memory. We build upon parameter offloading algorithms and propose a novel strategy that accelerates offloading by taking advantage of innate properties of MoE LLMs. Using this strategy, we build can run Mixtral-8x7B with mixed quantization on desktop hardware and free-tier Google Colab instances.\n# Introduction"
        ],
        "response": "LRU caching improves inference speed by keeping frequently used experts in GPU memory, reducing the need to reload them from system memory. This strategy exploits the pattern where experts are active in short sequences, allowing faster access when reused. For Mixtral-8x7B, LRU caching with k=2 or 4 experts per layer significantly increases cache hit rates. The cache hit ratio improves as cache size increases, as shown in Figure 2. Speculative loading complements LRU by preloading likely experts, further reducing latency. The system splits experts between host and GPU memory, using pinned memory for faster transfers. Overall, this approach speeds up inference on consumer-grade hardware by minimizing expert loading delays.[source: http://arxiv.org/pdf/2312.17238]",
        "reference": "LRU caching improves the inference speed of Mixture-of-Experts language models by keeping active experts in GPU memory as a cache for future tokens. This allows for faster retrieval of experts when needed during inference."
    }
]